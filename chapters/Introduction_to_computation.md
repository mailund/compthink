
# Introduction

Using computers as more than glorified typewriters or calculators is an increasingly important aspect of any scientific or technological field, and knowing how to program a computer to solve new problems has become as essential a skill as mathematics. Learning how to program can be a frustrating experience at times, since computers require a level of preciseness and rigour in how we must express that we never encounter elsewhere in life. Further, computers will do *exactly* what you ask them to do, regardless of your actual intend, even if this can have catastrophic consequences. On the other hand, learning how to program can also be very rewarding. Programs are created out of pure thought, and it is a special feeling to seeing a computer translate your thoughts into actions and see it solve your problems for you.

Solving any kind of problem, on a computer or otherwise, requires a certain level of rigour. To solve the right problem, we must first understand what the problem *is*. We also need to have a very clear idea about what an adequate *solution* to the problem would be—or at the very least some way of distinguishing between two solutions to judge if one is better than another. These are concerns we will need to address in any problem solving task, but where everyday life might forgive some fuzzy thinking in problem solving, computers are far less forgiving. To solve a problem on a computer, you must first specify with mathematical clarity what the problem is and what a solution is, and after that, how you will go about deriving a solution. And only then, can you write a program and  put the computer to work.

For the novice programmer, the last step—implementing a solution in a computer language—is often the most frustrating. Computer programs do not allow any ambiguities, and that means that if you do not abide by the computer language’s rules—if you get the grammar wrong in the slightest—the computer will refuse to even consider your program. Learning how to write programs the computer will even attempt to run is the first hurdle to overcome.

There are many good books that can teach you different programming languages, and it is worth your while getting a few on the programming language you plan to use in your future work. This book is not so much about programming, however, but about all the work you have to do *before* you implement your solutions.

We will consider programming in the book, and we will use the Python programming language. The Python language generally considered a good first language to learn because of its high-level yet intuitive features, and at the same time, Python is one of the most popular languages for scientific programs. It is one of the most frequently used languages for Data Science. It is number one on the [Kaggle machine learning platform](https://www.kaggle.com). It has powerful libraries for machine learning, data analysis, scientific computing through various software modules. It is also one of the most popular languages for scripting workflows of data analysis and for administrating computer systems. We will only use a little bit of the language, however, so this book is not sufficient if you want to become an effective Python programmer. We use the language to illustrate ideas and for exercising topics we cover, but the focus of the book is not on programming. The focus is how to *think* about problem solving in a formal way; to consider problems as computational tasks and how to plan solutions in ways that are computationally efficient. This is what we mean by *computational thinking*.

## Models of the world and formalising problems

Our goal is to learn how to *formalise objectives* in such a way that we can specify *mathematically and objectively* what solutions to our objectives are. This also means formalising what data we have and how we should interpret it. Formalising a problem might reveal that we do not have sufficient data for the problem at hand. It might also reveal that we do not truly understand our problem—if we cannot clearly define what we want, we won’t be able to formalise how to get it. We might, with some luck, be able to fudge it a bit and get *something*, and then use subjective opinion to judge if what we get is what we wanted. As with the definition of obscene in *acobellis v. Ohio*, “I know it when I see it.” If you and I disagree on whether one solution is better than another or not, we have no way of resolving the issue.

Formalising problems and formalising what data we have to work with is what you do in all natural sciences. You might not have thought about it this way before—depending on which science you have a background in—but when we derive theories (or laws) about the natural world, we are making formal statements about how the world works. For some theories there are exceptions—the world is breaking a natural law—which tells us we do not have a complete law. But any theory worth its salt can be falsified, which is another way of saying that we can judge if a data point matches the formalisation of the theory or not.

In the hard sciences, like physics and chemistry, these theories are described in the language of mathematics. The even harder sciences, like biology, have simpler formulas, and the rules almost always have exceptions; biology is more complex than particle physics, so it is harder to formalise, and thus we stick with simpler mathematics. There is no point in using very complex mathematics to describe something we do not understand; simple mathematics suffices for that. Any quantitative evaluation of the natural world requires some mathematics and some formalisation of scientific theories. Even if the mathematics is simple counting to see if some quantity is more frequent in some situations than others. All quantitative data analysis involves formalising our thoughts about reality and reducing data to the relevant aspects for those formal descriptions.

Abstracting the complex natural world to something we can formalise is called *modelling*. We build models of the real world—usually mathematical models. We aim at making the models simple enough to understand, yet complex enough to describe the aspects of the world we are interested in. In principle, we could model molecular evolution as a physical system at the level or particles. We don’t, because this would be much too complex for us to work with, and probably wouldn’t help us answer most or the questions about evolution we are interested in. Instead, we model molecular evolution as random mutations in strings of DNA, abstracting the three-dimensional DNA molecules into one-dimensional strings over the four letters A, C, G, and T. We abstract away aspects of the world that are not relevant for the models and we abstract away aspects about the data that is not modelled.

Building models of the natural world is the goal of the sciences, and much too large a topic for this book. The models are relevant for computational thinking, however. When we formalise how to solve problems, we do so within a model of the world. This model will affect how we can formalise problems and which level of detail we consider data. Sometimes, changing the model of reality can change what can be efficiently computed—or make an easy problem intractable. Of course, we should not pick scientific theories based on what we can easily compute, but sometimes, abstracting away aspects of the world that are not essential for the problem at hand, will not qualitatively change solutions but might make otherwise impossible to solve problems simple.

This book is not about modelling the world. We will generally assume that we have some formal models to work with in whatever scientific field we find ourselves in. You are rarely in the situation where you can pick your theories at random to satisfy your computational needs, but keep in mind that formalising the *problem* you want to solve might give you some wiggle room within those formal scientific theories. When we study genome evolution or population genetics we abstract complex DNA molecules to the level of strings or reduce populations to gene frequencies. These abstractions are there to simplify the subject matter to something that can be attacked computationally.

## What is computational thinking?

Computational thinking is what you do when you take a problem and formalise it. When you distil it into something where you can objectively determine if something is a solution to it or not. For example, given a sequence of numbers, are all positive? Easy to check, and either all the numbers are positive or they are not. Or, perhaps the problem is not a yes-no question but an optimisation issue. Find the shortest route to get from point A to B is an optimisation problem. It might be easy for me to determine if one route is shorter than another, which would be a yes-no problem, but actually coming up with short routes might be a harder problem. It is still a computational problem, as long as we can formalise what a route is and how we measure distance.

Computational thinking is also what happens after you have formalised the problem. When you figure out how to solve it. A formal description for how to solve a problem is called an [*algorithm*](https://en.wikipedia.org/wiki/Algorithm)—after the 9th Century mathematician Muḥammad ibn Mūsā al'Khwārizmī who is also responsible for the term algebra. To qualify as an algorithm, a description of how to solve a problem must be in sufficient detail that we can follow it without having to involve any guesswork—if you implement it on the computer I guarantee you that you do not want to leave any room for guessing, the description must always get to a solution in a finite number of steps—we don’t want to keep computing forever, and the description must always lead to a valid solution—we don’t want to follow all the steps and end up with something we cannot use anyway.[^algorithm_exceptions]

[^algorithm_exceptions]: There are exceptions to the requirement that an algorithm should always complete in a finite number of steps. When we implement something like a web service or an operating system, we don’t want our programs to terminate after a finite number of calculations, but rather want them to run, and be responsive, indefinitely. In those cases, we relax the requirement to them responding to input in a finite number of steps. We also have exceptions to always getting correct answers. Sometimes, we can accept that we get the right answer with high probability—if we can easily test if the answer is correct and maybe run the algorithm again for another answer, and continue this until we get lucky. These are special cases, however, and we do not consider them in this book.

Designing algorithms is part science and part art. There are general guidelines we can use to approach a computational problem in order to develop algorithms and general approaches to organising data so we can efficiently manipulate it, but you will almost always have to adapt the general ideas to your specific problem. Here, sparks of insight cannot be underestimated—sometimes, just looking at a problem in different ways will open completely new ways of approaching it. The general approaches can be taught and learned, and is the main topic of this book. The art of designing algorithms come with practise, and as with all arts, the more you practice the better you get.

Most of the algorithms we will see in this book are used in almost all software that runs on your computer (with some excepts in the exercises where we do have some toy problems). Sorting and searching in data and arranging data for fast retrieval or fast update is part of almost all computations. The models these algorithms work in are very formalised. Much more than how we model the natural world when we use computers to do science. A problem such as sorting a sequence of objects live in models that determine how we can determine if one element is smaller than another and maybe that is all we can do, or maybe there is more structure in the data that we can exploit. Working in such abstract models can feel far from the world your problem is from, but it is because the models are so abstract that we can apply the algorithmic solutions to so many varied problems.

Some people spend their entire lives developing new algorithms for general problems. Those people would be professional computer science academics. Most people that solve problems on computers are not doing this, even if they develop algorithms on a day to day basis. When we solve actual concrete problems, we can usually do so by combining existing algorithms in the right ways. Having a toolbox of algorithms to pick from, and knowing their strengths and weaknesses is more important in day to day computational work than being able to design algorithms completely from scratch. Although that can be important as well, of course, on the rare occasions when your toolbox does not suffice.

Whether you can get where you want to go by combining existing algorithms or you have to design new ones, the general approach is the same. You have to break down big tasks, that you do not know how to solve (yet), into smaller tasks that, when all done, will have solved the larger tasks. Tasks such as “find the largest number in a sequence” can be broken into smaller tasks such as “compare the first two numbers and remember the largest”, “compare the largest of the first two to the third and remember the largest”, and so on. You start out with one big task—the problem you want to solve—and you keep breaking down the problem until smaller tasks until they all are tasks you know how to solve—either because they are trivial or because you have an algorithm in your toolbox that can solve them. The practise of breaking down tasks until you can solve them all is at the heart of computational thinking.

Developing and combining algorithms is a key part of computational thinking, but algorithms alone do not solve any problems. Algorithms need to be executed to solve concrete problems; we need to follow the instructions they provide on actual data to get actual solutions. Since we rarely want to do this by hand or with pen and paper, we want to instruct computers how to run algorithms, which means that we have to translate a high-level description of an algorithm to a lower level description that can be put into a computer program. This task is called *implementing* the algorithm.

Designing an algorithm and implementing it as a computer program are two separate tasks, although tightly linked. The first task involves understanding the problem you want to solve in sufficient detail that you can break it down into pieces that you know how to solve. The second task involves breaking those pieces into even smaller ones that the computer can solve. What that level is depend on the programming language you will use to implement your algorithm, and which libraries of existing solutions you have available, and this is where the algorithm design task meets the programming task.

The abstraction level at which you can implement an algorithm depends intimately on the programming language and the libraries of functionality you have access to. At the most basic level, the hardware of your computer, the instructions available do little more than move bits around and do basic arithmetic. A modern CPU is a very sophisticated machine for doing this, with many optimisations implemented in hardware, but the basic operations you have at this level are still this primitive. This level of abstraction is where you can get the highest performance out of your CPU, but we rarely if ever program at this level, because it is also the level of abstraction where you get the lowest performance out of a programmer. Basic arithmetic is simply too primitive a level of abstraction for us to efficiently think about algorithms.

Programming languages provide higher levels of abstraction to the programmer. They can do this by knowing how to translate a high level operation, for example an operation that runs through all elements of a sequence, into instructions at a lower level, where there is no concept of elements, sequence, or loop. Alternatively, they have programs for executing higher level operations, implemented at the lower level, and they run these programs to interpret programs at the higher level. We sometimes talk about high-level and low-level programming languages, but there isn't a true dichotomy. There are simply differences in the higher level abstractions provided by all programming languages. Some programming languages provide an environment for programming very close to the hardware, where you can manipulate bits at the lowest level while still having some abstractions to control the steps taken by your program and some abstractions for representing data beyond mere bit patterns. These we would call low-level languages because they aim to be close to the lowest level of abstraction on the computer. Other languages, high-level languages, provide a programming environment that tries to hide the lower levels from the programmer. How data is actually represented at lower levels is hidden from the programmer by abstractions in the language and the programming environment simply guarantees that the mapping between abstractions and bits is handled correctly and that resources allocated by the environment to execute a program are also appropriately freed again when possible—something that must usually be explicitly programmed in low-level languages and can take up a sizeable fraction of the development time for a program.

## Computational thinking in context

**FIXME: stack of CT**

## What is to come

The purpose of this book is to introduce computational thinking as basic problem solving approaches for designing algorithms and implementing them in a computer language. We will focus on the design of algorithms more than the implementation, and only use a subset of the Python programming language for exercises. This will make it easier to transfer what you learn to other programming languages, but keep in mind that it also means that the solutions we consider are not the most effective solutions for an experienced Python programmer. There are ways of expressing things in Python that can implement our algorithms more effectively, but those are Python specific and not generally found in other languages.

We use an actual programming language to explain the algorithms in the book to make it easier for you to experiment with them. Many algorithmic textbooks will not, preferring to describe algorithms in pseudo code where the abstractions can be fitted to the problem. This might make the description of algorithms slightly easier to follow, but can also easily hide away the issues that you will have to resolve to actually implement them. We prefer to use an actual language. It is a very high-level language, so some details that you will have to deal with in lower level languages are still hidden from you, but what you can implement in Python you can actually run on your computer. And it is important that you do take the code in this book and experiment with it.

To get the full benefit out of this book, or any book like it, you must practice. And practice a lot. Programming can look deceptively easy—at least for the complexity level we consider in this book—but it is substantially harder to write your own code than it is to read and understand code already written. Without exercising the skills involved in computational thinking and algorithmic programming, you will at best get a superficial understanding. Watching the Olympics doesn't prepare you for athletics. Each chapter has an exercise set associated with it, and you should expect to use at least as much time doing exercises as you spend reading the chapters if you want the full benefit out of the book.


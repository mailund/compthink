# Introduction to algorithms

We briefly discussed what we mean by *algorithms* in the introduction, but perhaps it bears repeating—algorithms are recipes for solving a specific computational problem. They describe the steps you (or rather, your computer) must take to get from input to output and should guarantee you that if you follow the instructions exactly, you will 1) finish after a finite number of steps and 2) your output will be a solution to the problem they should solve, given the input you had.

If you think I’m being pedantic with 1), consider this program:

```python
x = 0
while True:
    x += 1
```

It computes the largest possible integer—or rather, it would if it ever finished computing. It runs forever, increasing `x` by one in each iteration of the loop, but, of course, it will never finish. Theoretically, at least, eventually the computer the program runs on will succumb to the second law of thermodynamic and stop running, but if the computer lasts for ever, it will keep running this program forever. This is not an algorithm because it takes an infinite number of steps to finish.

When we design an algorithm, the first goal is to make sure that we actually design a proper *algorithm*. That means that steps we describe for solving a problem must be finite and actually solve the problem. We say that an algorithm *terminates* if it actually finish computing in a finite number of steps (and it isn’t a proper algorithm if it doesn’t). If it also computes the right answer, we say that it is *correct*. When we design an algorithm, we must ensure both properties, which usually means proving them mathematically. With experience, you can get a little lax in formally proving this for simple algorithms, but as soon as your algorithm gets sufficiently complicated, you will revert to formal proofs, and as a beginner, it pays to do this for simple algorithms as well.

We could describe this as the algorithm for checking an algorithm:

```
Check if it terminates.
If so, check if it is correct.
```

This, funny enough, is not an algorithm, even if I have seen it described as such a few places. Of course, there aren’t sufficient details to see how we would execute either steps, but the first step cannot be solved by any algorithms at all. It is beyond this book to show you why, but there are problems that cannot be solved on any computer ever constructed, in the past or in the future, and checking if a given algorithm, however you choose to encode it as input to the computer, will terminate on any specific input is one such problem. It is known as the [*halting problem*](https://en.wikipedia.org/wiki/Halting_problem), since we also use the term *halt* for *terminates* when we consider more fundamental aspects of computation.

It is impossible to write a computer program—or generally design an algorithm—that can check these two properties of a suggested algorithm, but nevertheless, it is what you must do to show that your proposed solution to a problem is actually an algorithm. We cannot put up a formula for doing this, for very fundamental reasons, but there are some general techniques that might help you with proving the properties. These techniques are useful when you already have proposed an algorithm, but they can often also guide you in coming up with this proposal in the first place. The first part of this chapter focuses on these techniques and how they can guide you in designing an algorithm, prove that it terminates, and prove that it is correct.

Once we have an actual algorithm, we know how to solve the computational problem at hand. You can compute a solution to your problem in a finite number of steps. That number, however, can be very large. In practice, there is little difference between a program that never finishes and one that will finish in a billion years. Knowing we can solve a problem in a finite number of steps is good, but we usually need more than that. We want to solve the problem in reasonable time, where “reasonable” can depend on the problem at hand.

We have ways of reasoning about the computational complexity of both problems and algorithms that abstract away details about the actual software and hardware the algorithm will be implemented in and run on. Those techniques are the topic for the second part of this chapter. These techniques lets us compare different algorithms and decide which is best independently on how different low-level operations can be executed on any given computer and also gives us some rough ideas about whether an algorithm will run in reasonable time on any computer system you have available.

When we compare algorithms this way, we will say we compare their *efficiency*, and we generally go for efficient algorithms over inefficient algorithms, naturally, at least as long as we can implement either with roughly the same effort. We use algorithmic efficiency to determine how fast a given problem can be solved by a given algorithm.

We can also compare problems this way, and when we do, we say that we compare the problems’ *complexity*. Some problems are intrinsically harder to solve than others. The halting problem, determining if a given algorithm will ever terminate on given input, is *undecidable*, meaning that no algorithm can solve it in general. That is a very hard problem indeed. There are other problems that can be solved but, as far as we know, any solution will take unacceptable long time to do it. We call such problems *intractable*, and unfortunately many of the problems we are interested in, in may fields, are in this category. Very many optimisation problems fall into this category. I did say “as far as we know”, because for many of these problems we do not *know* if it is impossible to derive efficient solutions, and this is one of the greatest open questions in theoretical computer science, known as the [*P vs. NP problem*](https://en.wikipedia.org/wiki/P_versus_NP_problem).

Working out how difficult different problems are to solve is a discipline of theoretical computer science know as *complexity theory*, and the topic is beyond this book. Suffices to know here is that it is possible to put lower bounds on how fast it is possible to solve a given problem, and where it is important I will mention those know lower bounds.

For designing algorithms, we do not necessarily care so much about a problem’s complexity except for two reasons: we do not want to spend time attempting to find an efficient solution to a problem that does not have any efficient solutions. If we have a problem that is known to be intractable, we cannot solve it efficiently—or if we can, we have just solved the N vs. NP problem and should bask in the glory of that rather than worry about problem that led us there in the first place. Instead, we should try to rethink the problem instead. Figure out if a different problem would be equally interesting to solve. Surprisingly often, rephrasing a computational problem solves the underlying scientific problem equally well as the original phrasing, but changes the problem from being intractable to tractable. If we cannot do this, we can try to come up with algorithms that approximate a solution—they might not be the optimal solutions but can perhaps guarantee that they are within a certain fraction of optimal.

The other case where we care about complexity is when we have a problem that is tractable, i.e., we know a lower bound of how fast it is possible to solve the problem and it is not too terrible, and we want to design an algorithm that solves the problem in within that bound. When we reason about algorithmic efficiency, we usually derive upper bounds for how fast (or slow) they will run, while when we reason about problem complexity, we derive lower bounds for how long all algorithms must run, at least, to solve the problem. If we can make these two ends meet, we have an optimal algorithm. We cannot make an even faster algorithm than what we already have. Not measured the way we measure efficiency and complexity, at least. In practice, theoretical running times can be misleading for actual running times, and often, a theoretically faster algorithm, but a more complex one, can be slower on typical input than a simpler but theoretically slower one. At the end of the day, efficiency of programs are measured by how long they run on actual input, so some experimentation is needed when you start implementing algorithms.

Before we worry about algorithmic complexity and efficiency, however, we need to learn how to construct algorithms.

## Designing algorithms

We will explore the tricks for developing algorithms through an example. Consider this problem: You are in city $A$ and want to know if it is possible to get to city $B$, and to answer this question, you have a map available. The problem is a toy example of a real problem. No matter which pair of cities you can think of, you can get from one to the other. There are no city or set of cities that are completely cut off from the rest of the world. But we could ask the question if it is possible to *drive* from city $A$ to city $B$—which isn’t possible for all pairs of cities in the world—and then we would have the same problem. Or we could ask if it is possible to get from city $A$ to city $B$ in less than three hours—in which case we have a slightly harder problem, but still a similar one. Or, we could even ask, what is the shortest route from $A$ to $B$—if we could solve that problem we could use it to solve all the others. Later, in [Chapter @sec:trees-and-graphs], we will see an efficient algorithm for solving the shortest route problem, but for now, we will only consider the first question: is it possible to get from $A$ to $B$?

Consider the map in [@fig:simplified-map]. This is a simplification of a real map of roads between cities. We do not care about the actual landscape the roads pass through, only how cities are connected. So, in this map, we abstract cities to be nodes in a network, and we connect two cities by a line if there is a road between them. It is a simplification of a real map, but a simplification that might be familiar to you from subway maps, where the actual geography is usually not visualised but the connection between stations is.

![Simplified map we can use to determine if we can get from $A$ to $B$.](figures/simplified-map){#fig:simplified-map}

Another simplification is this: we only name cities $A$ and $B$. We assign a number to all cities, so we can refer to them, but we only care to name $A$ and $B$. We can simplify the problem slightly more and ask, “is it possible to get from the city with index 0 to the city with index 13?”, and then we do not even need to name these two cities.

From the figure, you can probably immediately tell that city number 0 and city number 13 are connected (and by several routes), but this partly because the map is very simple, with few cities and partly because it is laid out in a way that makes it easy for your brain to detect the connections. With thousands or millions of connected towns and cities, it would not be so easy to see if two selected cities are connected. It would not be trivial for you to answer the question either, if you got the information in the form that a computer would most likely get it. Our brains are very good at handling visual input, and you can intuitively answer the question on this small map. You probably cannot tell *how* you solve the problem; you just know the answer. You can train computers to “see” in some ways, but it is not the most straightforward approach to telling them about the connectedness of cities—and most likely, if we took that approach, they would run into the same limitations as our brains—they could answer the question for small and well laid out graphs but could not handle very large graphs.

The input we will give the computer is a list of the direct connections:

```python
roads = [
	(0, 1), (0, 2), (0, 3),
	(1, 4), (1, 5),
	(2, 5), (2, 6),
	(3, 6), (3, 7),
	(4, 8),
	(5, 9),
	(6, 9),
	(7, 10),
	(8, 11),
	(8, 12),
	(9, 13),
	(10, 13)
]
```

The roads are represented as pairs of cities. In this example, the first city in a pair has an index that is smaller than the second city in the pair, but we do not mean to imply any order to the pair; if there is a road from $i$ to $j$ there is also a road from $j$ to $i$.

From this list of pairs of cities, we want to design an algorithm that determines if the city at index 13 can be reached from the city at index 0.

### A reductionist approach to designing algorithms

The first and perhaps the most important lesson about designing algorithms is this: if you cannot see an immediate solution to your problem, try to break the problem into subproblems and see if you can solve these. This is generally a good recipe for solving any problems in life—and, incidentally, what recipes do. It might seem like an insurmountable task to cook a large dinner, but a dinner can be split into different dishes and each dish is prepared in a number of smaller steps, and each of the small steps are manageable. When I write a book, I can’t attack the writing as a single task. I break the book into chapters, then chapters into sections, and sections into paragraphs and paragraphs into sentences.

There must be some system in the madness of breaking down a problem into smaller steps. We should ensure that the smaller steps are easier to take than it is to solve the full problem, and we should ensure that if we take all the steps we have solved the full problem. These two requirements are, not coincidentally, similar to the requirements we had for a recipe being an algorithm. When we break a problem into smaller steps, such that the steps are easier, we are making a kind of progress, and this progress should lead us to termination; when we know that the steps, when all are taken, solves the original problem, we have a correct algorithm.

In our original problem we have the question of whether city 0 is connected to city 13, which we cannot immediately answer. What we can answer is whether two cities are neighbours—we could simply run through the list of roads and check if there is one between the two cities. If we could reduce the first problem to the second, we would have a solution to our original problem.

How we can get from testing neighbour-ness to general connectedness might not be obvious, but if we rephrase the two questions to look more similar, it might give us an idea for how to proceed.

* **Original problem:** Is it possible to get from city $i$ to city $j$ by visiting no more than $n$ cities along the way.
* **Neighbour problem:** Is it possible to get from city $i$ to city $j$ by visiting no intermediate cities.

The rephrasing of the original problem is not *entirely* the original problem. Instead of only considering city 0 and city 13, we have made the problem a little harder by wanting to answer it for any cities $i$ and $j$. Also, we have added a constraint, $n$, for the maximum number of cities we visit while going from $i$ to $j$. If $N$ is the number of cities, though, the question of whether it is possible to get from city $i$ to city $j$ is the same as asking if you can do it in less than $N$ steps. Any route that goes through the same city twice can be shortened by removing a circular detour, so if a road from $i$ to $j$ is possible at all, it can be done by visiting less than $N$ intermediate cities.

Obviously, the neighbour problem is a special case of the first problem we listed above, and as we just argued, so is the original question of whether we can get from city 0 to city 13. They all simply differ in what we use for the number of intermediate cities along the route—$n=0$ for the neighbour problem and $n=N$ for the original problem.

To get to $n=N$ from $n=0$ we use another general trick: *induction*. You might have seen this trick used form proving mathematical theorems. Induction works simply like this: if you can show that you have a solution for $n=0$ and that whenever you have a solution for some arbitrary $n$ you can turn that into a solution for $n+1$, then you also have a solution for some desired $n=N$.[^induction-and-algorithms]

[^induction-and-algorithms]: There is a close correspondence between algorithms and proofs by induction, that is not shared for example by proofs by contradiction. When you prove something by contradiction, you prove that some assumption will lead you to conclude something that you know is false. If you use this to prove that a route exists between cities 0 and 13, you will know that such a route exists, but usually have no idea about how to find it. With proofs by induction, the step you take to get from $n$ to $n+1$ usually tells you more than simply that a step is *possible*; it tells you what the step *is*. Therefore, induction proofs are often called constructive proofs. If we wanted to, we could use the step we take in the proof in this chapter to compute a path from city 0 to city 13, but we leave that for [Chapter @sec:trees-and-graphs].

### Assertions and invariants

To ensure that the steps that the algorithm that we are designing comprise of truly solve the problem at hand, we need to know exactly what we can expect to be handled by each step. One way to do this, is to make explicit claims about what should be true before and after each step we take. Such steps are usually called *assertions*, and the claims we make for what should be true *before* a step are called *pre-conditions* and the claims that should be true *after* a step are called *post-conditions*. To prove that the algorithm works according to plan, we need each step to ensure its post-conditions assuming its pre-conditions are met. If each step taken in the algorithm does this, and we can show two additional properties, we have a correct algorithm. These additional properties are simply that the pre-conditions for the first step must be satisfied for all valid input and that the post-conditions for the last step must imply that we have a solution for the problem at hand.

For the concrete problem of finding out if there is a path from city 0 to city 13, we will define the property, $\mathrm{connected}_n(i,j)$, which is true if and only if there is a path from $i$ to $j$ that goes through no more than $n$ intermediate cities. Further, we will assume that we can represent connectedness in a table `table`, such that `table[i] == table[j]` means that we know there exists a path between $i$ and $j$. Finally, let $\mathrm{consistent}(\mathtt{table},\mathrm{connected},n)$ denote that `table[i] == table[j]` if and only if $\mathrm{connected}_n(i,j)$, i.e. the table considers two cities connected if and only if they are connected in less than $n$ steps.

The algorithm will now have this form:

1. Initialise `table` such that `table[i] != table[j]` for all $i\neq j$.
2. Process all road-edges.
3. $\{\mathrm{consistent}(\mathtt{table},\mathrm{connected},0)\}$
4. Now, for $n=0,\ldots,N-1$ do:

    4.1. $\{\mathrm{consistent}(\mathtt{table},\mathrm{connected},n)\}$

    4.2. Update `table` such that `table[i] == table[j]` if and only if $\mathrm{connected}_n(i,j)$.

    4.3. $\{\mathrm{consistent}(\mathtt{table},\mathrm{connected},n+1)\}$

5. $\{\mathrm{consistent}(\mathtt{table},\mathrm{connected},N)\}$
6. Return `table[0] == table[13]`.

In this listing of steps, I have put assertions in curly braces. They are usually not part of the algorithm as such; they do not represent any actions that the computer must take when running the algorithm. They are merely properties that we must show are satisfied at the different locations of the algorithm, and that will help us prove that the algorithm works the way it is intended.

We need to figure out how to implement `table` and how to update it for each step, of course, but let us first see how this algorithm matches an inductive proof and how we can exploit this to show that, if it terminates, it answers the problem we are interested in.

The first assertion we make is in line 3. Here, we say that after reading in all the direct connections between cities, `table[i]` equals `table[j]` if and only if there is a direct road between cities $i$ and $j$. The last assert we make is in line 5, where we we make the same claim about `table` except that the connections should be in $n$ or fewer steps, i.e. `table[i]` is equal to `table[j]` if the two cities are connected at all. Obviously, if this statement is true, then `table[0] == table[13]` if and only if we can find a way from city 0 to city 13, so if the property is true (and we ever reach this step) we will return the correct answer.

The property in line 3 is a pre-condition for the loop and the property in line 5 is a post-condition for the loop. Inside the loop we make two additional assertions, in lines 4.1 and 4.3. These are pre- and post-conditions for the loop-body and captures what we would call the induction step in a proof by induction. We have a property for $n$ and need to ensure the same property for $n+1$. We usually do not make the assertion twice but for updated $n$ as we do in this listing. Instead, we associate an assertion with the loop in general and call it a *loop-invariant*. A loop-invariant, however, is just a short way of specifying that a property must be true for iteration $n$ (or whatever index we use to specify the iteration we are about to execute) *before* we execute the loop-body and must be true for $n+1$ *after* we have executed the body.

Loops are not always identified by a single iteration variable as here, where $n$ counts how many times we have executed the loop-body. We have such a variable in `for`-loops, but usually not in `while`-loops. We should always have some measure of progress we can use to capture the state before and after a loop-body, though.

Now, without knowing how any of the algorithmic steps are performed, we can show that the algorithm, if it terminates, will compute the right answer. This follows because we can show that before we enter the loop, the loop invariant is satisfied—just put $n=0$—and when we exit the loop we satisfy $\{\mathrm{consistent}(\mathtt{table}, \mathrm{connected}, N)$—just set $n=N-1$—and the latter we have already argued ensures that we get the right answer.

If the steps in lines 1, 2, 4, and 4.2 all terminate and all satisfy their pre- and post-condition, we have built an algorithm that terminates and is correct.



### Measuring progress

**FIXME: potential or something that ensure that we make progress in each iteration**

### Exercises on Sequential algorithms

The following exercises tests that have understood algorithms and algorithmic design in sufficient detail to reason about termination and correctness and that you have enough experience with Python programming to implement simple sequential (also known as iterative) algorithms.  Here, by sequential, I simply mean that the algorithms iterate through one or more loops to achieve their goal.

For the exercises, we need some code for picking random numbers and for validating user input. That code is listed below. You do not need to understand it to do the exercises, but you do need to evaluate this code before the exercises can run.

```python
from numpy.random import randint

def input_integer(prompt):
    while True:
        try:
            inp = input(prompt)
            i = int(inp)
            return i
        except:
            print(inp, "is not a valid integer.")

def input_selection(prompt, options):
    modified_prompt = "{} [{}]: ".format(
        prompt.strip(), ", ".join(options)
    )
    while True:
        inp = input(modified_prompt)
        if inp in options:
            return inp
        else:
            print("Invalid choice! Must be in [{}]".format(
                ", ".join(options)
            ))
```

#### Below or above

Here's a game you can play with a friend: one of you think of a number between 1 and 20, both 1 and 20 included. The other has to figure out what that number is. He or she can guess at the number, and after guessing will be told if the guess is right, or if it is too high, or if it is too low. Unless the guess is correct, the guesser must try again until the guess *is* correct.

The following program implements this game for the case where the computer picks the number and you have to guess it. Play with the computer as long as you like.


```python
# When picking a random mumber, we specify the interval
# [low,high). Since high is not included in the interval, 
# we need to use 1 to 21 to get a random number in the
# interval [1,20].
n = randint(1, 21, size = 1)

# Now, repeat guessing until we get the right number.
guess = input_integer("Make a guess. ")
while guess != n:
    if guess > n:
        print("Your guess is too high!")
    else:
        print("Your guess is too low!")
    guess = input_integer("Make a guess. ")
print("You got it!")
```

Here are three different strategies you could use to guess the number:
1. Start with 1. If it isn't the right number, it has to be too low--there are no smaller numbers the right one could be. So if it isn't 1, you guess it is 2. If it isn't, you have once again guessed too low, so now you try 3. You continue by incrementing your guess by one until you get the right answer.
2. Alternatively, you start at 20. If the right number is 20, great, you got it in one guess, but if it is not, your guess must be too high--it cannot possibly be too small. So you try 19 instead, and this time you work your way down until you get the right answer.
3. Tired of trying all numbers from one end to the other, you can pick this strategy: you start by guessing 10. If this is correct, you are done, if it is too high, you know the real number must be in the interval $[1,9]$, and if the guess is too low, you know the right answer must be in the interval $[11,20]$--so for your next guess, you pick the middle of the interval it must be. With each new guess, you update the interval where the real number can be hidden and pick the middle of the new interval.

**Exercise:** Prove that all three strategies terminate and with the correct answer, i.e. they are algorithms for solving this problem.

**Exercise:** Would you judge all three approaches to be equally efficient in finding the right number? If not, how would you order the three strategies such that the method most likely to get the right number first is ranked highest and the algorithm most likely to get the right number last is ranked lowest. Justify your answer.

If you do not lie to the computer when it asks you about its guess compared to the number you are thinking of, this program implements the first strategy:

```python
for guess in range(1,21):
    result = input_selection(
        "How is my guess {}?".format(guess),
        ["low", "hit", "high"]
    )
    if result == "hit":
        print("Wuhuu!")
        break
    else:
        print("I must have been too low, right?", result)
```

**Exercise:** Implement the other two strategies and test them.

When iterating from 20 and down, for the second strategy, you should always get the result `"high"` when you ask about your guess, so you can use a `for` loop and not worry about the actual result form `input_selection`. When you implement strategy number three, however, you need to keep track of a candidate interval with a lower bound, initially 1, and an upper bound, initially 20. When you guess to high, you should lower your upper bound to the value you just guessed minus one (no need to include the guess we know is too high). When you guess to low, you must increase your lower bound to the number you just guessed plus one. In both cases, after updating the interval, you should guess for the middle point in the new interval. When you compute the middle value in your interval, you can use 

```python
guess = int(upper_bound + lower_bound) / 2)
```


#### Finding square roots

Given a number positive number $S > 0$, we want to compute its positive square root, $\sqrt{S}$. We don't need our answer to be perfectly accurate. Using floating point numbers with a finite number of bits to represent the uncountable set of real numbers prevents this anyway. However, we want to be able to put an upper bound on the error we get, $\epsilon$, such that we are guaranteed that for our result, $\hat{S}$, we have $|S-\hat{S}|<\epsilon$.

One algorithm that solves this problem is known as the *Babylonian method* and is based on two observations. The first is this, for any $x>0$, if $x>\sqrt{S}$ then $S/x<\sqrt{S}$ and if $S/x>\sqrt{S}$ then $x<\sqrt{S}$, i.e., if we guess at a value for the square root of $S$ and the guess is too high, we have a lower bound on what it *could* be, and if the guess is too low, we have an upper bound on what it could be, see [@fig:babylonian-method-range].

![Bounds for where to find $\sqrt{S}$ depending on where $x$ lands.](figures/Babylonian-method-range){#fig:babylonian-method-range}

To see this, consider the case where $x>\sqrt{S}$ and therefore $x^2>S$. This inequality naturally also implies that $S/x^2 < x^2/x^2$, and from this we derive $S=S\frac{x^2}{x^2}>S\frac{S}{x^2}=\left(\frac{S}{x^2}\right)^2$, i.e., $S/x<\sqrt{S}$. The other case is proven similarly.

Because of this, if we start out knowing nothing about $\sqrt{S}$, it could be anywhere between $0$ and $S$, so we can make an initial guess of some $x_0$, $0<x_0<S$. If $|S-x|<\epsilon$, then $x_0$ is an acceptable output and we are done. If not, we know that $\sqrt{S}$ lies in the interval $(x/S,x)$ (if $x^2>S$) or in the interval $(x,x/s)$ (if $x^2<S$), and we can make a new guess inside that interval.

The Babylonian method for finding square roots follows this idea and work as follows:

1. First, make a guess for $x_0$, e.g. $x_0=S/2$. Any number in $(0,S)$ will do.

2. Now, repeat the following, where we denote the guess we have at iteration $i$ by $x_i$.

    1. If $|S/x_i-x_i|<\epsilon$ report $x_i$.
    2. Otherwise, update $x_{i+1}=\frac{1}{2}\left(x_i+S/x_i\right)$.
    

The test $|S/x_i-x_i|<\epsilon$ is different than the requirement we made about the error we would accept, which was $|\sqrt{S}-x_i|<\epsilon$, but since we don't know $\sqrt{S}$ we cannot test that directly. We know, however, that $\sqrt{S}$ lies in the interval $(S/x,x)$ or the interval $(x,S/x)$, so if we make this interval smaller than $\epsilon$, we have reached at least the accuracy we want.

The update $x_{i+1}=\frac{1}{2}\left(x_i+S/x_i\right)$ picks the next guess to be the average of $x_i$ and $S/x_i$, which is also the midpoint in the interval $(S/x,x)$ (for $x>S/x$) or the interval $(x,S/x)$ (for $x<S/x$), so inside the interval we know must contain $\sqrt{S}$.

**Exercise:** From this description alone you can argue that *if* the method terminates, it will report a correct answer. Prove that the algorithm is correct.

In each iteration, we update the interval in which we know $\sqrt{S}$ resides by cutting the previous interval in half.

**Exercise:** Use this to prove that the algorithm terminates.

**Exercise:** Implement and test this algorithm.


### Algorithms on lists

Lists are representations of ordered sequences of elements. These exercises involve algorithms where we have to examine or manipulate lists in order to solve a problem.

#### Changing numerical base

When we write a number such as 123 we usually mean this to be in base 10, that is, we implicitly understand this to be the number $3 \times 10^0 + 2 \times 10^1 + 1 \times 10^2$. Starting from the right and moving towards the left, each digit represents an increasing power of tens. The number *could* also be in octal, although then we would usually write it like $123_8$. If the number was in octal, each digit would represent a power of eight, and the number should be understood as $3\times 8^0 + 2 \times 8^1 + 3 \times 8^2$.

Binary, octal and hexadecimal numbers--notation where the bases are 2, 8, and 16, respectively--are frequently used in (low-level) computer science as they are good choice for describing patterns of bits as well as numbers. Base 12, called duodecimal, has been proposed as a better choice than base 10 for doing arithmetic because 12 has more factors than 10 and this system would be easier to do multiplication and division in.

In this exercise, we do not want to do arithmetic in different bases, but want to write a function that prints an integer in different bases.

When the base is higher than 10, we need a way to represent the digits from 10 and up. There are proposed special symbols for these, and these can be found in unicode, but we will use letters, as is typically done for hexadecimal. We won't go above base 16, so we can use this table to map a number to a digit up to that base:

```python
digits = {}

for i in range(0,10):
    digits[i] = str(i)

digits[10] = 'A'
digits[11] = 'B'
digits[12] = 'C'
digits[13] = 'D'
digits[14] = 'E'
digits[15] = 'F'
```

To get the last digit in a number, in base $b$, we can take the division rest, the modulus, and map that using the `digits` table:

```python
def get_last_digit(i, b):
    return digits[i % b]
```

Try it out.

You can extract the base $b$ representation of a number by building a list of digits starting with the smallest. You can use `digits[i % b]` to get the last digit and remember that in a list. Then we need to move on to the next digit. Now, if the number we are processing is $n = b^0 \times a_0 + b^1\times a_1 + b^2 \times a_2 + \cdots + b^m a_m$, then $a_0$ is the remainder in a division by $b$ and the digit we just extracted. Additionally, if $/$ denotes integer division, $n/b=b^0\times a_1 + b^1 \times a_2 + \cdots b^{m-1}a_m$. So, we can get the next digit by first dividing $n$ by $b$ and then extract the smallest digit.

If you iteratively extract the lowest digit and put it in a list and then reduce the number by dividing it by $b$, you should eventually have a list with all the digits, although in reverse order. If your list is named `lst`, you can reverse it using this function call `reversed(lst)`.

**Exercise:** Flesh out an algorithm, based on the observations above, that can print any integer in any base $b\le 16$. Show that your method terminates and outputs the correct string of digits.


#### The Sieve of Eratosthenes

> *Sift the Two's and Sift the Three's,*
>
> *The Sieve of Eratosthenes.*
>
> *When the multiples sublime,*
>
> *The numbers that remain are Prime.*


The [Sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) is an early algorithm for computing all prime numbers less than some upper bound $n$. It works as follows: we start with a set of candidates for numbers that could be primes, and since we do not a priori know which numbers will be primes we start with all natural numbers from two and up to $n$.

```python
candidates = list(range(2, n + 1))
```

We are going to figure out which are primes by elimination and put the primes in another list that is initially empty

```python
primes = []
```

The trick now is to remove from the candidates numbers we know are not primes, and we do this by elimination. I will require the following loop invariants:

1. All numbers in `primes` are prime.
2. No number in `candidates` can be divided by a number in `primes`.
3. The smallest number in `candidates` is a prime.

**Exercise:** Prove that the invariants are true with the initial lists defined as above.

We will now look as long as there are candidates left. In the loop, we take the smallest number in the `candidates` list, which by the invariant must be a prime. Call it $p$. We then remove all candidates that are divisible by $p$ and then add $p$ to `primes`.

**Exercise:** Prove that the invariants are satisfied after these steps whenever they are satisfied before the steps.

**Exercise:** Prove that this algorithm terminates and is correct, i.e., that `primes` once the algorithm terminates contain all primes less than or equal to $n$. Correctness does not follow directly from the invariants, so you might have to extend them.

**Exercise:** Implement and test this algorithm.


## Algorithmic complexity



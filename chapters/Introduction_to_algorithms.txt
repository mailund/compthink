# Introduction to algorithms

We briefly discussed what we mean by *algorithms* in the introduction, but perhaps it bears repeating—algorithms are recipes for solving a specific computational problem. They describe the steps you (or rather, your computer) must take to get from input to output and should guarantee you that if you follow the instructions exactly, you will 1) finish after a finite number of steps and 2) your output will be a solution to the problem they should solve, given the input you had.

If you think I’m being pedantic with 1), consider this program:

```python
x = 0
while True:
    x += 1
```

It computes the largest possible integer—or rather, it would if it ever finished computing. It runs forever, increasing `x` by one in each iteration of the loop, but, of course, it will never finish. Theoretically, at least, eventually the computer the program runs on will succumb to the second law of thermodynamic and stop running, but if the computer lasts for ever, it will keep running this program forever. This is not an algorithm because it takes an infinite number of steps to finish.

When we design an algorithm, the first goal is to make sure that we actually design a proper *algorithm*. That means that steps we describe for solving a problem must be finite and actually solve the problem. We say that an algorithm *terminates* if it actually finish computing in a finite number of steps (and it isn’t a proper algorithm if it doesn’t). If it also computes the right answer, we say that it is *correct*. When we design an algorithm, we must ensure both properties, which usually means proving them mathematically. With experience, you can get a little lax in formally proving this for simple algorithms, but as soon as your algorithm gets sufficiently complicated, you will revert to formal proofs, and as a beginner, it pays to do this for simple algorithms as well.

We could describe this as the algorithm for checking an algorithm:

```
Check if it terminates.
If so, check if it is correct.
```

This, funny enough, is not an algorithm, even if I have seen it described as such a few places. Of course, there aren’t sufficient details to see how we would execute either steps, but the first step cannot be solved by any algorithms at all. It is beyond this book to show you why, but there are problems that cannot be solved on any computer ever constructed, in the past or in the future, and checking if a given algorithm, however you choose to encode it as input to the computer, will terminate on any specific input is one such problem. It is known as the [*halting problem*](https://en.wikipedia.org/wiki/Halting_problem), since we also use the term *halt* for *terminates* when we consider more fundamental aspects of computation.

It is impossible to write a computer program—or generally design an algorithm—that can check these two properties of a suggested algorithm, but nevertheless, it is what you must do to show that your proposed solution to a problem is actually an algorithm. We cannot put up a formula for doing this, for very fundamental reasons, but there are some general techniques that might help you with proving the properties. These techniques are useful when you already have proposed an algorithm, but they can often also guide you in coming up with this proposal in the first place. The first part of this chapter focuses on these techniques and how they can guide you in designing an algorithm, prove that it terminates, and prove that it is correct.

Once we have an actual algorithm, we know how to solve the computational problem at hand. You can compute a solution to your problem in a finite number of steps. That number, however, can be very large. In practice, there is little difference between a program that never finishes and one that will finish in a billion years. Knowing we can solve a problem in a finite number of steps is good, but we usually need more than that. We want to solve the problem in reasonable time, where “reasonable” can depend on the problem at hand.

We have ways of reasoning about the computational complexity of both problems and algorithms that abstract away details about the actual software and hardware the algorithm will be implemented in and run on. Those techniques are the topic for the second part of this chapter. These techniques lets us compare different algorithms and decide which is best independently on how different low-level operations can be executed on any given computer and also gives us some rough ideas about whether an algorithm will run in reasonable time on any computer system you have available.

When we compare algorithms this way, we will say we compare their *efficiency*, and we generally go for efficient algorithms over inefficient algorithms, naturally, at least as long as we can implement either with roughly the same effort. We use algorithmic efficiency to determine how fast a given problem can be solved by a given algorithm.

We can also compare problems this way, and when we do, we say that we compare the problems’ *complexity*. Some problems are intrinsically harder to solve than others. The halting problem, determining if a given algorithm will ever terminate on given input, is *undecidable*, meaning that no algorithm can solve it in general. That is a very hard problem indeed. There are other problems that can be solved but, as far as we know, any solution will take unacceptable long time to do it. We call such problems *intractable*, and unfortunately many of the problems we are interested in, in may fields, are in this category. Very many optimisation problems fall into this category. I did say “as far as we know”, because for many of these problems we do not *know* if it is impossible to derive efficient solutions, and this is one of the greatest open questions in theoretical computer science, known as the [*P vs. NP problem*](https://en.wikipedia.org/wiki/P_versus_NP_problem).

Working out how difficult different problems are to solve is a discipline of theoretical computer science know as *complexity theory*, and the topic is beyond this book. Suffices to know here is that it is possible to put lower bounds on how fast it is possible to solve a given problem, and where it is important I will mention those know lower bounds.

For designing algorithms, we do not necessarily care so much about a problem’s complexity except for two reasons: we do not want to spend time attempting to find an efficient solution to a problem that does not have any efficient solutions. If we have a problem that is known to be intractable, we cannot solve it efficiently—or if we can, we have just solved the N vs. NP problem and should bask in the glory of that rather than worry about problem that led us there in the first place. Instead, we should try to rethink the problem instead. Figure out if a different problem would be equally interesting to solve. Surprisingly often, rephrasing a computational problem solves the underlying scientific problem equally well as the original phrasing, but changes the problem from being intractable to tractable. If we cannot do this, we can try to come up with algorithms that approximate a solution—they might not be the optimal solutions but can perhaps guarantee that they are within a certain fraction of optimal.

The other case where we care about complexity is when we have a problem that is tractable, i.e., we know a lower bound of how fast it is possible to solve the problem and it is not too terrible, and we want to design an algorithm that solves the problem in within that bound. When we reason about algorithmic efficiency, we usually derive upper bounds for how fast (or slow) they will run, while when we reason about problem complexity, we derive lower bounds for how long all algorithms must run, at least, to solve the problem. If we can make these two ends meet, we have an optimal algorithm. We cannot make an even faster algorithm than what we already have. Not measured the way we measure efficiency and complexity, at least. In practice, theoretical running times can be misleading for actual running times, and often, a theoretically faster algorithm, but a more complex one, can be slower on typical input than a simpler but theoretically slower one. At the end of the day, efficiency of programs are measured by how long they run on actual input, so some experimentation is needed when you start implementing algorithms.

Before we worry about algorithmic complexity and efficiency, however, we need to learn how to construct algorithms.

## Designing algorithms

### Assertions and invariants

### Exercises on Sequential algorithms

The following exercises tests that have understood algorithms and algorithmic design in sufficient detail to reason about termination and correctness and that you have enough experience with Python programming to implement simple sequential (also known as iterative) algorithms.  Here, by sequential, I simply mean that the algorithms iterate through one or more loops to achieve their goal.

For the exercises, we need some code for picking random numbers and for validating user input. That code is listed below. You do not need to understand it to do the exercises, but you do need to evaluate this code before the exercises can run.

```python
from numpy.random import randint

def input_integer(prompt):
    while True:
        try:
            inp = input(prompt)
            i = int(inp)
            return i
        except:
            print(inp, "is not a valid integer.")

def input_selection(prompt, options):
    modified_prompt = "{} [{}]: ".format(
        prompt.strip(), ", ".join(options)
    )
    while True:
        inp = input(modified_prompt)
        if inp in options:
            return inp
        else:
            print("Invalid choice! Must be in [{}]".format(
                ", ".join(options)
            ))
```

#### Below or above

Here's a game you can play with a friend: one of you think of a number between 1 and 20, both 1 and 20 included. The other has to figure out what that number is. He or she can guess at the number, and after guessing will be told if the guess is right, or if it is too high, or if it is too low. Unless the guess is correct, the guesser must try again until the guess *is* correct.

The following program implements this game for the case where the computer picks the number and you have to guess it. Play with the computer as long as you like.


```python
# When picking a random mumber, we specify the interval
# [low,high). Since high is not included in the interval, 
# we need to use 1 to 21 to get a random number in the
# interval [1,20].
n = randint(1, 21, size = 1)

# Now, repeat guessing until we get the right number.
guess = input_integer("Make a guess. ")
while guess != n:
    if guess > n:
        print("Your guess is too high!")
    else:
        print("Your guess is too low!")
    guess = input_integer("Make a guess. ")
print("You got it!")
```

Here are three different strategies you could use to guess the number:
1. Start with 1. If it isn't the right number, it has to be too low--there are no smaller numbers the right one could be. So if it isn't 1, you guess it is 2. If it isn't, you have once again guessed too low, so now you try 3. You continue by incrementing your guess by one until you get the right answer.
2. Alternatively, you start at 20. If the right number is 20, great, you got it in one guess, but if it is not, your guess must be too high--it cannot possibly be too small. So you try 19 instead, and this time you work your way down until you get the right answer.
3. Tired of trying all numbers from one end to the other, you can pick this strategy: you start by guessing 10. If this is correct, you are done, if it is too high, you know the real number must be in the interval $[1,9]$, and if the guess is too low, you know the right answer must be in the interval $[11,20]$--so for your next guess, you pick the middle of the interval it must be. With each new guess, you update the interval where the real number can be hidden and pick the middle of the new interval.

**Exercise:** Prove that all three strategies terminate and with the correct answer, i.e. they are algorithms for solving this problem.

**Exercise:** Would you judge all three approaches to be equally efficient in finding the right number? If not, how would you order the three strategies such that the method most likely to get the right number first is ranked highest and the algorithm most likely to get the right number last is ranked lowest. Justify your answer.

If you do not lie to the computer when it asks you about its guess compared to the number you are thinking of, this program implements the first strategy:

```python
for guess in range(1,21):
    result = input_selection(
        "How is my guess {}?".format(guess),
        ["low", "hit", "high"]
    )
    if result == "hit":
        print("Wuhuu!")
        break
    else:
        print("I must have been too low, right?", result)
```

**Exercise:** Implement the other two strategies and test them.

When iterating from 20 and down, for the second strategy, you should always get the result `"high"` when you ask about your guess, so you can use a `for` loop and not worry about the actual result form `input_selection`. When you implement strategy number three, however, you need to keep track of a candidate interval with a lower bound, initially 1, and an upper bound, initially 20. When you guess to high, you should lower your upper bound to the value you just guessed minus one (no need to include the guess we know is too high). When you guess to low, you must increase your lower bound to the number you just guessed plus one. In both cases, after updating the interval, you should guess for the middle point in the new interval. When you compute the middle value in your interval, you can use 

```python
guess = int(upper_bound + lower_bound) / 2)
```


#### Finding square roots

Given a number positive number $S > 0$, we want to compute its positive square root, $\sqrt{S}$. We don't need our answer to be perfectly accurate. Using floating point numbers with a finite number of bits to represent the uncountable set of real numbers prevents this anyway. However, we want to be able to put an upper bound on the error we get, $\epsilon$, such that we are guaranteed that for our result, $\hat{S}$, we have $|S-\hat{S}|<\epsilon$.

One algorithm that solves this problem is known as the *Babylonian method* and is based on two observations. The first is this, for any $x>0$, if $x>\sqrt{S}$ then $S/x<\sqrt{S}$ and if $S/x>\sqrt{S}$ then $x<\sqrt{S}$, i.e., if we guess at a value for the square root of $S$ and the guess is too high, we have a lower bound on what it *could* be, and if the guess is too low, we have an upper bound on what it could be, see [@fig:babylonian-method-range].

![Bounds for where to find $\sqrt{S}$ depending on where $x$ lands.](figures/Babylonian-method-range){#fig:babylonian-method-range}

To see this, consider the case where $x>\sqrt{S}$ and therefore $x^2>S$. This inequality naturally also implies that $S/x^2 < x^2/x^2$, and from this we derive $S=S\frac{x^2}{x^2}>S\frac{S}{x^2}=\left(\frac{S}{x^2}\right)^2$, i.e., $S/x<\sqrt{S}$. The other case is proven similarly.

Because of this, if we start out knowing nothing about $\sqrt{S}$, it could be anywhere between $0$ and $S$, so we can make an initial guess of some $x_0$, $0<x_0<S$. If $|S-x|<\epsilon$, then $x_0$ is an acceptable output and we are done. If not, we know that $\sqrt{S}$ lies in the interval $(x/S,x)$ (if $x^2>S$) or in the interval $(x,x/s)$ (if $x^2<S$), and we can make a new guess inside that interval.

The Babylonian method for finding square roots follows this idea and work as follows:

1. First, make a guess for $x_0$, e.g. $x_0=S/2$. Any number in $(0,S)$ will do.

2. Now, repeat the following, where we denote the guess we have at iteration $i$ by $x_i$.

    1. If $|S/x_i-x_i|<\epsilon$ report $x_i$.
    2. Otherwise, update $x_{i+1}=\frac{1}{2}\left(x_i+S/x_i\right)$.
    

The test $|S/x_i-x_i|<\epsilon$ is different than the requirement we made about the error we would accept, which was $|\sqrt{S}-x_i|<\epsilon$, but since we don't know $\sqrt{S}$ we cannot test that directly. We know, however, that $\sqrt{S}$ lies in the interval $(S/x,x)$ or the interval $(x,S/x)$, so if we make this interval smaller than $\epsilon$, we have reached at least the accuracy we want.

The update $x_{i+1}=\frac{1}{2}\left(x_i+S/x_i\right)$ picks the next guess to be the average of $x_i$ and $S/x_i$, which is also the midpoint in the interval $(S/x,x)$ (for $x>S/x$) or the interval $(x,S/x)$ (for $x<S/x$), so inside the interval we know must contain $\sqrt{S}$.

**Exercise:** From this description alone you can argue that *if* the method terminates, it will report a correct answer. Prove that the algorithm is correct.

In each iteration, we update the interval in which we know $\sqrt{S}$ resides by cutting the previous interval in half.

**Exercise:** Use this to prove that the algorithm terminates.

**Exercise:** Implement and test this algorithm.


### Algorithms on lists

Lists are representations of ordered sequences of elements. These exercises involve algorithms where we have to examine or manipulate lists in order to solve a problem.

#### Changing numerical base

When we write a number such as 123 we usually mean this to be in base 10, that is, we implicitly understand this to be the number $3 \times 10^0 + 2 \times 10^1 + 1 \times 10^2$. Starting from the right and moving towards the left, each digit represents an increasing power of tens. The number *could* also be in octal, although then we would usually write it like $123_8$. If the number was in octal, each digit would represent a power of eight, and the number should be understood as $3\times 8^0 + 2 \times 8^1 + 3 \times 8^2$.

Binary, octal and hexadecimal numbers--notation where the bases are 2, 8, and 16, respectively--are frequently used in (low-level) computer science as they are good choice for describing patterns of bits as well as numbers. Base 12, called duodecimal, has been proposed as a better choice than base 10 for doing arithmetic because 12 has more factors than 10 and this system would be easier to do multiplication and division in.

In this exercise, we do not want to do arithmetic in different bases, but want to write a function that prints an integer in different bases.

When the base is higher than 10, we need a way to represent the digits from 10 and up. There are proposed special symbols for these, and these can be found in unicode, but we will use letters, as is typically done for hexadecimal. We won't go above base 16, so we can use this table to map a number to a digit up to that base:

```python
digits = {}

for i in range(0,10):
    digits[i] = str(i)

digits[10] = 'A'
digits[11] = 'B'
digits[12] = 'C'
digits[13] = 'D'
digits[14] = 'E'
digits[15] = 'F'
```

To get the last digit in a number, in base $b$, we can take the division rest, the modulus, and map that using the `digits` table:

```python
def get_last_digit(i, b):
    return digits[i % b]
```

Try it out.

You can extract the base $b$ representation of a number by building a list of digits starting with the smallest. You can use `digits[i % b]` to get the last digit and remember that in a list. Then we need to move on to the next digit. Now, if the number we are processing is $n = b^0 \times a_0 + b^1\times a_1 + b^2 \times a_2 + \cdots + b^m a_m$, then $a_0$ is the remainder in a division by $b$ and the digit we just extracted. Additionally, if $/$ denotes integer division, $n/b=b^0\times a_1 + b^1 \times a_2 + \cdots b^{m-1}a_m$. So, we can get the next digit by first dividing $n$ by $b$ and then extract the smallest digit.

If you iteratively extract the lowest digit and put it in a list and then reduce the number by dividing it by $b$, you should eventually have a list with all the digits, although in reverse order. If your list is named `lst`, you can reverse it using this function call `reversed(lst)`.

**Exercise:** Flesh out an algorithm, based on the observations above, that can print any integer in any base $b\le 16$. Show that your method terminates and outputs the correct string of digits.


#### The Sieve of Eratosthenes

> *Sift the Two's and Sift the Three's,*
>
> *The Sieve of Eratosthenes.*
>
> *When the multiples sublime,*
>
> *The numbers that remain are Prime.*


The [Sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) is an early algorithm for computing all prime numbers less than some upper bound $n$. It works as follows: we start with a set of candidates for numbers that could be primes, and since we do not a priori know which numbers will be primes we start with all natural numbers from two and up to $n$.

```python
candidates = list(range(2, n + 1))
```

We are going to figure out which are primes by elimination and put the primes in another list that is initially empty

```python
primes = []
```

The trick now is to remove from the candidates numbers we know are not primes, and we do this by elimination. I will require the following loop invariants:

1. All numbers in `primes` are prime.
2. No number in `candidates` can be divided by a number in `primes`.
3. The smallest number in `candidates` is a prime.

**Exercise:** Prove that the invariants are true with the initial lists defined as above.

We will now look as long as there are candidates left. In the loop, we take the smallest number in the `candidates` list, which by the invariant must be a prime. Call it $p$. We then remove all candidates that are divisible by $p$ and then add $p$ to `primes`.

**Exercise:** Prove that the invariants are satisfied after these steps whenever they are satisfied before the steps.

**Exercise:** Prove that this algorithm terminates and is correct, i.e., that `primes` once the algorithm terminates contain all primes less than or equal to $n$. Correctness does not follow directly from the invariants, so you might have to extend them.

**Exercise:** Implement and test this algorithm.


## Algorithmic complexity


## Answers to exercises

### Above or below

```python
for guess in range(20,0,-1):
    result = input_selection(
        "How is my guess {}?".format(guess), 
        ["low", "hit", "high"]
    )
    if result == "hit":
        print("Wuhuu!")
        break
    else:
        print("I must have been too hight, right? ", result)
```

```python
lower_bound = 1
upper_bound = 20
while True:
    guess = int((upper_bound + lower_bound) / 2)
    print("{} is my guess in [{},{}]".format(
        guess, lower_bound, upper_bound)
    )
    result = input_selection(
        "How is my guess {}?".format(guess),
        ["low", "hit", "high"]
    )
    if result == "hit":
        print("Wuhuu!")
        break
    elif result == "low":
        lower_bound = guess + 1
    else:
        upper_bound = guess - 1
```


### Finding square roots

**FIXME: solution:**

```python
def babylonian_sqrt(S, epsilon):
    lower_bound = 0
    upper_bound = S
    x = upper_bound / 2
    while (upper_bound - lower_bound) > epsilon:
        x = (lower_bound + upper_bound) / 2
        if x**2 > S:
            lower_bound = S/x
            upper_bound = x
        else:
            lower_bound = x
            upper_bound = S/x
    return x

babylonian_sqrt(2, 1e-5)
```


### Changing numerical base

```python
def print_base(n, b):
    base_b = []
    while n > 0:
        base_b.append(digits[n % b])
        n //= b
    print("".join(reversed(base_b)))
```

### The sieve of Eratosthenes

```python
def eratosthenes(n):
    candidates = list(range(2, n+1))
    primes = []
    while len(candidates) > 0:
        p = candidates[0]
        candidates = [m for m in candidates if m % p != 0]
        primes.append(p)
    return primes
```

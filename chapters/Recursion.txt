# Recursion

In this chapter, we consider an immensely powerful technique for solving problems: *recursion*. Recursion involves recognising that a problem really consists of the same kind of problem, just on a smaller scale. For example, to sort $n$ elements, we can first find the smallest, then sort all the others and put them after the smallest. This is, in its essence, what selection sort does, we just didn't explain it in these terms. When we do describe the algorithm like this, the recursive part is that we sort all but the smallest object as part of sorting all the items. To sort $n$ elements, we need to sort $n-1$. This is what recursion is.

Recursion is both a way to write functions and a way to design algorithms. When used to develop new algorithms, it is also called *divide and conquer*, and we return to this in [Chapter @sec:divide-and-conquer]. In this chapter, we will focus on recursive functions.

## Definitions of recursion

Recursion means defining something in terms of itself, and you have no-doubt seen recursive definitions before, even if they were not called that. A classical example is the factorial of a number $n!$. The factorial is usually defined as this:

$$n! = \begin{cases}
 1 & n=1 \\
 n\times(n-1)! & \text{otherwise}
\end{cases}$$

The *Fibonacci* numbers can be defined as this:

$$F(n) = \begin{cases}
0 & n = 0 \\
1 & n = 1 \\
F(n-1) + F(n-2) & \text{otherwise}
\end{cases}$$

The pattern we see for these definitions is that we define the value for a number, $n$, as either a fixed constant or some expression that involves the function we are defining itself, applied to some smaller number. The cases where we can get a result immediately from the definition are called *base cases*. For factorial, the base case is $n=1$ where we directly get the value 1. For Fibonacci numbers, the base cases are $n=0$ and $n=1$, where we get the values 0 or 1 right away. The other cases are called the *recursive cases*.

One way of defining natural numbers is also recursive. A number $n$ is a natural number if it is zero or if $n-1$ is a natural number.

For a recursive definition to be well-defined, the recursive cases must bring us closer to base cases, so we do not have an infinite regression that never brings us to termination. And when I say termination here, it is not unlike the issue with the termination of algorithms; it is the same problem, just in a different disguise. We cannot take an arbitrary recursive function and an input and determine if the function will evaluate to a function or go infinitely deep in recursions. Just as we cannot take a general program and its input and decide if it halts. They are two sides of the same coin. We must take some care in defining recursive functions to avoid this, just as we must take care to ensure that our algorithms terminate. In the case of recursions, we have a natural termination function---how far a function call is from a base case. If we can show that each recursive call brings us closer to a base case, then we are in the clear.

Because recursion is self-referential---it solves a smaller instance of the problem it is solving---comic definition of recursion is this:

>	Recursion, *see Recursion.*

The more useful definition of recursion is definitions where we have one or more base cases and one or more formulae that covers all other cases with references to the definition itself.

Strictly speaking, this would be a definition of *recursive definitions*, but it works equally well when we consider computational problems. We have a recursive algorithm when we have some base cases that we can handle directly, and some rules for solving all other cases by resolving the same problem on smaller parts of the input.

## Recursive functions

We used recursion several times in [Chapter @sec:searching-and-sorting] even though we never called it that. Consider the linear search algorithm. When we search through a list `x`, if we have reached the end of the list without finding the element we are searching for, we are done and can report that the object is not in the list. Otherwise, we look at the first item in the list, and if that is the item that we are looking for we are done and can report that we found it. These two cases are the base cases. Otherwise, we do a linear search in the remainder of the list. That is the recursive case.

We can make the recursive nature of linear search more explicit by defining a *recursive function:*

```python
def linear_search(x, e, i = 0):
	if i == len(x):
		return False
	if e == x[i]:
		return True
	else:
		return linear_search(x, e, i + 1)
```

This does exactly what we described above. There are two base cases and one recursive case. For the function, the recursive case is handled by the function calling itself.

This version of linear search is, unmistakably, much more complicated than the one we had before, and I do not recommend that you use it instead of iterating through `x`. You should, however, be able to convince yourself that it does the same thing.

The only difference between a recursive definition and a recursive function in Python is that the former defines something while the latter actually computes something. There is no other difference. For example, we can write a function for computing—as opposed to defining—the factorial of a number like this:

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

**Exercise:** Implement a recursive function that computes the $n$’th Fibonacci number.

Binary search is another example of a recursive algorithm. In this algorithm, we either have an empty interval to search in, in which case we can report `False`. Or, we have the object we are searching for right in the middle of the range we need to explore, in which case we can report `True`. If all else fails, we have the recursive case, we continue our search in either the lower or, the higher half of the range.

Again, we can be more explicit in defining this as a recursive computation by implementing it as a recursive function:

```python
def bsearch(x, e, low = 0, high = len(x)):
	if low >= high:
		return False
	mid = (low + high) // 2
	if x[mid] == e:
		return True
	elif x[mid] < e:
		return bsearch(x, e, mid + 1, high)
	else:
		return bsearch(x, e, low, mid)
```

You should convince yourself that this, indeed, does the same as the binary search we have seen earlier.

If you recall, we required of recursive definitions that the recursive cases must move us closer to base cases and observed that this was related to termination. If each recursive call moves us closer to a base case—whatever that means—then the computation will eventually terminate. If not, then there is no such guarantee. You should think about recursive functions as more general termination functions and prove that they reach a base case for all input.

For binary search, the termination function was `high - low`. This works equally well for the iterative version we have seen earlier as for the recursive function defined above. We didn’t use a termination function for our earlier implementation of linear search; we didn’t need one because we know that a `for`-loop over a finite sequence will terminate. For the recursive case, we cannot make as simple an argument, but of course, the situation is the same. In each recursive call, the index `i` gets closer to the end of `x`. So, we can use as termination function `len(x)-i`.

Some people find recursion a challenging concept, primarily when we use recursion for computation. Most people do not have any problem with accepting recursive definitions, but when we solve a problem by solving the exact same problem, it feels like a circular definition. “For recursion, see recursion”. It isn’t, however, and the reason is that we never solve a problem by trying to solve exactly the same problem. We solve a problem that is closer to a base case; our termination function decreases with every recursive function call.

If you still find recursive functions hard to wrap your mind around, you might take comfort in knowing that many early computer scientist did as well. Early programming languages could not define recursive functions. It just didn’t occur to people that this might be useful. This doesn’t mean that they didn’t solve problems recursively; they just didn’t use recursive functions. Just like we did the linear and the binary search without recursive functions before we reformulated the algorithms as recursive. Recursion is such a powerful technique, however, that all modern languages support it. Some even go so far that they have replaced loops with recursion entirely; they will not let you implement loops at all, only recursive functions.

## Recursion stacks

To understand how recursive functions work, we first need to understand a little deeper how function calls work. Recall that we have two different kinds of variables in Python, global and local variables.^[There are a bit more to variables than this, but we do not need to worry about that for now. Global and local variables are all we have seen so far.] Global variables are those we assign to at the outermost level in a Python program while local variables are either function parameters or variables we assign to inside a function. For example, in this program:

```python
x = 2
def add_one(x):
	return x + 1
add_one(2 * x)
```

We have two global variables, `x` and `add_one`. The `x` variable is one we have defined by assigning two to it. The `add_one` variable is the result of a function definition. Inside the `add_one` function we have another variable named `x`, we have created it by making a function parameter. This variable is distinct from the global variable `x`; they have the same name but can refer to two different objects. Inside `add_one`, `x` is a local variable; outside of `add_one`, `x` is a global variable. When we call `add_one(2 * x)` we first look up what the variable `add_one` refers to and finds the function. Before we can call the function, we must evaluate the expression that will be its argument, that is, we must evaluate `2 * x`. Since we call `add_one` at the outermost level, the global scope, `x` is the global variable. It refers to the value two, so `2 * x` is four. We then call `add_one` the parameter, which is the local variable `x`, will then refer to four. This does not change the global variable, that variable still refers to two. When we return `x + 1`, we use the local variable, which refers to four, so we return five.

You should be comfortable with the difference between global and local variables by now, but what happens when a function calls itself, as we did with the factorial function.

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

When we call 

```python
factorial(4)
```

the local variable, `n` will be set to refer to four. There is no problem there. But then we call `factorial` with `n - 1` to evaluate `n * factorial(n - 1)`. This makes the local variable refer to three, but we still need it to refer to four when, after the recursive call, we multiply the result of `factorial(n - 1)` by `n`. We need the local variable to refer to both four and three in the recursive case. How is that achieved?

This is something early programming languages usually couldn’t handle. In these early languages, local variables were tied to functions—each function had memory locations reserved for their local variables. Those were modified when calling a function and by updating variables inside the function body. If you called the same function twice, you would overwrite the memory locations assigned to the local variables. This isn’t a problem if you do not use recursion, but it apparently is if you do. You cannot use the same memory location to hold both $n=4$ and $n=3$.

The solution to this problem is beautiful in its simplicity: instead of reserving fixed memory locations for local variables for each function, you reserve memory for the local variables in each function *call*. The memory locations you need for local variables are not hardwired in the program but will be allocated when you call a function. This memory is allocated on a so-called *call stack*, and it works as follows. You have a piece of your computer’s memory set aside for this stack. You keep track of the top of the stack, and the function call you are currently executing has access to the top of the stack, and any global variables, but not the rest of the stack. When you call a function, you reserve memory for the variables used in the function at the top of the stack. The memory set aside in this way is known as a function’s *call frame*, but it isn't that important what we call it; what is essential is that we have it.^[The way Python handles global and local variables, and how it handles call frames, is a bit different from what I explain below. Python runs in a so-called *virtual machine*, which is a program that understands Python better than the raw hardware does and that works as an intermediate between the two. The virtual machine runs as actual machine instructions on the CPU and Python runs on the virtual machine. My description of how calls work is almost correct, and all programming languages will do something similar to it, but the details can vary. For the details of what Python really does, I refer to [Chater @sec:python-vm].]

Consider the call:

```python
factorial(4)
```

If we make this call at the outermost level, the call stack is empty. When we make the function call, Python puts two things on the call stack and increases the call stack pointer to point to the top of the stack. The function will know where to get to the local variables relative to the top of the stack. Not the direct memory addresses that hold them but where they can be found below the stack pointer. Knowing the absolute memory addresses is what early programming languages would do, and it fails if we need to have more instances of function calls at the same time. Knowing the offset below the stack pointer instead resolves this problem.

When we return from a function call, we also need to know where to return to. A function can be called from different places in your program, and we need some way of knowing where a function was called from so we can return to that location in the program. We cannot use a fixed memory location to hold this information any more than we can use fixed locations for local variables, so we need put this information on the stack as well.

When we call recursively, Python will put the information it needs for each function call—local variables and the return location—onto the stack and grow it as needed, see [@fig:growing-call-stack].

![Growing the call stack in recursive calls to `factorial`.](figures/growing-call-stack){#fig:growing-call-stack}

When we reach $n=1$, we hit a base case and immediately return one as the result. Now, Python needs to do two things. It needs to provide the return value of the function to its caller, and it t needs to continue executing the program from the point where the function call was made. The caller location is found in the call stack, so we can always determine where the program needs to jump to. What about the return value, though?

We hadn't thought about where Python stores result of expressions before; we just assumed that it could remember them somehow. But of course, on a computer, you need to store intermediate results of expressions somewhere. Since results of (local) expressions are local to a function call, the only appropriate place is on the stack.[^python_local_values_details] 

[^python_local_values_details]: The way Python actually deals with local values, and how it deals with the results of local expressions differ from the explanation I give here. Python does use a stack for this, it actually uses several, but that is beyond the scope of this chapter. It is the topic of [Chapter @sec:python-vm]. Programs that run on the raw hardware on your computer do not just use a stack. It is more efficient to put values in a CPU's registers than put them in memory, and that is what is usually done. Python has a layer between the raw hardware and your programs, the virtual machine. The virtual machine runs on the actual hardware, and it uses registers to hold temporary values and for passing values between functions. Your program, on the other hand, runs on the virtual machine, and this machine uses a different approach. If your Python programs actually ran on the real CPU, the description here wouldn't be far off of what would actually be happening with function calls.

When we return a value from a function, we need to put that value on the stack. We cannot make assumptions about where our function is called from so we cannot use the caller's call frame. Instead, what is commonly done, is to pop the current call frame off the stack, which means we decrease the stack pointer to where it was before the function call, and then put the result of the function call just one position above the new position of the stack pointer. To be able to do this, it needs to remember the return position if it overwrites that memory location, or it needs to allocate space on the stack for the return value. Let us just assume that it is able to return and also put the function result-value in its call-frame.

Since we write the return value on the call-frame of the function, we do not muck about in the caller's call frame, and the caller knows where to get the result. When we return from a recursion, we pop call frames from the stack, put the return values above the stack (overwriting the call frame), and keep doing this until we leave the recursion, see [@fig:shrinking-call-stack]. 

![Shrinking the call stack when returning from recursive calls to `factorial`.](figures/shrinking-call-stack){#fig:shrinking-call-stack}

If the calling function wants to remember the returned value, it needs to move it somewhere else before it can make further function calls. If it only calls a function for its side-effects, it can ignore any value the called function might return, but if it needs to use the result, it must remember it. If it calls another function, the call frame of this function will be written on the stack, overwriting the returned value. We do not call other functions before we have used the result of the recursive call for `factorial`, but we do multiply a number with the result of a call. The simplest way to handle this is to save the result of the recursion and then handle the multiplication as we would handle any temporary result in an expression. With the factorial function, we can make the optimisation where we use the returned value direction. We cannot always do this, however. For example, when computing Fibonacci numbers, we need to make two recursive calls and then add their returned values together. We need somewhere to store the result of the first recursive call so we can add it to the result of the second call. In general, we might need to store the result of many function calls before we can use them in a local expression. Everything we need to save for later, we need to move to a location in the current call frame before we call another function.

The call stack is not only used for recursion. It works the same way with all function calls. Consider this program:

```python
def add_one(x):
	return x + 1

def add_two(x):
	return add_one(add_one(x))

add_two(1)
```

[@Fig:call-stack-add-two] shows the call stack when evaluating this program. The call to `add_two` will put a return location and the local variable `x` on the stack. It also allocates space for the temporary value that it gets from the first call to `add_one` so it can remember this for the second call. The `add_two` function then calls `add_one`. This call will put a return location and another `x` on the call stack. Once the call frame is set up, `add_one` will do its calculations, move the stack pointer down to where it was before the call, in effect poping the call frame from the stack, it will put the result of the function call just above the stack pointer, and then return to whence it was called.

![Call stack when evaluating `add_two(1)`.](figures/call-stack-add-two){#fig:call-stack-add-two}

Now, the `add_two` function needs to call `add_one` once more. Since this call would overwrite the result of the first call, we need to save this value. We do this by moving the value from just above the stack pointer to the memory cell we allocated for this very purpose. We can then set up the call frame for the second call to `add_one`, make this call, and grab the result. This result is also the result of the `add_two` call, but we cannot simply use this result. We need to pop the call frame from the stack, which means we also need to move the `add_one` result down to where the caller of `add_two` expects to see it. 

This is example does not involve recursive calls, but the principle behind all function calls is the same. We might be able to optimise a specific combination of calls. For `add_two` we might manage to avoid moving the return value of the first call to `add_one` but simply putting it where it is needed for the second call; or, we might be able to put the return value from the second call to `add_one` where it is needed when we return from `add_two`. Such optimisation would only work for particular combinations of calls, however, while the operations we did in the example would work for all combinations of functions.

**Exercise:** Draw the call stack for computing the fourth Fibonacci number using the implementation you did in the previous exercise.

All this call stack business can look complicated, but the good news is that you never need to worry about it unless you implement recursion yourself. The call stack is the reason why recursive function calls are possible, but you only need to know that Python can handle them.

Well, you *almost* never have to worry about the call stack. You might run into problems with a call stack if you fill it up. For example, if you do something like this:

```python
def f(x):
	return f(x + 1)
f(1)
```

This example results in an infinite recursion; the function doesn't even have a base case. You will not get an infinite recursion if you execute the code, however, instead you will get an error that says “RecursionError: maximum recursion depth exceeded”. This happens because the memory set aside for the call stack is limited and if the recursion gets too deep, there is no more stack available for more call frames. 

For infinite recursions, we wouldn't expect something meaningful anyway, but there are many cases where there *is* a result of a recursion, but you do not have sufficient stack space. Linear or binary search, if implemented recursively, could run into this problem if they are called on sufficiently many elements.

You will not run into this if you use iteration instead of recursion. It is certainly possible to write an infinite loop, but you will not run out of stack-space as a result. Loops are also more efficient than recursive calls because the latter needs to handle call frames while the former does not. If you can implement an algorithm using iteration just as easily as you can do with recursion, you should always prefer the looping version. If you do need recursion, but the recursion is deeper than your call stack allows, you have no choice but to use a loop. There are general ways of handling recursion that avoids filling up the call stack (and we will return to this in [Chapter sec:return-to-functions]), but for some recursive functions, it is particularly easy. These are *tail-recursive* functions. We get to those in [@sec:tail-recursion]. First, though, we will consider the relationship between iteration and recursion in more detail.

## Recursion and iteration {#sec:recursion-and-iteration}

There is a close correspondence between recursion and iteration. You can always, directly, translate a loop into a recursive function; you cannot necessarily do the opposite. There just are more things you can do with a stack than you can with a loop.^[Obviously, you can implement the stack functionality yourself and then avoid recursive function calls, but this is just an implementation detail. Conceptually, will still be using recursion and not a simple loop.] 

Just because you can implement all loops as recursive function calls doesn’t mean that you should. Besides the problems with exceeding the stack limit, there is a substantial overhead in calling functions, so a recursive solution to a problem will always be slower than one that relies on loops.^[Programming languages that do not allow loops but only recursive function calls actually translate recursion into loops under the hood whenever possible. Python does not do such optimisations of recursive functions, so you should always prefer loops over recursions when you can.] If you can solve a problem directly using loops, you should do that. Sometimes, however, it is easier first to derive a recursive solution to a problem and then translate it into a loop if possible. Divide and conquer algorithms ([Chapter @sec:divide-and-conquer]) are more naturally constructed in terms of recursion, but can still often be implemented using loops.

The purpose of this section is not to convince you that recursion is a better tool than iteration. If we can implement an algorithm using loops, then that is the better choice. When we cannot, then recursion is the only choice. We see examples of this when we discuss divide and conquer algorithms. The purpose of this section is to get you familiarised with the relationship between recursion and iteration. In [@sec:tail-recursion] we consider simple recursive functions that we can translate into loops. In this section, we will translate looping functions into recursive ones. 

We will take a problem with an iterative solution and translate it into a recursive one. The result, I hope you will agree, is more straightforward than the iterative solution. This is why we usually prefer to develop an algorithm using recursion before we consider iteration. We do the translation in the opposite direction here. We start with loops, which you are already familiar, and show you how these loops can be simplified using recursion. The only reason for this is to get from something familiar, loops, and see how it relates to something new, recursion. As a side-effect, I also hope to convince you that recursive solutions can be much more straightforward than iterative ones.

The problem we will consider is that of merging two lists. This was an exercise in [Chapter @sec:algorithmic-efficiency]; if you haven't solved it already, give it a go before continuing.

The problem we need to solve is this: given two sorted lists, `x` and `y`, we want to create a list that contains the same elements as in `x` and `y`, combined, in sorted order.

One implementation could look like this:

```python
def merge(x, y):
	result = []
	i, j = 0, 0
	while True:
		if i == len(x):
			# no more elements in x
			while j < len(y):
				result.append(y[j])
				j += 1
			return result
		if j == len(y):
			# no more elements in y
			while i < len(x):
				result.append(x[i])
				i += 1
			return result
		if x[i] < y[j]:
			result.append(x[i])
			i += 1
		else:
			result.append(y[j])
			j += 1
```

The function doesn’t do anything complicated, but it is rather long. It can, therefore, be hard to see, at a glance, what it is doing. It is simple enough: We move through `x` and `y`, using the indices `i` and `j`, pick the smallest of `x[i]` and `y[j]`, and append that to our result. If we have made it to the end of either `x` or `y`, the first two `if`-statements in the loop, we copy the remainder of the other list.

A much simpler implementation of the same idea is this:

```python
def merge(x, y):
	if len(x) == 0:	return y
	if len(y) == 0:	return x
	if x[0] < y[0]:
		return [x[0]] + merge(x[1:], y)
	else:
		return [y[0]] + merge(x, y[1:])
```

Here we can directly see the two base cases and the recursive case—the recursive case is one of two recursive calls, depending on which list has the smallest element. The most straightforward recursive solution is usually *much* simpler than an iterative solution. It is often also a lot less efficient, even if we ignore the function call overhead.

For reasons that I have not explained yet, slicing to get everything except the first element of a list, as we do when we call `x[1:]` and `y[1:]`, is an expensive operation. It takes time proportional to the length of the lists (minus one). There is another implementation of lists than the one Python uses, where this would be a constant time operation, but for Python `list` objects it is not. Therefore, the recursive call takes time $O(n)$, where the lengths of `x` and `y` are in $O(n)$, plus how long it might take to compute the recursive function, and the result is an $O(n^2)$ running time all in all. I won’t go into details about why we get this running time since we cover that in [Chapter @sec:divide-and-conquer]. 

The iterative implementation runs in time $O(n)$—if you cannot see why immediately, try to work through the analysis. The recursive implementation runs in $O(n^2)$. This isn't a great advertisement for recursion.  We generally do not want to trade efficiency for simplicity.

We can get rid of the expensive slicing of the first elements by reintroducing the index variables and get this implementation:

```python
def merge(x, y, i = 0, j = 0):
	if i == len(x): return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return [x[i]] + merge(x, y, i + 1, j)
	else:
		return [y[j]] + merge(x, y, i, j + 1)
```

Unfortunately, this isn’t much better. We avoid the slicing, but concatenating two lists, as we do in the recursive case, is also an $O(n)$ operation, so we end up with the same $O(n^2)$ running time.

To avoid both concatenation and slicing, we can do this:

```python
def merge(x, y, i = 0, j = 0, result = []):
	if i == len(x):
		# no more elements in x
		while j < len(y):
			result.append(y[j])
			j += 1
		return result
	if j == len(y):
		# no more elements in y
		while i < len(x):
			result.append(x[i])
			i += 1
		return result
	if x[i] < y[j]:
		result.append(x[i])
		return merge(x, y, i + 1, j, result)
	else:
		result.append(y[j])
		return merge(x, y, i, j + 1, result)
```

This leaves us pretty much back where we started. We now have a recursive solution to the problem, but it is just as complex as the iterative function we started with.

All is not lost, however. We can reconsider why we couldn’t use the simpler recursive solutions. The main problem here was concatenation—we could avoid the slicing simply by using indices. Maybe we can avoid the concatenation in some other way? Indeed we can:

```python
def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		res = merge_rec(x, y, i + 1, j)
		res.append(x[i])
		return res
	else:
		res = merge_rec(x, y, i, j + 1)
		res.append(y[j])
		return res

def merge(x, y):
	return list(reversed(merge_rec(x, y)))
```

Since prepending one element to a list involves a concatenation operation, which is expensive, we replace it with an append operation, which is cheap.[^justification_for_list_complexity] We construct the merged list in the reversed order, so we need to reverse it to get the right order once we are done with the merge. I have split this into two functions, one that recursively constructs the reversed result and one that reverses it to get the result in the right order. In the basis cases we still slice a list, but since this takes time proportional to the length of the list, it is not slower than the `while`-loops we used earlier to do the same thing. In fact, it is likely to be faster since slicing is a built-in operation in Python and implemented very efficiently.

[^justification_for_list_complexity]: Why appending is cheap while prepending is expensive comes down to how lists are implemented by Python. We return to this in [@sec:dynamic-arrays]. For now, you just have to take my word for it.

This solution is not quite as simple as the first recursive function, but we can make it almost so by moving the append operation to a function:

```python
def app(lst, x):
	lst.append(x)
	return lst

def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return app(merge_rec(x, y, i + 1, j), x[i])
	else:
		return app(merge_rec(x, y, i, j + 1), y[j])
```

This solution is almost as simple as the first recursive function, and it runs in $O(n)$; we have replaced the expensive operations in the first recursive function with constant time operations in this function. It is not as efficient as the iterative function. Function calls are constant time operations but expensive ones, and we use those extensively here. We also need to reverse the result, which adds additional computations. The iterative solution avoids any function call beyond the call to the `merge` function and directly construct the result list.

It is the simplicity of the recursive solution, compared to the iterative version, that makes it easier to construct recursive algorithms. Once we have a recursive solution, we usually then want to replace it with an iterative solution. In the next section, we will start with a recursive solution and use that to guide us to an iterative solution. Usually, we end up with an implementation very similar to what we would get if we set out to implement an iterative solution in the first place, but starting with a recursive solution and then modifying it, step by step, until we have an efficient iterative solution makes the programming task more manageable. As an added benefit, it also makes it easier to test our implementation; we can use the simplest solution to check the more complicated solutions against (see [Chapter @sec:testing] for more details on approaches to testing). As we, stepwise, transform a function to make it more efficient, we can test each rewrite. Since it is easier to get a simple solution correct, and since it is easier to make incrementation changes to a working implementation than it is to construct a function from scratch, implementing efficient solutions using this approach is a good strategy. Building an efficient function by a series of changes from a simple to an efficient one is more often than the most effective way to get a fast and correct solution to a problem; aiming directly for a sophisticated, efficient solution is usually not a practical approach.

Before we get to translating recursive functions into iterative ones, however, you should do some exercises to test that you have understood recursion.

**Exercise:** To compute the sum of the elements in a list, we can obviously do this iteratively:

```python
result = 0
for e in x:
  result += e
```

Implement a recursive function that computes the sum of the elements of a list.

**Exercise:** We can find the smallest element in a non-empty list, `x`, like this:

```python
smallest = x[0]
for e in x:
	smallest = min(smallest, e)
```

Write a recursive function for finding the smallest element in a list. To avoid copying the list using slices, you will want to have the function take an index parameter as an argument.

**Exercise:** Modify your function, so it returns `None` if the list is empty. The easiest way to do this is probably to include the “smallest element seen so far” as a parameter to the function, with a default value of `None`. To compute the smallest value and still handle `None` you can use this function:

```python
def my_min(x, y):
	return y if x is None else min(x, y)
```

**Exercise:** Write a recursive function that reverses a list.

**Exercise:** Recall the exercise where you had to translate a base-10 number into some other base $b$ (where we restricted the base to be less than 16). We can get the last digit of a number $i$, in base $b$ using this function:

```python
def get_last_digit(i, b):
    return digits[i % b]
```

where we defined the `digits` list as

```python
digits = {}

for i in range(0,10):
    digits[i] = str(i)

digits[10] = 'A'
digits[11] = 'B'
digits[12] = 'C'
digits[13] = 'D'
digits[14] = 'E'
digits[15] = 'F'
```

We can then reduce the problem to the second-to-last digit by dividing $i$ by $b$. Implement this idea using a recursive function.

## Tail-recursion {#sec:tail-recursion}

You can always translate an iterative algorithm into a recursive one, but since iterative algorithms are more efficient, you shouldn’t do this. Recursive algorithms are more straightforward, though, so you will often find yourself in the situation that you have an elegant recursive solution to a problem, and you want to translate it into a more efficient iterative solution. You cannot always translate a recursive function into an iterative one,[^iterative_is_always_possible] but this section is about the cases where you can.

[^iterative_is_always_possible]: In the interest of complete honesty, I am lying here. You can always translate a recursive function into an iterative one. It just requires some tricks we are not ready for here. I will show you how it can be done in [@sec:thunks_and_trampolines]. The general solution, however, usually isn’t a particularly good idea. It is not efficient and more often than not you get a more efficient solution by implementing your own stack as a substitute for the recursion stack. If speed is not an issue, but stack size is,  the general translation from recursion to iteration is more straightforward and therefore the prefered solution.

Functions that can always be translated into loops are so-called *tail-recursive* functions. In many programming languages, tail-recursive functions are automatically translated into loops—this is called the *tail recursive optimisation*—but Python is not one of them. Not to worry, though, the translation is so simple that you can always do it by hand with minimal effort.

A function is *tail-recursive* when the recursive case only consists of a recursive call. Consider the factorial function:

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

The recursive case involves a recursive call but not as the single result of the recursive case. We make a recursive call, and then we multiply the result with `n`. Because we have to multiply the result of the recursive call with `n`, the function is not tail-recursive. We can translate it into a tail-recursive function by adding an accumulator to the function:

```python
def factorial(n, acc = 1):
	if n == 1:
		return acc
	else:
		return factorial(n - 1, n * acc)
```

The accumulator handles the multiplication with `n`. We multiply the accumulator by `n` as we go down the recursion rather than multiply the result of recursive calls by `n` when we return from the recursion. Functions that only involve a single recursive call in the recursive case can always be translated into tail-recursive versions by adding an accumulator.

**Exercise:** Rewrite your recursive function for computing the sum of a list of numbers such that it becomes tail-recursive.

**Exercise:** Rewrite your recursive function for finding the smallest element in a list to a version that is tail-recursive.

With tail-recursive functions, we do not need to do anything with the result of the recursive call. Not doing anything with the result of a recursive call is just another way of saying that a function is tail-recursive. The reason that this is important is that, when we do not need to do anything after the recursive call, then we can reuse the call frame for the recursive call. We can directly update the local variables to those we would use in the recursive call frame and go from there; we can replace a recursive call with a simple update of the function argument local variables.

When we call a function we assign values to the function arguments. If we have a tail-recursive function, we can directly update the arguments we already have and then start executing the function body from the beginning again. If we wrap the function body in one big `while True` loop, we can replace the recursive function call with an update to the function arguments and then `continue` the loop. If the recursive case is put at the end of the loop, we do not even need to `continue`; we are at the end of the loop, so we return to the beginning right after we update the variables.

For the factorial function, this transformation gives us:

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		n, acc = n - 1, n * acc
```

If you split the variable updates over multiple statements, you have to be careful about the order. When you update a variable, you affect expressions that depend on it. So you have to update the variables in the right order.

This will work

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		acc = n * acc
		n = n - 1
```

This will not:

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		n = n - 1
		acc = n * acc
```

A parallel-assignment, as we did for the first iterative implementation of the factorial function, will usually work. If we never do any operations with side-effects, i.e., whenever we need to update a data structure such as a list, we create a new one instead, then parallel-assignment will work. If we actually modify a data structure, we cannot use parallel assignment so we must be careful that we perform the update operations in the same order as they would have been performed in a function call.

**Exercise:** Do this transformation for your tail-recursive summation function.

**Exercise:** Do this transformation for your tail-recursive “find minimum” function.

**Exercise:** Consider our recursive implementation of binary search:

```python
def bsearch(x, e, low = 0, high = len(x)):
	if low >= high:
		return False
	mid = (low + high) // 2
	if x[mid] == e:
		return True
	elif x[mid] < e:
		return bsearch(x, e, mid + 1, high)
	else:
		return bsearch(x, e, low, mid)
```

This function is tail-recursive, so use the transformation to replace it with a loop. Compare it to the iterative solution we considered before this chapter.

To see a more complex case of using an accumulator in a tail-recursive function, and then translate it into an iterative function, we can return to the problem of merging two lists. We left this problem with this recursive implementation:

```python
def app(lst, x):
	lst.append(x)
	return lst

def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return app(merge_rec(x, y, i + 1, j), x[i])
	else:
		return app(merge_rec(x, y, i, j + 1), y[j])

def merge(x, y):
	return list(reversed(merge_rec(x, y)))
```

The reason we had to construct the merged list in reverse order, and then reverse it when we are done, was actually because we didn't use an accumulator. If we add an accumulator, we can build the merged list in the right order:

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	if i == len(x):	return acc + y[j:]
	if j == len(y):	return acc + x[i:]
	if x[i] < y[j]:
		return merge(x, y, i + 1, j, app(acc, x[i]))
	else:
		return merge(x, y, i, j + 1, app(acc, y[j]))
```

The way we handle the default value of the accumulator might look a bit weird, but it is crucial. If we set the default value of `acc` to an empty list, each call to `merge` that rely on the default parameter will get the *same* list. This means that if you call `merge` twice, the result of the first call will still be in the accumulator, and the new merge will be appended to it. This is not what we want, and it is because of this that we handle the default parameter the way we do.^[A slightly more *Pythonic* way of writing the `if` statement would be to use the `or` operator instead. You could write `ac = acc or []`. Because `or` will return the second argument if the first is false, and because `None` is interpreted as `False` in this context, you will get `[]` when `acc` is `None`. You will also get a new empty list if `acc` is an empty list because empty lists are also interpreted as `False`. We never recurse on an empty list, so that is not a problem.]

This function is tail-recursive so we can translate it into a looping version. The `app` function simply append its second argument to its first, and it does this before the recursive call (because function arguments are evaluated before a function is called). Because of this, we can get rid of it and simply append instead. We have to be careful to append before we update the indices, though. The rewritten function looks like this:

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	while True:
		if i == len(x):	return acc + y[j:]
		if j == len(y):	return acc + x[i:]
		if x[i] < y[j]:
			acc.append(x[i])
			i += 1
		else:
			acc.append(y[j])
			j += 1
```

If you want to avoid copying the accumulator in the base cases, you can use the `extend` method on the accumulator list.  Using `extend` and a slice on one of the input lists is unlikely to be slower than a `while`-loop where we move individual elements, since `extend` and slice are builtin operations and therefore highly optimised.

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	while True:
		if i == len(x):
			acc.extend(y[j:])
			return acc
		if j == len(y):
			acc.extend(x[i:])
			return acc
		if x[i] < y[j]:
			acc.append(x[i])
			i += 1
		else:
			acc.append(y[j])
			j += 1
```

If in the first iterative solution, we used the expend method as well, we would have this solution. So, we end up back where we started. Hopefully, we have learned something along the way.

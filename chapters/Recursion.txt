# Recursion and “divide and conquer”

In this chapter we explore two powerful techniques for designing algorithms: *recursion* and *divide and conquer*. These are really two sides of the same coin, and can be described as “solve a problem by breaking it down to smaller subproblems that we can solve recursively”. The first part is how we would describe all algorithms. The “recursively” part is where these techniques differ—and I am aware that I am defining *recursion* using *recursively*, which is ironic considering what recursion is. We get to that in the next section. Purist might consider the two terms as different, considering recursion a programming construction where a function will call itself and consider divide and conquer the algorithmic trick of solving a problem by splitting into subproblems of the same kind—perhaps using recursive functions and perhaps not. I will not make this distinction, but I will present the idea of recursion first and then classical divide and conquer algorithms after that.

## Recursion

Recursion means defining something in terms of itself, and you have no-doubt seen recursive definitions before, even if they were not explained as such. A classical example is the factorial of a number $n!$. The factorial is usually defined as this:

$$n! = \begin{cases}
 1 & n=1 \\
 n\times(n-1)! & \text{otherwise}
\end{cases}$$

The *Fibonacci* numbers can be defined as this:

$$F(n) = \begin{cases}
0 & n = 0 \\
1 & n = 1 \\
F(n-1) + F(n-2) & \text{otherwise}
\end{cases}$$

The pattern we see for these definitions is that we define the value for a number, $n$, as either a fixed constant or some expression that involves the function we are defining itself, applied to some smaller number. The cases where we can get a result immediately from the definition are called *base cases*. For factorial, the base case is $n=1$ where we can immediately get the value 1. For Fibonacci numbers, the base cases are $n=0$ and $n=1$, where we immediately can get the values 0 and 1, respectively. The other cases are called the *recursive cases*.

One way of defining natural numbers is also recursive. It goes like this: 0 is a natural number and whenever $n$ is a natural number then so is $n+1$. This definition looks like it works in the opposite direction as the others, since it goes from $n$ to $n+1$, but looks can be deceiving. We can rewrite the recursion

$$\begin{cases}
n\in\mathbb{N} & n=0 \\
n\in\mathbb{N} & n-1\in\mathbb{N}
\end{cases}$$

For a recursive definition to be well-defined, the recursive cases must express a value in terms of recursions on values that are closer to the base cases, in some order. We cannot define the natural numbers like this:

$$\begin{cases}
n\in\mathbb{N} & n=0 \\
n\in\mathbb{N} & n+1\in\mathbb{N}
\end{cases}$$

If we want to check if 4 is a natural number, we would first need to check if 5 is, to answer that we need to consider 6, and so forth to infinity. By defining natural numbers in the original way, we would check 4, then 3, then 2, then 1, and then reach a base case, 0, after which we would have our answer. Recursive definitions are self-referential but must be so in a way where recursive cases move you closer to base cases.

Because recursion is self-referential in this way, a comic definition of recursion is this:

>	Recursion, *see Recursion.*

The more useful definition of recursion is definitions where we have one or more base cases and one or more formulae that covers all other cases with references to the definition itself.

Strictly speaking, this would be a definition of *recursive definitions*, but it works equally well when we consider computational problems. We have a recursive algorithm when we have some base cases where we can get the solution immediately, and some rules for solving all other cases by solving the same problem on smaller parts of the input.

### Recursive functions

You have already used recursion, defined in this way, several times in the previous chapters. We just didn’t call it recursion then.^[Again, purists would not consider these algorithms recursive. They would reserve that word for when we use recursive functions. I would argue that they *are* recursive algorithms. I would actually argue that *most* algorithms are recursive; it is not always helpful to think of them as such, but recursion is often the way to think about solving a problem in the first place.] Consider linear search. When we search through a list `x`, if we have reached the end of the list without finding the element we are searching for, we are done and can report that the element is not in the list. Otherwise, we look at the first element in the list, and if that is the element we are looking for we are done and can report that we found it. These two cases are the base cases. Otherwise, we do a linear search in the remainder of the list. That is the recursive case.

We can make the recursive nature of linear search more explicit by defining a *recursive function:*

```python
def linear_search(x, e, i = 0):
	if i == len(x):
		return False
	if e == x[i]:
		return True
	else:
		return linear_search(x, e, i + 1)
```

This does exactly what we described above. There are two base cases and one recursive case. For the function, the recursive case is handled by calling the function itself.

This version of linear search is, obviously, much more complex than the one we had before, and I do not recommend that you use this version instead of simply iterating through `x`. You should, however, be able to convince yourself that it does the same thing.

The only difference between a recursive definition and a recursive function in Python is that the former defines something while the latter actually computes something. There is no other difference. For example, we can write a function for computing—as opposed to defining—the factorial of a number like this:

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

**Exercise:** Implement a recursive function that computes the n’th Fibonacci number.

Binary search is another example of a recursive algorithm. In this algorithm, we either have an empty interval to search in, in which case we can report `False`. Or, we have the element we are searching for in the middle of the interval, in which case we can report `True`. Or, the recursive case, we search in the lower or the higher half of the interval.

Again, we can be more explicit in defining this as a recursive computation by implementing it as a recursive function:

```python
def bsearch(x, e, low = 0, high = len(x)):
	if low >= high:
		return False
	mid = (low + high) // 2
	if x[mid] == e:
		return True
	elif x[mid] < e:
		return bsearch(x, e, mid + 1, high)
	else:
		return bsearch(x, e, low, mid)
```

You should convince yourself that this, indeed, does the same as the binary search we have seen earlier.

If you recall, we required of recursive definitions that the recursive cases must move us closer to base cases. If we consider recursive computations, this requirement relates to termination. If each recursive call moves us closer to a base case—whatever that means—then the computation will eventually terminate. If not, then there is no such guarantee. Termination functions is one way to guarantee that a recursive computation will eventually provide an answer. We have seen them used to prove that loops will terminate; the idea is exactly the same for recursive functions.

For binary search, the termination function was `high - low`. This works equally well for the iterative version we have seen earlier as for the recursive function defined above. We didn’t use a termination function for our earlier implementation of linear search; we didn’t need one because we know that a `for`-loop over a finite sequence will terminate. For the recursive case, we cannot make as simple an argument, but of course the situation is the same. In each recursive call, the index `i` gets closer to the end of `x`. So, we can use as termination function `len(x)-i`.

Some people find recursion a difficult concept, especially when we use recursion for computation. Most people do not have any problem with accepting recursive definitions, but when we solve a problem by solving the exact same problem it feels like a circular definition. “For recursion, see recursion”. It isn’t, however, and the reason is that we never solve a problem by trying to solve exactly the same problem. We solve a problem that is closer to a base case; our termination function decreases with every recursive function call.

If you still find recursive functions hard to wrap your mind around, you might take comfort in knowing that early computer scientist did as well. Early programming languages could not define recursive functions. It simply didn’t occur to people that this might be useful. This doesn’t mean that they didn’t solve problems recursively; they just didn’t use recursive functions. Just like we did linear and binary search without recursive functions before we reformulated the algorithms as recursive. Recursion is such a powerful technique, however, that all modern languages support it. Some even go so far that they have replaced loops with recursion entirely; they will not let you implement loops at all, only recursive functions.

### Recursion stacks

To understand how recursive functions work, we first need to understand a little deeper how function calls work. Recall that we have two different kinds of variables in Python, global and local variables.^[There are a bit more to variables than this, but we do not need to worry about that for now. Global and local variables are all we have seen so far.] Global variables are those we assign to at the outermost level in a Python program while local variables are either function parameters or variables we assign to inside a function. For example, in this program

```python
x = 2
def add_one(x):
	return x + 1
add_one(2 * x)
```

We have two global variables, `x` and `add_one`. The `x` variable is one we have defined by assigning to `x`. The `add_one` is a function we have defined, but it is also a variable, and we can access the function through its name, `add_one`. Inside the `add_one` function we have the parameter `x`, and this is distinct from the global variable `x`. The two variables have the same name, but they are two different variables. Inside `add_one`, `x` refers to the local variable, outside of `add_one`, `x` refers to the global variable. When we call `add_one(2 * x)` we look up the name `add_one` and find the function and before we call it, we must evaluate the expression we provide as its argument, that is, we must evaluate `2 * x`. Since we call `add_one` in at the outermost level, the global scope, the `x` variable is the global variable. That holds the value 2, so `2 * x` is 4. We then call `add_one` and set the parameter `x`, the local variable, to 4. This does not change the global variable, that variable is still 2. When we return `x + 1`, we use the value of the local variable, which is 4, so we return 5.

You should be comfortable with the difference between global and local variables by now, but what happens when a function calls itself, as we did with the factorial function.

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

When we call 

```python
factorial(4)
```

the local variable, `n`, gets set to 4. No problem there. But then we call `factorial` with `n - 1` to evaluate `n * factorial(n - 1)`. This sets the local variable to 3, but we still need it to be for when we multiply the result of `factorial(n - 1)` by `n` after the recursive call. We need the local variable to be both four and three in the recursive case. How is that achieved?

This is something early programming languages couldn’t handle. In such early languages, local variables were tied to functions—each function had memory locations reserved for their local variables, and those were modified in function calls. If you called the same function twice, you would overwrite the memory locations assigned to the local variables. This isn’t a problem if you do not use recursion, but it clearly is if you do. You cannot use the same memory location to hold both $n=4$ and $n=3$.

The solution to this problem is very elegant in its simplicity: instead of reserving memory for local variables for each function, you reserve memory for local variables for each function *call*. The memory locations you need for local variables are not hardwired in the program but will be allocated when you call a function. This memory is allocated on a so-called *call stack*, and it works as follows. You have a piece of your computer’s memory set aside for this stack. You keep track of the top of the stack, and the function call you are currently executing has access to the top of the stack but not the rest of the stack. When you call a function, Python will reserve memory for the variables used in the function at the top of the stack. The memory set aside in this way is known as a function’s *call frame*, but what we call it is not important. What is important is that we have it.

Consider the call

```python
factorial(4)
```

at the outermost level. When we are at the outermost level, the call stack is empty. When we make the function call, Python puts two things on the call stack and increases the call stack pointer. What the call stack remembers is the function argument and where it should return to when in the program when we return from the function. The way Python actually handles local variables is a little different—it doesn’t reserve memory for each variable in this way—but it is very similar. The same goes for how return values are handled. The description here is conceptually true for how call stacks work, but the actual implementation is slightly different.

The stack needs to contain both the local variables and the return location. The local variables so we can have different instances of the same function and the return location so we know where the program should continue from when we return. It is because we remember this return location that we can call the same function from different places in a program and return to where we called the function once we are done with it. In old programming languages the location for this return point was also hardwired, but this doesn’t work for recursive functions for the same reasons that we cannot have a single location for local variables.

When we call recursively, Python will put the information it needs for each function call—local variables and the return location—on the stack and grow it as needed, see [@fig:growing-call-stack].

![Growing the call stack in recursive calls to `factorial`.](figures/growing-call-stack){#fig:growing-call-stack}

When we reach $n=1$, we have a base case and can return a result, 1. Now, Python needs to do two things. It needs to go to the place in the program pointed to by the return location, obviously, but it also needs to store the result of the function call somewhere. We haven’t thought about where Python stores results of expressions before, we just assumed that it could remember them somehow, but of course, on a computer you need to store intermediate results of expressions somewhere. Since results of (local) expressions are local to a function call, the only appropriate place is on the stack.^[Conceptually, all local results end up on the stack. In an actual program, you want to avoid putting results in main memory since it is much faster to access registers on the CPU. So your programming language will try to allocate registers instead of stack locations when possible. This all happens under the hood, and you do not need to worry about it. Conceptually, all local results go on the stack.] When we return from the recursive call, we first remove the call frame for that call and then put the result there on the stack instead. When we put the result of the call on the stack, the stack pointer needs to be moved up. If we didn’t do this, then another function call would overwrite it. We do not call other functions before we have used the result of the recursive call for `factorial`, but this would not be the case when computing Fibonacci numbers. Here we need two recursive calls the results of which we need to add together. We need to store the result of the first call while executing the second.

We keep doing the same thing while we return from the recursive calls. We replace the call frame with the result of the call, see [@fig:shrinking-call-stack]. These intermediate results are part of the call frame—they are local to the individual function calls—and they will also be removed from the stack when the function returns.

![Shrinking the call stack when returning from recursive calls to `factorial`.](figures/shrinking-call-stack){#fig:shrinking-call-stack}

Once we have returned from the last function call, the final result is still on the stack. It will be removed from there when Python uses this result (or simply be removed if we never use the result).

The call stack is not only used for recursion. It works the same way with all function calls. Consider this program:

```python
def add_one(x):
	return x + 1

def add_two(x):
	return add_one(add_one(x))

add_two(1)
```

[@Fig:call-stack-add-two] shows the call stack when evaluating this program. The call to `add_two` will put a return location and the local variable `x` on the stack. This function calls `add_one` that will put a return location and another `x` on the call stack. The first call to `add_one` is the one that becomes the argument to the second call to `add_one`. This call returns before we do the second call, though, so we do not have call frames for two `add_one` calls at the same time. The result of the first is put on the stack while we call the second. This is different from recursive calls, but the principle is the same.

![Call stack when evaluating `add_two(1)`.](figures/call-stack-add-two){#fig:call-stack-add-two}


**Exercise:** Draw the call stack for computing the fourth Fibonacci number using the implementation you did in the previous exercise.

All this call stack business can look complicated, but the good news is that you never need to worry about it unless you need to implement recursion yourself. The call stack is the reason why recursive function calls are possible, but you only need to know that they are. Well, you *almost* never have to worry about the call stack. You might run into problems with a call stack if you fill it up. If you do something like this:

```python
def f(x):
	return f(x + 1)
f(1)
```

you will get an error that says “RecursionError: maximum recursion depth exceeded”. This happens because the memory set aside for the call stack is limited and if the recursion gets too deep, there is no more stack available for more stack frames. This doesn’t happen if you use loops instead of recursion, and is another reason, besides efficiency, to prefer loops over recursion. Of course, in this particular example, there is no way to compute a value in finite time. The function never terminates, so it is not a proper recursive function in the first place, but there are cases where you have a perfectly well-defined recursive function that just needs more stack space than you have available. If that is the case, you need to replace the recursion with a loop.

### A few more examples of recursion

Later in this chapter we will see how we can use recursion to solve algorithmic problems, but first I want to show you another classic application of recursion: parsing and evaluating expressions. Parsing is what we do when we take a string (or a file) as input and translate that into something we can manipulate in a program. Such a string can be simple or complex. When you write a Python program, that program is a text (when in a Jupyter Notebook) or a file (otherwise). Before Python can execute your program, it parses it into a structure it can interpret.

We will use a much simpler example. We will take a string that represent an arithmetic expression, we will parse that expression, and we will compute the result of the expression.

You are familiar with arithmetic expressions such as $(2\cdot 3)+1$. This notation is known as *infix notation* because the operators you apply are written between expressions. So you write that you want to multiply $2$ to $3$ as $2\cdot 3$. We have rules that says that $2\cdot 3\cdot 4$ should interpreted as $(2\cdot 3)\cdot 4$ but we can simplify expressions slightly by requiring parentheses whenever we have more than two operators. We will see how we can parse and evaluate expressions in this format shortly, but first we will deal with a slightly simpler for of expressions. 

Infix notation is only one way to write expressions. It is the one you are most likely used to, and you might not have seen the others, but some calculators support one of the variants, postfix notation, so you might have. The alternatives are *prefix notation* and *postfix notation*.

In prefix notation, also know as *Polish notation*, you always write the operator before the operands. The expression $(2\cdot3)+1$ would be written $+ \cdot 2 3 1$. If you think of the operators as functions, this would be `add(multiply(2, 3), 1)` so the notation is not entirely unfamiliar to you. This notation has the benefit that you never need to use parentheses. When you have an operator you first evaluate its left operand and then its right and then finally you apply the operator to the results of this. If the left or right expressions involve other expressions there is implicitly parentheses. So while $(2\cdot 3)+1$ is written `+ * 2 3 1`, the expressions $2\cdot(3 + 1)$ is written `* 2 + 3 1`. No parentheses are needed.

**FIXME**

```python

prefix_expr = '+ * 2 3 1'

def evaluate(expr):
	expr_list = expr.split()
	val, idx = evaluate_rec(expr_list, 0)
	return val

def evaluate_rec(expr, i):
	if expr[i] == '+':
		a, i = evaluate_rec(expr, i + 1)
		b, i = evaluate_rec(expr, i)
		return a + b, i
	elif expr[i] == '-':
		a, i = evaluate_rec(expr, i + 1)
		b, i = evaluate_rec(expr, i)
		return a - b, i
	elif expr[i] == '*':
		a, i = evaluate_rec(expr, i + 1)
		b, i = evaluate_rec(expr, i)
		return a * b, i
	elif expr[i] == '/':
		a, i = evaluate_rec(expr, i + 1)
		b, i = evaluate_rec(expr, i)
		return a / b, i
	else:
		return int(expr[i]), i + 1

import operator
operator_table = {
	'+': operator.add,
	'-': operator.sub,
	'*': operator.mul,
	'/': operator.truediv, # The '/'' operator. 
	                       # The '//'' operator is floordiv
}

def evaluate_rec(expr, i):
	if expr[i] in ('+', '-', '*', '/'):
		op = operator_table[expr[i]]
		a, i = evaluate_rec(expr, i + 1)
		b, i = evaluate_rec(expr, i)
		return op(a, b), i
	else:
		return int(expr[i]), i + 1
```

The other notation, *postfix notation* or *reverse Polish notation* is very similar to prefix notation. You simply put the operator behind the operands instead of before them. Older calculators used this notation and some programming languages have used it. The expression $(2\cdot 3)+1$ would be written `2 3 * 1 +` and $2\cdot(3+1)$ would be written `2 3 1 + *`. We cannot as easily implement a recursive function that evaluate expressions in this notation. We will see later that it is very easy with an explicit stack, but if you want to evaluate an expression recursively you have the problem that you do not know how many recursive calls you need to make before you see the operator, and you do not see the operator until the recursive calls should already have been evaluated.

Instead of writing a function to evaluate postfix notation expressions, we will now implement one that evaluates infix notation expressions. To make it easier to do so, we require that parentheses are always explicitly written, so we do not allow expressions such as `1 + 2 + 3`. You have to write this as `(1 + 2) + 3`. Likewise, we do not allow `2 * 3 + 1` and you have to write it as `(2 * 3) + 1`. We are not ready to handle precedence in infix notation quite yet. Dealing with precedence and default evaluate order is much harder than dealing with expressions that use explicit parentheses wherever needed. 

**FIXME**

### Recursion and iteration

There is a close correspondence between recursion and iteration (or loops). You can always directly translate a loop into recursive function. You cannot always do the opposite—translate a recursive function into a loop. The recursion stack is more powerful than simple iterations, so you can do more with a stack than you can with a loop.^[Obviously, you can implement the stack functionality yourself and then avoid recursive function calls, but this is just an implementation detail. You will still be using recursion and not a simple loop.] Some recursions, however, can be directly translated into loops. These are called *tail-recursive*. We will get to these in a moment.

Just because you can implement all loops as recursive function calls doesn’t mean that you should. There is a substantial overhead in calling functions, so a recursive solution to a problem will always be slower than one that relies on loops.^[Programming languages that do not allow loops but only recursive function calls actually translate recursion into loops under the hood whenever possible. Python does not do such optimisations of recursive functions, so you should always prefer loops over recursions when you can.] If you can solve a problem directly using loops, you should do that. Sometimes, however, it is easier to first derive a recursive solution to a problem and then translate it into a loop if possible. Divide and conquer algorithms are easiest to think about as recursive, but some can be translated into loop-versions, also know as iterative algorithms. If lists are the most complex data structure we use in an algorithm, then we can usually solve problems directly using loops, when this is possible, and it doesn’t help us much to first derive recursive solutions. With more complex data structures, however, recursive solutions are often simpler to derive than iterative ones.

The purpose of this section is not to convince you that recursion is a better tool than iteration. If we can implement an algorithm using iteration, then that is the better choice. When we cannot, then recursion is the only choice. We see examples of this when we discuss divide and conquer. The purpose of this section is to get you familiarised with recursion, on simple problems, so you are well equipped to handle recursion when we consider more complex problems.

We will take some problems we have previously handled using loops and translate them into recursive function solutions. I will handle the first, and leave the rest for you to do as exercises.

The case I will implement is merging. Given two sorted lists, `x` and `y`, we want to compute the merge of the two, i.e., a list that contains the same elements as in `x` and `y`, combined, in sorted order.

We can implement this using a loop in this way:

```python
def merge(x, y):
	result = []
	i, j = 0, 0
	while True:
		if i == len(x):
			# no more elements in x
			while j < len(y):
				result.append(y[j])
				j += 1
			return result
		if j == len(y):
			# no more elements in y
			while i < len(x):
				result.append(x[i])
				i += 1
			return result
		if x[i] < y[j]:
			result.append(x[i])
			i += 1
		else:
			result.append(y[j])
			j += 1
```

The function doesn’t do anything complicated, but it is rather long, and therefore it can be hard to see at a glance what it is doing. What it does is simple enough. We move through `x` and `y`, using the indices `I` and `j`, and pick the smallest of `x[I]` and `y[j]` and and append that to our result. If we have made it to the end of either `x` or `y`, the first two `if`-statements in the loop, we copy the remainder of the other list.

A much simpler implementation of the same idea is this:

```python
def merge(x, y):
	if len(x) == 0:	return y
	if len(y) == 0:	return x
	if x[0] < y[0]:
		return [x[0]] + merge(x[1:], y)
	else:
		return [y[0]] + merge(x, y[1:])
```

Here we can directly see the two base cases and the recursive case—the recursive case is one of two recursive calls, depending on which list has the smallest element, but it is only one recursive call.

This version is shorter and simpler, and because of that, we would prefer it over the former version, all else being equal. Unfortunately, all else is not equal. For reasons I have not explained yet, getting everything except the first element of a list, as we do when we call `x[1:]` and `y[1:]`, is an expensive operation. It takes time proportional to the length of the lists we slice the first element off (minus one). There is another implementation fo lists than the one that Python use where this would be a constant time operation, but for Python `list` objects it is not. Therefore, the recursive call takes time $O(n)$, where the lengths of `x` and `y` are in $O(n)$, plus how long it might take to compute the recursive function, and the result is is an $O(n^2)$ running time all in all. I won’t go into details about why we get this running time, since we cover that in the section on divide and conquer later. The first `merge` implementation runs in time $O(n)$—if you cannot see why immediately, try to work through the analysis.

We can get rid of the expensive slicing of the first elements by reintroducing the index variables and get this implementation:

```python
def merge(x, y, i = 0, j = 0):
	if i == len(x): return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return [x[i]] + merge(x, y, i + 1, j)
	else:
		return [y[j]] + merge(x, y, i, j + 1)
```

Unfortunately, this isn’t much better. We avoid the slicing, but concatenating two lists, as we do in the recursive case, is also an $O(n)$ operation, so we end up with the same $O(n^2)$ running time.

To avoid both concatenation and slicing, we can do this:

```python
def merge(x, y, i = 0, j = 0, result = []):
	if i == len(x):
		# no more elements in x
		while j < len(y):
			result.append(y[j])
			j += 1
		return result
	if j == len(y):
		# no more elements in y
		while i < len(x):
			result.append(x[i])
			i += 1
		return result
	if x[i] < y[j]:
		result.append(x[i])
		return merge(x, y, i + 1, j, result)
	else:
		result.append(y[j])
		return merge(x, y, i, j + 1, result)
```

This leaves us pretty much back where we started. We now have a recursive solution to the problem, but it is not simpler than the iterative we started with.

All is not lost, however. We can reconsider why we couldn’t use the simpler recursive solution. The main problem here was concatenation—we could avoid the slicing simply by using indices. Maybe we can avoid the concatenation in some other way. Indeed we can:

```python
def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		res = merge_rec(x, y, i + 1, j)
		res.append(x[i])
		return res
	else:
		res = merge_rec(x, y, i, j + 1)
		res.append(y[j])
		return res

def merge(x, y):
	return list(reversed(merge_rec(x, y)))
```

Since prepending one element to a list involves a concatenation operation, which is expensive, we replace it with an append operation, which is cheap. We construct the merged list in the wrong order, so we need to sort it once we are done. I have split this into two functions, one that recursively construct the reversed result and one that reverses. In the basis cases we still slice a list, but since this takes time proportional to the length of the list, it is not slower than the `while`-loops we used earlier to do the same thing. In fact, it is likely to be faster since slicing is a built-in operation in Python and implemented very efficiently.

This solution is not quite as simple as the first recursive function, but we can make it almost so by making the append operation a function call:

```python
def app(lst, x):
	lst.append(x)
	return lst

def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return app(merge_rec(x, y, i + 1, j), x[i])
	else:
		return app(merge_rec(x, y, i, j + 1), y[j])
```

This solution is simpler than the first we had, almost as simple as the initial recursive one, and it runs in $O(n)$ since we have replaced the expensive operations in the recursive case with constant time operations. It is not as efficient as the first function we wrote. Function calls are expensive, and we use those extensively here, where the first solution didn’t use any function calls beyond the `merge` call needed to merge the two lists. We also need to reverse the result, which adds additional computations. 

In the next section, we will start from a recursive solution and use that to guide us to an iterative solution. We will end up with the same solution as we had before, so we will not get a better or simpler solution. The recursive solution is simple and by starting from that and getting to the iterative one is often an easier approach. It can be hard to design the iterative solution directly, but by starting from a simple recursive solution we can get it almost automatically.

Before we get to that, however, you should do some exercises to test that you have understood recursion.

**Exercise:** To compute the sum of the elements in a list, we can obviously do this iteratively:

```python
result = 0
for e in x:
  result += e
```

Implement a recursive function that computes the sum of the elements of a list.

**Exercise:** We can find the smallest element in a non-empty list, `x`, like this:

```python
smallest = x[0]
for e in x:
	smallest = min(smallest, e)
```

Write a recursive function for finding the smallest element in a list. To avoid copying the list using slices, you will want to have the function take an index parameter as an argument.

**Exercise:** Modify your function so it returns `None` if the list is not empty. The easiest way to do this is probably to include the “smallest element seen so far” as a parameter to the function, with a default value of `None`. To compute the smallest value and still handle `None` you can use this function:

```python
def my_min(x, y):
	return y if x is None else min(x, y)
```

**Exercise:** Write a recursive function that reverses a list.

**Exercise:** Recall the exercise where you had to translate a base-10 number into some other base $b$ (where we restricted the base to be less than 16). We can get the last digit of a number $i$, in base $b$ using this function:

```python
def get_last_digit(i, b):
    return digits[i % b]
```

where we defined the `digits` list as

```python
digits = {}

for i in range(0,10):
    digits[i] = str(i)

digits[10] = 'A'
digits[11] = 'B'
digits[12] = 'C'
digits[13] = 'D'
digits[14] = 'E'
digits[15] = 'F'
```

We can then reduce the problem to the second-to-last digit by dividing $i$ by $b$. Implement this idea using a recursive function.

### Tail-recursion

You can always translate an iterative algorithm into a recursive one, but since iterative algorithms are more efficient, you shouldn’t do this. Recursive algorithms are simpler, though, so you will often find yourself in the situation that you have an elegant recursive solution to a problem, and you want to translate it into a more efficient iterative solution. You cannot always translate a recursive function into an iterative one,[^iterative_is_always_possible] but this section is about the cases where you can.

[^iterative_is_always_possible]: In the interest of complete honesty, I am lying here. You can always translate a recursive function into an iterative one. It just requires some tricks we are not ready for here. It usually isn’t a particularly good idea; it is not efficient and you can solve such problems more efficiently by implementing your own stack as a substitute for the recursion stack. It is possible, though. It is just a lot more involved than the cases we consider in this chapter.

Functions that can always be translated into loops are so-called *tail recursive* functions. In many programming languages, tail recursive functions are automatically translated into loops—this is called the *tail recursive optimisation*—but Python is not one of them. Not to worry, though, the translation is so simple that you can always do it by hand with very little effort.

A function is *tail recursive* when the recursive case is only a recursive call. Consider the factorial function:

```python
def factorial(n):
	if n == 1:
		return 1
	else:
		return n * factorial(n - 1)
```

The recursive case involves a recursive call and we then multiply the result of this call to `n`. Because we have to multiply the result of the recursive call with `n`, the function is not tail recursive. We can translate it into a tail recursive function by adding an accumulator to the function:

```python
def factorial(n, acc = 1):
	if n == 1:
		return acc
	else:
		return factorial(n - 1, n * acc)
```

The accumulator handles the multiplication with `n`. We multiply the accumulator by `n` as we go down the recursion rather than multiply the result of recursive calls by `n`. Functions that only involve a single recursive call in the recursive case can always be translated into tail recursive versions by adding an accumulator.

**Exercise:** Rewrite your recursive function for computing the sum of a list of numbers so it becomes tail recursive.

**Exercise:** Rewrite your recursive function for finding the smallest element in a list to a version that is tail recursive.

With tail recursive functions, we do not need to do anything with the result of the recursive call. Not doing anything with the result of a recursive call is just another way of saying that a function is tail recursive. The reason that this is important is that, when we do not need to do anything after the recursive call, then we can reuse the call frame for the recursive call. We can simply update the local variables to those we would use in the recursive call frame and go from there; we can replace a recursive call with a simple update of the function argument local variables. When we call a function we assign values to the function arguments. If we have a tail recursive function, we can simply update the arguments we already have and then start executing the function body from the beginning again. If we wrap the function body in one big `while True` loop, we can replace the recursive function call an update to the function arguments and then `continue` the loop. If the recursive case is put at the end of the loop, we do not even need to `continue`; we are at the end of the loop so we return to the beginning right after we update the variables.

For the factorial function, this transformation gives us:

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		n, acc = n - 1, n * acc
```

If you split the variable updates over multiple statements, you have to be careful about the order. When you update a variable, you affect expressions that depend on it. So you have to update the variables in the right order.

This will work

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		acc = n * acc
		n = n - 1
```

This will not:

```python
def factorial(n):
	acc = 1
	while True:
		if n == 1:
			return acc
		n = n - 1
		acc = n * acc
```

A parallel assignment, as we did for the first iterative implementation of the factorial function, will usually work. If we never do any operations with side-effects, i.e., whenever we need to update a data structure such as a list, we create a new one instead, then parallel assignment will work. If we actually modify a data structure, we cannot use parallel assignment so we must be careful that we perform the update operations in the same order as they would have been performed in a function call.

**Exercise:** Do this transformation for your tail recursive summation function.

**Exercise:** Do this transformation for your tail recursive “find minimum” function.

**Exercise:** Consider our recursive implementation of binary search:

```python
def bsearch(x, e, low = 0, high = len(x)):
	if low >= high:
		return False
	mid = (low + high) // 2
	if x[mid] == e:
		return True
	elif x[mid] < e:
		return bsearch(x, e, mid + 1, high)
	else:
		return bsearch(x, e, low, mid)
```

This function is tail recursive, so use the transformation to replace it with a loop. Compare it to the iterative solution we considered before this chapter.

To see a more complex case of using an accumulator in a tail-recursive function, and then translate it into an iterative function, we can return to the problem of merging two lists. We left this problem with the recursive implementation below:

```python
def app(lst, x):
	lst.append(x)
	return lst

def merge_rec(x, y, i = 0, j = 0):
	if i == len(x):	return y[j:]
	if j == len(y):	return x[i:]
	if x[i] < y[j]:
		return app(merge_rec(x, y, i + 1, j), x[i])
	else:
		return app(merge_rec(x, y, i, j + 1), y[j])

def merge(x, y):
	return list(reversed(merge_rec(x, y)))
```

The reason we had to construct the merged list in reverse order, and then reverse it when we are done, was actually because we didn't use an accumulator. Because of this, we had to build the list by combining a recursive call with the front of one of the lists in the recursive case. If we add an accumulator, we can build the merged list in the right order:

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	if i == len(x):	return acc + y[j:]
	if j == len(y):	return acc + x[i:]
	if x[i] < y[j]:
		return merge(x, y, i + 1, j, app(acc, x[i]))
	else:
		return merge(x, y, i, j + 1, app(acc, y[j]))
```

The way we handle the default value of the accumulator might look a bit weird, but it is important. If we set the default value of `acc` to an empty list, each call to `merge` that rely on the default parameter will get the *same* list. This means that if you call `merge` twice, the result of the first call will still be in the accumulator, and the new merge will be appended to it. This is not what we want, and it is because of this that we handle the default parameter the way we do.

This function is tail recursive so we can translate it into a looping version. The `app` function simply append its second argument to its first, and it does this before the recursive call (because function arguments are evaluated before a function is called). Because of this, we can get rid of it and simply append instead. We have to be careful to append before we update the indices, though. The rewritten function looks like this:

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	while True:
		if i == len(x):	return acc + y[j:]
		if j == len(y):	return acc + x[i:]
		if x[i] < y[j]:
			acc.append(x[i])
			i += 1
		else:
			acc.append(y[j])
			j += 1
```

If you want to avoid copying the accumulator in the basis cases, you can use the `extend` method on the accumulator list.  Using `extend` and a slice on one of the input lists is unlikely to be slower than a `while`-loop where we move individual elements, since `extend` and slice are builtin operations and therefore highly optimised.

```python
def merge(x, y, i = 0, j = 0, acc = None):
	if acc is None:
		acc = []
	while True:
		if i == len(x):
			acc.extend(y[j:])
			return acc
		if j == len(y):
			acc.extend(x[i:])
			return acc
		if x[i] < y[j]:
			acc.append(x[i])
			i += 1
		else:
			acc.append(y[j])
			j += 1
```

## Divide and conquer

Divide and conquer is the algorithmic version of recursion. The term comes from the political doctrine *divide et impera*, but for algorithms, a more correct description would be *divide and combine*. The key idea is to

1. Split a problem into subproblems of the same type.
2. Recursively solve these problems.
3. Combine the results of the recursive calls to a solution of the original problem.

Components 1 and 3 can be very simple or very complex, while 2 is usually one or two recursive calls.

The binary search algorithm, we have seen several times by now, is an example of a divide and conquer algorithm. Step 1 in this algorithm is identifying whether we should search to the left or to the right of the midpoint, the recursive step is searching in one of these intervals. Step 3 is almost non-existing, since we simply return the result of the recursive solution in step 2.

The recursive step(s) in divide and conquer are often implemented as recursive function calls, but need not be. Conceptually, we recurse, but as we saw in binary search, we can replace recursive calls with loops. It is not necessary to use recursion in your implementation of a divide and conquer algorithm; the key component of this class of algorithms is that we solve a subproblem of the same type as the original problem. Since we are using recursion, even if it is only conceptually, we need to have basis cases and recursive cases. The basis case in binary search is when we have an empty interval or when the midpoint is the element we are looking for. The recursive case handles everything else.

As another example of divide and conquer, we can consider a sorting algorithm known as *merge sort*. This algorithm works as follows:

1. Split the initial input into two pieces of half the size of the original problem: the first and the second half of the input list.
2. Sort these two smaller lists recursively.
3. Combine the two sorted lists using merge.

This algorithm involves two recursive subproblems, so it is not easy to implement it as an iterative solution. We will implement it recursively. The basis cases for the recursion are when we have empty lists or lists of length one—these will be lists that are already sorted. The recursive case handles everything else.

A straightforward implementation of this could look as follows:

```python
def merge_sort(x):
	if len(x) <= 1: return x
	mid = len(x) // 2
	return merge(merge_sort(x[:mid]), merge_sort(x[mid:]))
```

It does exactly what we identified as the steps that merge sort should do, but you might be uncomfortable with the slicing we do to split the input `x`. For `merge`, doing this slicing increased the running time from $O(n)$ to $O(n^2)$. It is not quite as bad here since the linear time slice operation is slower than the time it takes to sort the sub-lists—we discuss the running time shortly, but it will be $O(n\log n)$. Still, we could avoid it using indices into `x` instead:

```python
def merge_sort_rec(x, low, high):
	if high - low <= 1: return x[low:high]
	mid = (low + high) // 2
	return merge(merge_sort_rec(x, low, mid), 
		         merge_sort_rec(x, mid, high))

def merge_sort(x):
	return merge_sort_rec(x, 0, len(x))
```

I have implemented this using two separate functions, one that handle the actual sorting but takes indices as arguments, and one that only takes `x` as its argument and calls the former. We cannot set `low` and `high` as default arguments, since `high` should be set to the length of `x`, and we do not know what `x` is until we call the function, and default arguments must be known when we define a function. We could use the trick of setting them to `None` and then check if they are, as we did `merge`, but then we would need to check the arguments in each recursive call. This wasn’t a problem when we translated `merge` into an iterative algorithm, but we cannot do this with `merge_sort` since it is not tail recursive. Therefore, I prefer to split the algorithm into two functions.

We still slice `x` when the interval is one or zero, and we still need to concatenate two lists when we merge. We can also avoid this by doing the sorting in-place. For this we need an in-place merge. Doing this was an exercise earlier in the book, but a recursive solution could look like this:

```python
def inplace_merge(x, i, j, k, l):
	# invariant: i <= j <= k <= l
	if i == j or k == l: return # nothing left to be done
	if x[i] < x[k]:
		inplace_merge(x, i + 1, j, k, l)
	else:
		x[i], x[k] = x[k], x[i]
		inplace_merge(x, i + 1, j, k, l)
```

Since this is a tail recursive function, we can transform it into an iterative algorithm in the straightforward way:

```python
def inplace_merge(x, i, j, k, l):
	while True:
		# invariant: i <= j <= k <= l
		if i == j or k == l: return # nothing left to be done
		if x[i] < x[k]:
			i += 1
		else:
			x[i], x[k] = x[k], x[i]
			i += 1
```

With the in-place merge we can implement an in-place merge sort like this:

```python
def merge_sort_rec(x, low, high):
	if high - low <= 1: return
	mid = (low + high) // 2
	merge_sort_rec(x, low, mid)
	merge_sort_rec(x, mid, high)
	inplace_merge(x, low, mid, mid, high)
	
def merge_sort(x):
	merge_sort_rec(x, 0, len(x))
```

### Divide and conquer running times

Figuring out the running time for recursive functions, or algorithms that are recursive even if they are not implemented as recursive functions, involves solving recurrence equations. If $T(n)$ denotes the running time on input of size $n4, then a recurrence equation could look like this:

$$T(n) = 2\cdot T(n/2) + O(n)$$

This is the recurrence equation for merge sort. To sort a list of length $n$ we solve two problems of half the size, $2T(n/2)$, and do $O(n)$ additional work. In the first version of merge sort, where we sliced the input, we used linear time both for the slicing and then for the merge; in the final version we only spend linear time with the merge. In either case, we spend linear time in addition to the recursive calls.

What characterise recurrence equations is similar to what characterises recursive solutions to problems. The equations refer to themselves. Strictly speaking, we need basis cases for the recurrence equations to be well defined, so we would have

$$T(n) = \begin{cases}
 O(1) & n \leq 1 \\
 2\cdot T(n/2) + O(n) & \text{otherwise}
\end{cases}$$

but when we consider the running time of an algorithm, the basis cases almost always involve constant time, so we often leave that out.

You can solve such recurrence equations by expanding them

\begin{align*}
	T(n) &= O(n) + 2\cdot T(n/2) \\
       &= O(n) + 2\left[
	        O(n/2) + 2\cdot T(n/4)
       \right] \\
       &= O(n) + 2\left[
	        O(n/2) + 2\cdot \left[
		        O(n/4) + T(n/8)
	        \right]
       \right] \\
       &= \ldots
\end{align*}

but you have to be a little careful with the expansion and big-Oh notation. Here, we see that we get a $O(n/2)$ in the first expansion, and normally we would consider this equal to $O(n)$. It is, but as we keep expanding, we see the series $O(n)+O(n/2)+O(n/4)+\cdot+O(n/n)$. If we pretend that all the $O(n/2^k)$ components are $O(n)$ this would give us $n\times O(n)=O(n^2)$. This *is* an upper bound on the expression, but it is not actually tight. If you multiply into the parentheses in the equation you will also get

$$2\left[O(n/2) + 2\cdot T(n/4)\right] = 2 O(n/2) + 4 T(n/4)
 = O(n) + 4 T(n/4)$$

which is actually fine, but if you translated $O(n/2)$ into $O(n)$, you would get

$$T(n) = 2O(n) + 4O(n) + 8O(n) + 16\cdot T(n/8)$$

where each of the $O(n)$ components are multiplied by a number $2^k$. You can consider that a constant in each step, but $k$ actually depend on $n$, so it isn't really a constant. Neither is the number we divide $n$ by inside the big-Oh.

The problem here is that if we blindly translate the expanded numbers $2 O(n/2)$ into $O(n)$ are do not take into account that, as we continue the expansion, the number we multiply with and the number we divide by, changes for each expansion. They depend on $n$ in how many times we do this and what the numbers are in each step. They are not constants. The arithmetic rules we have learned for the big-Oh notation is fine, the problem is not in that. The problem is if we misunderstand the numbers in the expansion as constants when they are not. This is usually not a trap you will fall into when reasoning about iterative algorithms as we have done earlier, but it is easy to fall into here.

When expanding a recurrence equation, it is easier to translate $O(n)$ into $cn$ for some constant $c$. We know such a constant exists so $cn$ is an upper bound for whatever the $O(n)$ is capturing. Then we get an expansion like this:

\begin{align*}
	T(n) &= cn + 2\cdot T(n/2) \\
       &= cn + 2\left[
	       cn/2 + 2\cdot T(n/4)
       \right] \\
       &= cn + 2\left[
	       cn/2 + 2\left[cn/4) + 2\cdot T(n/8)
	       \right]
       \right]\\
       &= \ldots
\end{align*}

If we take this expansion all the way down, we get

$$T(n)=\sum_{k=0}^{\log n} c\cdot 2^k n / 2^k
		  =\sum_{k=0}^{\log n} c\cdot n = c\cdot n\log n$$
		  
where $\log n$ is the base-two logarithm, and we get that limit because $2^{\log n}=n$ is when we reach $n/n=1$. This means that this recurrence is in $O(n\log n)$, so this is the big-Oh running time for merge sort.

Usually, you can solve recurrence equations just by expanding them and recognising the form of the sum you get back from it. This often take the form of a series you know is convergent or such. By far, the divide and conquer algorithms you will run into are in one of the forms listed below, so I find it easier to just remember these.

$$T(n) = 2\cdot T(n/2) + O(n) \in O(n\log n)$$

We just saw an example of this. Whenever you can break a problem into two subproblems of half the length as the original and you do not spend more than linear time splitting or combining, you get an $O(n\log n)$ algorithm. The merge sort is one such algorithm. This is also the first comparison-based sorting algorithm we have seen that runs in this time, and it can be shown to be optimal (in the sense of big-Oh) because all comparison-based sorting algorithms need to do $\Omega(n\log n)$ comparisons.

$$T(n) = T(n-1) + O(1) \in O(n)$$

If we, in constant time, remove one from the problem size we have an algorithm that runs in linear time. If we consider linear search a recursive problem—the basis case is when the first element is the one we are searching for and the recursive case is doing a linear search on the rest of the input—then that would be an example of such an algorithm.

$$T(n) = T(n-1) + O(n) \in O(n^2)$$

If we need linear time to reduce the problem size by one, then we get a quadratic time algorithm. Selection sort, where we find the smallest element in the input, in linear time, swap it to the first element, and then recursively sort the rest of the list is an example of this.

$$T(n) = T(n/2) + O(1) \in O(\log n)$$

If you can reduce the problem to half its size in constant time, then you have a linear time algorithm. Binary search is an example of this.

$$T(n) = T(n/2) + O(n) \in O(n)$$

If you can reduce the problem to half its size in linear time, and get the solution for the full problem after the recursive computation in linear time as well, then you have a linear time algorithm. Notice that this equation is different from the one we had for merge sort; in that recursion we needed *two* recursive calls, in this we only need one.

As an example of this running time, consider a function that adds a list of numbers by first adding them pairwise and then adding all the pairwise sums in a recursive call:

```python
def binary_sum(x):
	if len(x) == 0: return 0
	if len(x) == 1: return x[0]

	# O(n) work
	y = []
	for i in range(1, len(x), 2):
		y.append(x[i - 1] + x[i])
	if i + 1 < len(x): # handle odd lengths
		y[0] += x[i + 1]
	
	return binary_sum(y) # recurse on n//2 size
```

This looks like just a complicated way of adding numbers—and in some ways it is—but it can be relevant if you need to compute the sum of floating point numbers (see the last section of this chapter). When you add two floating point numbers, you might not end up with their sum. If this surprises you, remember that a number must be represented in finite computer memory, but not all real numbers can. For example, the decimal representation of $1/3$ require infinitely many decimals, $0.333333\ldots$. In your computer, you do not use decimal notation but binary, but the problem is the same. (You can, of course, represent rational numbers like $1/3$ as two integers, and you can represent arbitrarily large integers, but there are more real numbers than rational numbers).

If you add two floating point numbers, you will lose bits of information proportional to how many orders of magnitude the numbers are apart, so the larger the difference, the less precise your addition is. If you add the numbers in a large list by starting from the left and moving right, as we have seen before, then this can become a problem. The accumulator will grow and grow as we add more and more numbers, and if the numbers in the list are of roughly the same order of magnitude, the next number to add will be many orders of magnitude smaller than the accumulator. If the difference gets large enough, adding a number to the accumulator simply gives you the accumulator back.

If you start with numbers of the same order of magnitude, then adding them pairwise as in the algorithm above, will keep them at roughly the same order of magnitude in the recursion, and this will alleviate the problem of loosing precision in floating point numbers.

$$T(n) = 2\cdot T(n/2) + O(1) \in O(n)$$

If we can split and combine in constant time but require two recursive calls on half the size, we also get a linear time algorithm. Again, notice that this is different from the recurrence equation for merge sort where we needed linear time to merge the results of the two recursive calls.

An example of an algorithm that has this recurrence, consider the problem of finding the largest difference between two numbers in a list. You could, of course, run through all pairs and find the largest, but that would take time $O(n^2)$. Instead, you can solve a slightly harder problem and get the solution from there—where by "harder" I mean that it solves more than just finding the maximum difference, I do not mean that this is a harder complexity class. We can actually solve it in linear time by using a divide and conquer algorithm with the recurrence above.

The problem we will solve is to find the smallest and the largest number in a list as well as the largest difference. We do this recursively. We split the input into two halves, find the smallest and largest elements in both halves, as well as the largest difference in the two halves. We can then combine these. The smallest element for the full list is the smallest of the smallest in either half. The largest element is the maximum of the two we got from the recursive calls. The largest difference in the full list is either found in one of the two halves, or we can get it as the difference between the largest element in one half and the smallest in the other. We can implement the entire algorithm like this:

```python
def min_max_maxdiff(x, i, j):
	# Invariant: j > i
	if i + 1 == j:
		return x[i], x[i], 0
	else:
		mid = (i + j) // 2
		min_l, max_l, maxdiff_l = min_max_maxdiff(x, i, mid)
		min_r, max_r, maxdiff_r = min_max_maxdiff(x, mid, j)
		min_res = min(min_l, min_r)
		max_res = max(max_l, max_r)
		maxdiff_res = max(maxdiff_l, maxdiff_r,
			              max_r - min_l, max_l - min_r)
		return min_res, max_res, maxdiff_res
```

If we only want the maximum difference, we can wrap the function:

```python
def maxdiff(x):
	if len(x) == 0: return None
	_, _, md = min_max_maxdiff(x, 0, len(x))
	return md
```

Splitting the data and combining the results from the recursive calls can be done in constant time and we make two recursive calls of half the size, so by the recurrence equation above, the running time is $O(n)$.

### Representing floating point numbers

Floating point numbers, the computer analogue to real numbers, can be represented in different ways, but they are all variations of the informal presentation I give in this section. If you are not particularly interested in how these numbers are represented, you can safely skip this section. You can already return to it if you think you are getting weird behaviour when working with floating point numbers.

In the representation used by modern computers is standardised as IEEE 754. It basically represents numbers as explained below, but with some tweaks that lets you represent plus and minus infinity, “not a number” (NaN), and with a higher precision for numbers close to zero than the presentation here would allow. It also uses a sign bit for the coefficient while it represents the exponent as a signed integer for reasons lost deep in numerical analysis. All that you need to know is that floating point numbers work roughly as I have explained here, but with lots of technical complications. If you find yourself a heavy user of floating point numbers, you will need to study numerical analysis beyond what we can cover in this book, and you can worry about the details of number representations there.

Floating point numbers are similar to the *scientific notation* for base-$b$ numbers,^[To get a binary notation, replace $b$ by $2$. For non-zero numbers, $a_1$ must be 1, so we do not represent it explicitly, which gives us one more bit to work with.] where numbers are represented as

$$x = \pm a\times b^{\pm q}$$

where $a = a_1.a_2a_3\ldots a_n, a_i\in\{0,1,\ldots,b-1\}$ is the *coefficient* and $q = q_1q_2q_3\ldots q_m, q_i\in\{0,1,\ldots,b-1\}$ is the *exponent* of the number. Not all real numbers can be represented with this notation if $a$ and $q$ are finite sequences,^[Which real numbers are representable using finite a number of digits depend on the base, $b$. You cannot represent $1/3$ in finite decimal ($b=10$) notation but in base $b=3$ it is simply $1\times 3^{-1}$.  Likewise, you cannot represent $1/10$ in binary in a finite number of digits, where you trivially can in base 10.] but all can be arbitrarily closely approximated by using sufficiently large numbers of digits, $n$ for the coefficient and $m$ for the exponent. We usually assume that if $x\neq 0$ then $a_1\neq 0$ since, if $a_1 = 0$ we can update $a$ to $a_2.a_3\ldots a_n$ and decrease $q$ by one if positive or increase it by one of positive.

Where floating point numbers differ from this notation is that we have a limit on how many digits we have available for the coefficient and the exponent. To represent any real number, we can choose sufficiently high values for $n$ and $m$, but with floating point numbers there is a fixed number of digits for $a$ and $b$. You cannot approximate all numbers arbitrarily close. For example, with $b=2$ and $n=m=1$, we have $\pm a\in\{-1,0,1\}, \pm q\in\{-1,0,1\}$, so we can only represent the numbers $\{-1, -1/2, 0, 1/2, 1\}$: $\pm 1/2 = \pm 1\times 2^{-1}$, $\pm 0 = \pm 0\times 2^q$, and $\pm 1 = \pm 1\times 2^{\pm 0}$ (where $\pm 0$ might be represented as two different numbers, signed and unsigned zero, or as a single unsigned zero, depending on the details of the representation). If we use two bits for the exponent, we get the number line shown in [@fig:number-line].

![Number line when we have one bit for the coefficient and two bits for the exponent.](figures/number-line){#fig:number-line}

As a rule of thumb, floating point numbers have the property that is illustrated in [@fig:number-line]. The numbers are closer together when you get closer to zero and further apart when their magnitude increase. There is a positive and a negative minimal number; you cannot get closer than those to zero except by being zero. If you need to represent a non-zero number of magnitude less than this, we say that you have an *underflow* error. There are also a smallest and a largest number (the positive and negative numbers furthest from zero). If you need to represent numbers of magnitude larger than these, we say you have an *overflow* error.

There isn’t really much you can do about underflow and overflow problems except to try to avoid them. Translating numbers into their logarithm is often a viable approach if you only multiply numbers, but can be tricky if you also need to add them.

In the binary sum example from earlier, the problem is not underflow or overflow, but rather loosing significant bits when adding numbers. The problem there is the fixed number of bits set aside for the coefficient. If you want to add two numbers of different magnitude, i.e. their exponents are different, then you first have to make the exponents equal, which you can do by moving the decimal point. Consider $1.01101\times 2^{3}$ to $1.11010\times 2^0$—where we have five bits for the coefficients (plus one that is always 1, i.e. $n=6$). If you want to add $1.01101\times 2^{3}$ to $1.11010\times 2^0$ you have move the decimal point in one of them. With the representation we have for the coefficients, $a=a_1.a_2\ldots a_n$, we can only have one digit before the decimal point, so we cannot translate $1.01101\times 2^3$ into $1011.01\times 2^0$ so we have to translate  $1.11010\times 2^0$ into $0.00111010\times 2^3$. We want the most significant bit to be one, of course, but we make this representation for the purpose of addition; once we have added the numbers we put it in a form where the most significant bit in the coefficient is one. The problem with addition is that we cannot represent $0.00111010\times 2^3$ if we only have five (informative, i.e. $n-1$) bits in the coefficient. So we have to round the number off and get $0.00111\times 2^3$. The difference in the sum is $2^{-4} = 0.0625$, so not a large difference in the final result, but we have lost three bits of accuracy from the smaller number.

```
1.01101    * 2**3 +
1.11010    * 2**0 =
1.01101    * 2**3 +
0.00111010 * 2**3 =
1.01101    * 2**3 +
0.00111010 * 2**3 =
1.10100010 * 2**3 !=
1.01101    * 2**3 +
0.00111    * 2**3 =
1.10100    * 2**3
```

In general, you expect to loose a number of bits equal to the difference in the exponents of the numbers. The actual loss depends on the details of the representation, but as a rule of thumb, this is what you will lose.

Assume we have one informative bit for the coefficient ($n=2$) and two for the exponent, ($m=2$), and we wanted to add six ones together $6\times 1.0_2 \times 2^0 = 1.1_2\times 2^2$. Adding the numbers one at a time we get:

```
1.0 * 2**0 +
1.0 * 2**0 = 1.0 * 2**1 +
1.0 * 2**0 = 0.1 * 2**1 = 1.1 * 2**1 +
1.0 * 2**0              = 0.1 * 2**1 = 1.0 * 2**2 +
1.0 * 2**0                           = 0.0 * 2**2 = 1.0 * 2**2 +
1.0 * 2**0                                        = 0.0 * 2**2 = 1.0 * 2**2
```

which is off by 2. If we add the numbers as we did with our `binary_sum` function, we instead have

```
1.0 * 2**0 +
1.0 * 2**0 = 1.0 * 2**1 +
1.0 * 2**0 + 
1.0 * 2**0 = 1.0 * 2**1 = 1.0 * 2**2 +
1.0 * 2**0 +
1.0 * 2**0 = 1.0 * 2**1 = 0.1 * 2**2 = 1.1 * 2**2
```

which is correct.

Obviously, the floating point numbers you use in Python have a much higher precision than one bit per coefficient and two per exponent, so you will not run into problems with accuracy as fast as in this example. The principle, is the same, however. If you add enough numbers you risk that the accumulator becomes too large for the addition with the next number to have an effect. If you run into this, then adding the numbers pairwise as in `binary_sum` can alleviate this.


# Algorithmic efficiency

As a general rule, we want our algorithms to work as efficiently as possible, which usually means that we want them to solve the problem they are designed for quickly and using as few resources, such as memory or disk space, as possible. This is just a rule of thumb, of course, because we also need to factor in programmer time—if you need a problem solved a month from now, and you have the choice between an algorithm that can solve the problem in two weeks and takes one week to implement, that is preferable over an algorithm that can solve the problem in an hour but takes a month to implement.

It is notoriously hard to estimate how long it will take to implement a given algorithm, however, as this depends on the skills of the programmer and which libraries and programming language abstractions he has available, and we will not attempt to do that here. With experience, you will get a feel for how hard it will be to implement a given algorithm, and while you are unlikely to ever be able to accurately estimate exactly how much work goes into implementing one, you will be able to determine which of two choices is the more complex algorithm. Unless there are good reasons for wanting a more complex but potentially more efficient algorithm, you should go for the simplest algorithm that can get the job done. Sometimes, the simplest algorithm *will* be too slow for your problem and you have to replace it with a more complex one, or sometimes you expect to run your algorithm many times on many data sets, and spending a little more programming time for faster execution down the line might pay off, but as a general rule, you do not want to spend more time programming than what is warranted for your problem. Programmer-time is much more valuable than computer-time, so simple algorithms that can be implemented quickly are generally preferable to complex algorithms that are hard to implement.

This chapter is not about the complexity of algorithms in the sense of how complicated they are to implement, however, but about how efficiently they solve a problem. That is, the chapter is about how fast you can expect an algorithm to execute. Whenever you have the choice between two algorithms that looks like they are roughly equally complicated to implement, you want to go with the one that is likely to be the most efficient of the two.

The best way to measure which of two algorithms is superior for solving a given problem is, of course, to program the best implementations of the two that you can manage and then measure how fast they are on your data. Whichever you stopwatch tells you is the best, clearly is. That, of course, is not an option if your focus is on actually solving a problem rather than comparing two algorithms. To solve your problem that way, you have to solve it twice. You need to implement both algorithms and then run both of them on your actual data. What we need is a way of reasoning about how likely it is that one algorithm is better than another without using a stop-watch.

The way we reason about algorithmic efficiency is similar to how we reason about the physical world when we do science: we use a model of a computer that is much simpler than the real thing, but one we can reason about without too many complications. We then work out how many primitive operations an algorithm needs to make on the input we give it. This is not a completely accurate measurement of how long the actual algorithm will run on an actual computer, but it is a good estimate.

To make it even simpler to reason about efficiency than this, we often don’t care about counting the number of primitive operations accurately either. We will only care about how the number of computations depend on the input in an asymptotic sense that should be clear by the end of this chapter, but loosely means that we only care about whether an algorithm needs to make a number of computations that is, say, linear or quadratic in the input size, and we do not care what the actual number of computations are; we would consider functions such as $2x^2+x$ and $100x^2+1000x$ the same but different from $500x$. This might sound a bit sloppy, but there is a mathematical underpinning that justifies this, and by using this approach we can very quickly determine which of two algorithms is likely to be faster than the other.

## The RAM model of a computer and primitive operations

Any actual computer you will ever use is a very complex machine. At its core is a gazillion transistors wired together to implement different logical operations in hardware. The hardware in your computer handles both simple computations, such as comparing numbers or doing arithmetic, and manages how data is moved around. Data is stored different places in such a computer; you have persistent storage on disk, that lives outside of running programs, and while programs run, you have data moving between different layers of memory hierarchies. At the core of the computer is one or more central processing units, or CPUs, that do actual computations. Data needs to be moved to these CPUs for them to compute on it. They can hold some data themselves, in what is called *registers*, and they can do computations on such data very quickly. There is only a finite number of registers, though, so data needs to be moved to and from these registers. Below registers are several layers of memory called *caches*, and the closer caches are to the registers, the faster that can happen. Below caches you have *random-access memory*, or RAM, and below that, disks, which can be solid state or actual rotating disks, or some combination thereof.

The levels of the memory hierarchy is vital to actual performance of an algorithm. Every time you move data from one level to the next, the time it takes to access it grows by orders of magnitude. Your CPUs can make billions or trillions of computations in the time it takes the computer to move data from your disk to a register. Moving data up and down the memory hierarchy, however, is handled automatically by the computer so we don’t have to deal with it explicitly. We can construct algorithms that ignores the memory hierarchy completely and still get well-working programs. Considering the memory hierarchy can help us improve the running time of a program immensely, but doing this also greatly complicates our algorithms, and in many cases solutions will depend on the actual hardware of the computer we execute such algorithms on.

I am mentioning all this to make it clear that considering any actual computer greatly complicates how we can reason about the actual performance of an algorithm. If we have to consider any actual computer, or even a simplified model of one that takes vital information such as memory hierarchy into consideration, reasoning about running time becomes practically impossible. When we reason about algorithmic complexity, we use much simpler models, and a traditional model is the so-called *RAM model*, where RAM again stands for random-access memory.

The key property of the RAM model is that we assume that any data we need, we can access in constant time. We assume that there is such a thing as a *computer word*, which you should think of as a number or a string character or a primitive piece of data like that. We cannot store arbitrarily large numbers in computer words, but we assume that if the size of the input data for your algorithm is $n$ such primitive pieces of data, then a computer word can contain numbers up to $n$. Put in another bit, if your input data is of size $n$, then you can represent $\lceil \log_2 n \rceil$ bits in a computer word, since that is the number of bits you need to represent numbers up to $n$. Any computer word in our input data, or any computer word we create in our algorithm, can be accessed on constant time. We ignore that some computer words can be accessed much faster than others; they are all just accessible in constant time. We also assume that we can compare computer words in constant time, and that any arithmetic operation we do on numbers that can be stored in a single computer word can be done in constant time, ignoring that dividing numbers on a real computer will take longer than comparing them and such.

The reason we have to be a little careful about what we consider a computer word is exactly because we want to make the assumption about how fast we can make computations on them. We want our abstract computer words to roughly match what the actual computer can work with in its registers. In the hardware of a real computer, the different operations you can do on numbers and bit-patterns are wired together, so the CPUs consider operating on words in registers as primitive operations. The time it takes actual hardware to do a primitive operation depends on what that operation is. The hardware for multiplication is more complex than the hardware for simple comparison. But the time it takes a CPU to do a primitive operation is captured in the hardware and will always be some constant amount of time; it doesn’t depend on the size of the input to the algorithm you are executing. To actually compare two numbers that can be up to $n$ large, we need to compare $\lceil \log_2 n \rceil$ bits, so it *isn’t* constant time. If we assume, however, that any number that is smaller than the input size *can* be represented in a physical register, then the hardware *can* operate on it in constant time. On any actual computer, the size of the numbers it can represent is bounded by a constant, but so is the size of any data we can actually give the computer, and if we start assuming that all our input data is bounded by a constant, then the mathematics we use to reason about algorithmic efficiency becomes meaningless. We have to consider the size of the input a variable that is not constant, and it is to make those two ends meet that we make assumptions about how large numbers we can represent in computer words.

You rarely have to worry about this when you think about algorithmic efficiency, though. As long as you do not work with numbers that are larger than the size of the input, the RAM model says that

1. You can compare numbers in constant time.
2. You can access any computer word in constant time.
3. You can do any arithmetic or bit-wise operation on computer words in constant time.

This is a great simplification of actual hardware. If we make these simple assumptions about how fast our model of a computer can do computations, and we assume that all such operations take the same amount of time, then we can simply count how many of these primitive operations an algorithm will do on any given input to get an estimate of how fast it will run.

Let us try counting primitive operations in a small example. Let us assume that we have a list of numbers,

```python
numbers = [1, 2, 3, 4, 5]
```

In this case, we have $n=5$ numbers. We usually do not count the operations it takes to get our input. Sometimes, we need to read it in from disk, or in this example, we need to tell Python what the numbers are, and this will take around $n$ operations to create this list. If the data is part of our input, however, we do not count those instructions. The reason for this is that there are many algorithms where we use less than $n$ operations to answer questions about our data, and we do not want to have our analysis of the efficiency of these dominated by how long it takes to get the input. Such algorithms are used as building blocks for other algorithms that will responsible for getting the input in the right form for what we need, and we consider that a cost of the other algorithm. If you need to create a list as part of your algorithm, the number of operations that take have to be included in the accounting, but we simply assume that the input is present before we start counting.

Let us now consider the task of adding all these numbers up. We can do this using this algorithm:

```python
accumulator = 0
for n in numbers:
	accumulator += n
```

First, we create a variable to store intermediate values, `accumulator`. Then we have a `for`-loop that runs over our input and for each input number add that number to `accumulator`. Initialising a variable with a number amounts to setting a computer word to a value, so it takes 1 operation in our RAM model. Adding two numbers is also a single operation, and on all modern computers you can add a number to an existing computer word as a single operation, so we consider `accumulator += n` as a single operation—if you want to think about it as `accumulator = accumulator + n` you can count it as two operations. If we consider updating `accumulator` a single operation, we have $1 + n$ operations for initialising and updating `accumulator`. It is less straightforward to know how many operations we spend on the `for`-loop however. That depends on how Python handles `for`-loop statements. The `for`-loop construction is not as primitive as the RAM model. We can implement the same loop using a `while`-loop, however, using only primitive operations:

```python
accumulator = 0
length = len(numbers)
i = 0
while i < length:
	n = numbers[i]
	accumulator += n
	i += 1
```

Here, we have a clearer view of the primitive operations a `for`-loop likely implements. To iterate over a list we need to know how long it is.^[In Python, iterating over sequences doesn’t actually require that we know how many elements we have, because the `for`-loop construction is vastly more powerful than what we implement with this simple `while`-loop, but using the length is the simplest we can do if we implement the `for`-construction ourselves.] The statement

```python
length = len(numbers)
```

isn’t actually primitive either. It uses Python’s `len` function to get the length of a sequence, and we don’t know how many primitive operations *that* takes. If we assume, however, that lists store their lengths in a single computer word, which they really do, and that `len` simply gets that computer word, which is not far from the truth, we can consider this statement a single operation as well.^[Again we have an operation that Python provides for us, like the `for`-loop construction, that is actually more powerful than we give it credit for when we count the number of primitive operations the operation takes. We have to stop somewhere for the example, however, so this is as deep I want to take us down the rabbit hole.]

```python
accumulator = 0.        # one operation
length = len(numbers)   # one operation
```

We then initialise the variable `i` that we use to keep track of how many elements we have processed. That is also a single primitive operation.

The `while`-loop construction does two things. It evaluates its condition and if it is `True` it executes the code in the loop body; if it is `False`, it sends the program to the point right after the loop to start executing the statements there, if any. We call such an operation a *branching operation* because it sends the point of execution to one of two places, and such an operation is implemented as a single instruction in hardware, so we will count the decision of where to continue execution as a single operation. We need to evaluate the loop condition first, however, which is a single numeric comparison, so that takes another primitive operation. We evaluate the loop-condition once for each element in `numbers`, i.e., $n$ times, so now we have:

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
  ...
```

Each of the statements inside the loop body will be evaluated $n$ times, so we just need to figure out how many operations are needed for each of the statements there and multiply that with $n$. The last two statements are simple, they update a number by adding another number to them, and that is a single operation.

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
	n = numbers[i]      # ? * n operations
	accumulator += n    # 2n operations
	i += 1              # 2n operations
```

The remaining question is how many operations it takes to compute `n = numbers[i]`. Here, we need to understand what something like `numbers[i]` means. Once again, we use a Python construction that we do not necessarily know how many operations take to execute. To understand that, we need to know how Python represents lists in computer memory, and that is beyond the scope of this chapter. You will have to take my word on this, but it is possible to get the $i$’th element in a list by adding two computer words and then reading one, so we count this as two operations and end up with:

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
	n = numbers[i]      # 2n operations
	accumulator += n    # n operations
	i += 1              # n operations
```

If we add it all up, we get that to compute the sum of $n$ numbers this way, we need to use $3 + 6n$ primitive operations.

If we assume that all `for`-loops are implemented by first capturing the length of the sequence, then setting a counter to zero, and then iterating thought the sequence, comparing the counter with the length as the loop-condition and incrementing the counter at the end of the loop-body, and accessing elements in the sequence by indexing, i.e., that `for`-loop

```python
for element in a_list:
	# do something
```

can be translated into this `while`-loop

```python
length = length(a_list)        # 1 operation
counter = 0                    # 1 operation
while counter < length:        # 2n operations
    element = a_list[counter]  # 2n operations
    # do something
    counter += 1               # n operations
```

we can say that a `for`-loop iterating over a sequence of length $n$ takes $5n + 2$ operations. If we go back to our original `for`-loop adding algorithm we then get

```python
accumulator = 0       # 1 operation
for n in numbers:     # 5n + 2 operations
	accumulator += n  # n operations
```

which gives us the same $6n + 3$ operations as before.

If you feel a bit overwhelmed by how difficult it is to count the number of primitive operations in something as simple as this, I understand and feel with you. If it is this hard to count operations in the abstract RAM model, where we even make some assumptions about how many operations Python needs to implement its operations, you will understand why we do not even attempt to take into account that different operations take different time and that data access time depends on memory hierarchies.

The good news is that we very rarely do count the number of operations an algorithm needs this carefully. Remember that I said, at the beginning of the chapter, that we generally consider functions such as $2x^2+x$ and $100x^2+1000x$ the same. We will also consider $6n + 3$ the same as $n$ general number of operations, in a way I will explain a little later. First, however, you should try to count primitive operations in a few examples yourself.

### Counting primitive operations execises

**Exercise:** Consider this way of computing the mean of a sequence of numbers:

```python
accumulator = 0
for n in numbers:
	accumulator += n
mean = accumulator / len(numbers)
```

Count how many primitive operations it takes. To do it correctly you need to distinguish between updating a variable and assigning a value to a new one. Updating the accumulator `accumulator += n` usually maps to a single operation on a CPU because it involves changing the value of a number that is most likely in a register. Assigning to a new variable, as in

```python
mean = accumulator / len(numbers)
```

doesn’t update `accumulator` with a new value, rather it needs to compute a division, which is one operation (and it needs `len(numbers)` before it can do this, which is another operation), and then write the result in a new variable, which is an additional operation.

**Exercise:** Consider this alternative algorithm for computing the mean of a sequence of numbers:

```python
accumulator = 0
length = 0
for n in numbers:
	accumulator += n
	length += 1
mean = accumulator / length
```

How many operations are needed here? Is it more or less efficient than the previous algorithm?

## Best-case, worst-case, and average-case efficiency

When computing the sum of a sequence of numbers, the number of operations we need is independent of what the actual numbers are, so we could express the number of operations as a function of the number of numbers, $n$. Things are not always that simple.

Consider this algorithm for determine if a number $x$ is in the sequence of numbers `numbers`:

```python
in_list = False
for n in numbers:
	if n == x:
		in_list = True
		break
```

Here, we assume that both `numbers` and `x` are initialised beforehand. The algorithm sets a variable, `in_list`, to `False`—one primitive operation—and then iterates through the numbers. This would usually cost us $5n + 2$ operations for the `for`-loop, but notice that we `break` if we find the value in the list. We won’t spend more than $5n + 2$ operations on the `for`-loop, but we might spend less. Let us say that we iterate $m$ times, so the cost of the `for`-loop is $5m + 2$ instead. Then we can count operations like this:

```python
in_list = False           # 1 operation
for n in numbers:         # 5m + 2 operations
	if n == x:            # 2m operations
		in_list = True    # 1 operation 
		break             # 1 operation
```

The `if`-statement is a branching operation just like the `while`-loop; we need to compare two numbers, which is one operation, and then decide whether we continue executing inside the `if`-body or whether we continue after the `if`-block, which is another operation. Inside the `if`-block, we have two primitive operations. Setting the variable `in_list` is a primitive operation like before, and `break` is another branching operation, that does not need a comparison first, that simply moves us to after the loop. These two operations are only executed once, since they are only executed when the `n == x` expression evaluates to `True` and we leave the loop right after that.

When we add up all the operations we find that the algorithm takes $7m + 5$ operations in total, where $m$ depends on the input. It is not unusual that the running time of an algorithm depends on the the actual input, but taking that into account vastly complicates the analysis. The analysis is greatly simplified if we count the running time as a function of the size of the input, $n$, and do not have to consider the actual data in the input.

For this simple search algorithm, we clearly cannot reduce the running time to a function of $n$, because it depends on how soon we see the value $x$ in the sequence, if at all. To get out of the problem, we usually consider one of three cases: the *worst-case* complexity of an algorithm, the *best-case* complexity, or the *average-case* complexity.

To get the worst-case estimate of the number of operations the algorithm needs to take, we need to consider how the data can have a form where the algorithm needs to execute the maximum number of operations it can possibly do. In the search, if $x$ is not found in the sequence we will have to search the entire sequence, which is as bad as it gets, so the worst-case time usage is $7n + 5$. If the very first element in `numbers` is $x$, then $m=1$, which is as good as it gets, so the best-case running time is $12$ operations.

The average-case is much harder to pin down. To know how well our algorithm will work on average we need consider our input as randomly sampled from some distribution—it does not make sense to think of averages over something that doesn’t have an underlying distribution the data could come from, because then we would have nothing to average over. Because of this, we will generally not consider average-case complexities in this book. Average-case complexity is an important part of so-called randomised algorithms, where randomness is explicitly used in algorithms, and where we have some control over the data. To get a feeling for how we would reason about average-case complexity, imagine for a second that $x$ is in the `numbers` list but at a random location. I realise that would make it very silly to run the algorithm in the first place, since the result will always be `True`, but indulge me for a second. If $x$ is a random number from `numbers`, then the probability that it is at index $j=1,\ldots,n$ and is $1/n$, in which case $m=j$. The mean $m$ is then $\mathrm{E}[m]=\sum_{j=1}^n j/n$ which is $\frac{n+1}{2}$. Plugging that into $7m + 5$ we get the average-case time usage $7/2(n+1)+5$.

Of the three cases, we are usually most concerned with the worst-case performance of an algorithm. The average-case is of interest if we have to run the algorithm many times over data that is distributed in a way that matches the distributions we use for the analysis—where the average over the input the algorithm will actually be run on matches the average-case analysis—but for any single input, it does not tell us much about how long the algorithm will actually run. The best-case is only hit by luck, unless we can somehow control what the data looks like, so relying on that in our analysis is often overly optimistic. The worst-case performance, however, gives us some guarantees about how fast the algorithm will be on any data, however pathological, we give it.

### Exercise

Recall the guessing game from last chapter, where one play thinks a number between 1 and 20 and the other has to guess it. We had three strategies for the guesser:

1. Start with 1. If it isn't the right number, it has to be too low--there are no smaller numbers the right one could be. So if it isn't 1, you guess it is 2. If it isn't, you have once again guessed too low, so now you try 3. You continue by incrementing your guess by one until you get the right answer.
2. Alternatively, you start at 20. If the right number is 20, great, you got it in one guess, but if it is not, your guess must be too high--it cannot possibly be too small. So you try 19 instead, and this time you work your way down until you get the right answer.
3. Tired of trying all numbers from one end to the other, you can pick this strategy: you start by guessing 10. If this is correct, you are done, if it is too high, you know the real number must be in the interval $[1,9]$, and if the guess is too low, you know the right answer must be in the interval $[11,20]$--so for your next guess, you pick the middle of the interval it must be. With each new guess, you update the interval where the real number can be hidden and pick the middle of the new interval.

Identify the best- and worst-case scenarios for each strategy and derive the best-case and worst-case time usage.

## Asymptotic running time and big-Oh notation

As we have seen, counting the actual number of primitive operations used by an algorithm quickly becomes very tedious. Considering that all operations are not equal, different arithmetic operations will take different time to compute, and considering that memory hierarchies make access to different data more or less expensive, often by orders of magnitude, it also seems to be a waste of time to be that precise in counting operations. If adding ten numbers can be much faster than multiplying three, why should we worry that the first involves nine operations and the second only two? In fact, we very rarely do count the actual operations, and when we do, it is for very simple but important algorithms—and when we do, we also take into account the hardware the operations will run on, so such counting doesn’t generalise from one computer to another. What we really care about is how the running time (or memory or disk space usage, or whatever critical resource our algorithm uses) grows as a function of the input size in some rough sense, that we will now consider.

When we consider a function of the input size, $f(n)$, we care about the *order* of $f$. We classify functions into classes and only care about which class $f$ belongs to, not what the actual function $f$ is. Given a function $f$ we define its “order class” $O(f)$ as:

$$O(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq g(n) \leq c f(n) \,\right\}$$
	
Put into words, this means that the class of functions of order $f$ are those functions $g(n)$ that, as $n$ goes to infinity, will eventually be dominated by $f(n)$ times a positive constant $c$.

If $g$ is in $O(f)$ we write $g\in O(f)$, as per usual mathematical notation, but you will often also see it written as $g=O(f)$. This is abuse of notation, but you might run into it, so I should mention it, even if I will not use that notation myself.

Consider functions $f(n) = 10n + 100$ and $g(n) = 3n + 100,000$. We have $g\in O(g)$ because, eventually, a line with incline 10 will grow faster than one with incline 3, so eventually, regardless of where the lines intersect the $y$-axis, the former will be above the latter. We also have $f\in O(g)$ because we can simply multiply $g(n)$ by $10/3$ and it is already above $f(n)$ because it intersects the $y$ axis at a higher value. This “big-Oh” notation is not symmetric, however. Consider the function $h(n)=n^2$. We have both $f\in O(h)$ and $g\in O(h)$ because, regardless of where a line intersects the $y$-axis, $n^2$ will eventually be larger than the line. We do *not* have $h\in O(f)$, however, for exactly the same reason. There is no constant $n_0$ after which a line, regardless of what its intercept is, will always be larger than $n^2$.

When we write $f\in O(g)$ we provide an *upper bound* on $f$. We know that as $n$ goes to infinity, $f(n)$ will be bounded by $cg(n)$ for some constant $c$. The upper bound need not be exact, however, as we saw with $h(n)$. We have a separate notation for that, the “big-Theta$ notation:

$$\Theta(f) = \left\{\,
g(n): \exists c_1,c_2\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq c_1 f(n) \leq g(n) \leq c_2 f(n)
\,\right\}$$

The class of functions $\Theta(f)$ are those that, depending on which constant you multiply them with, can be both upper- and lower-bounds of $f(n)$ as $n$ goes to infinity. This notation *is* symmetric: $g\in\Theta(f)$ if and only if $f\in\Theta(g)$. For our two lines, $f(n) = 10n + 100$ and $g(n) = 3n + 100,000$, we have $f\in\Theta(g)$ and $g\in\Theta(f)$, and we can in good conscience write $\Theta(f)=\Theta(g)$ as they are the same classes of functions. We can also write $O(f(n))=O(g(n))=O(n)$ since these are also the same classes, but $O(f)\neq O(h)$ because $O(h(n))=O(n^2)$ is a large class than $O(n)$. There are functions that are bounded, up to a constant, by $n^2$ that are not bounded by $n$, regardless of which constant we multiply to $n$.

The big-Oh notation can be thought of as saying something like “less than or equal to”. When we say that $g\in O(f)$ we are saying that $g$ doesn’t grow faster than $f$, up to some constant and after a certain $n_0$. Because the notation is concerned with what happens as $n$ goes to infinity, it captures *asymptotic behaviour*. This is just a math term for saying that we concern ourselves with what a function does as it moves towards a limit, in this case infinity. When we analyse algorithms, we usually care more about how efficient they are on large input data compared to how they behave on small input data, so we care about their asymptotic efficiency more than we care about the details of how they behave for small $n$.

Consider algorithms that use $n^2$ versus $10n + 100$ operations on input of size $n$. For small $n$, the $n^2$ algorithm will use fewer operations than the $10n + 100$ algorithm, but unless we expect that the input size will always be small, the $10n + 100$ algorithm is superior. As $n$ grows, it will eventually be *much* faster than the $n^2$ algorithm.

When we have an algorithm that uses $f(n)$ operations for input of size $n$, and we know $f\in O(g)$, then we know that $g$ will eventually bound the running time for the algorithm. We will say that “the algorithm runs in time $O(g)$”—or usually qualify that we consider worst-case complexity, say, by saying that “the algorithm runs in worst-case time $O(g)$”.

Consider the example from earlier where we added $n$ numbers together. We counted that this involved $6n + 3$ primitive operations. If we pick any constant $c>6$, then $cn$ will eventually be larger than $6n + 3$, so we can say that the algorithm runs in time $O(n)$. The cost of each individual operation will vary according the the actual hardware you will execute the algorithm on, and it will not be the same cost for each operation, so knowing that the running time is linear, i.e., that the algorithm runs in time $O(n)$, is much more important to know than knowing the exact number of operations. Therefore, we usually do not care about the actual number of operations but only the big-Oh of them.

The big-Oh notation is the most used notation of the two we have seen. It is sometimes easier to show that an algorithm does not do more than a certain number of operations, $f(n)$, that to show that it doesn’t do fewer. In many cases, we can show that an algorithm’s running time is in big-Theta, $\Theta(f)$, which gives us more information about the actual running time, but it can at times be harder to get such an exact function of its running time. When we use big-Oh, it is not a problem to over-count how many operations we actually use; when we use big-Theta, we have to get it right.

### Other classes

In the vast majority of times when we use this asymptotic notation, we use big-Oh. We often do that even when we know that an algorithm’s complexity accurately and could use big-Theta. This is simply because we usually care much more about bounding the running time of an algorithm than we do about getting the running time exactly right. For completeness, however, I will quickly define a few more classes, in case you run into these outside fo this book.

The big-Oh class of a function is all functions it is an upper bound of. If $g\in O(f)$, then after some $n_0$ and for some constant $c$, $g(n) \leq c f(n)$. If we can change the inequality sign to be less, rather than less-than, we get a different class of functions, called “little-oh”:

$$o(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq g(n) < c f(n) \,\right\}$$
	
If $g\in o(f)$ then we know that $g\in O(f)$ but that $f\not\in O(g)$. Asymptotically, $f$ grows strictly faster than $g$.

We also have notation for lower bounds, similar to big-Oh and little-oh: we have big-Omega and little-omega:

$$\Omega(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq f(n) \leq c g(n) \,\right\}$$
$$\omega(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq f(n) < c g(n) \,\right\}$$

These captures lower bounds, strict or otherwise, and since $g\in\Theta(f)$ means that, asymptotically, we have both that $f$ can dominate $g$ and that $g$ can dominate $f$, we have that $g\in\Theta(f)$ is equivalent to saying $g\in O(f)$ as well as $g\in\Omega(f)$.

When we reason about an algorithms efficiency, we care about upper bounds and use the oh-notation; if we instead worry about how intrinsically complex problems are, we usually care about the lower bounds for any algorithm that solves them will have to be, and use the omega-notation. If we have a problem that we know requires $\Omega(f)$ to solve, but we also have an algorithm that solves it in $O(f)$, we have a tight bound of $\Theta(f)$. We do not concern ourselves much with the complexity of problems in this book, but it will come up from time to time, so keep this in mind.

### Properties of complexity classes

These classes of functions have a few rules that are easy to derive, although we will not do this here, that can help us reason about them in general. As long as we do not use the strict upper- and lower-bound classes, a function is going to be in its own class:

$$f \in \Theta(f)$$
$$f \in O(f)$$
$$f \in \Omega(f)$$

There is a symmetry between upper- and lower bounds:

$$f\in O(g) \quad\text{ if and only if }\quad g\in\Omega(f)$$
$$f\in o(g) \quad\text{ if and only if }\quad g\in\omega(f)$$

Furthermore, there is transitivity in these senses:

$$f\in\Theta(g) \land g\in\Theta(h) \implies f\in\Theta(h)$$
$$f\in O(g) \land g\in O(h) \implies f\in O(h)$$
$$f\in\Omega(g) \land g\in\Omega(h) \implies f\in\Omega(h)$$
$$f\in o(g) \land g\in o(h) \implies f\in o(h)$$
$$f\in\omega(g) \land g\in\omega(h) \implies f\in\omega(h)$$

I have personally found very few opportunities where these rules have been useful, but if you can see why the rules must be true, you have taken a big step towards understanding this asymptotic function notation.

### Reasoning about algorithmic efficiency using big-Oh notation

Here is how we would reason about an algorithm when using big-Oh notation. Everything that we only do once, is simply $O(1)$. Regardless of what the actual cost of an operation is, say $c$, it is still $O(1)$ because $O(1)=O(c)$ for all constants. It also doesn’t matter how many things we do, as long as we do them once, because $O(1 + 1)=O(1)$, after all, $O(2)=O(1)$. Whenever we loop over $n$ elements, that loop costs us $O(n)$ operations. Any operation that happens inside the loop body can be multiplied by $n$ to account for its cost. It doesn’t matter if we actually *do* compute that operation for each iteration; the big-Oh notation gives us an upper bound for the running time, so if we count an operation more often than we really should, that is okay.

In the summation algorithm, we therefore have:

```python
accumulator = 0       # O(1)
for n in numbers:     # O(n)
	accumulator += n  # n * O(1)
```

where $n \times O(1)$ should be read as $O(n)$—we are really saying that we do something that has a constant cost, say $c$, but we do it $n$ times, so we do something that costs $cn$ which is in $O(n)$.

With the search algorithm, we can reason similarly and get:

```python
in_list = False           # O(1)
for n in numbers:         # O(n)
	if n == x:            # n * O(1)
		in_list = True    # n * O(1)
		break             # n * O(1)
```

We know that we only ever execute the body of the `if`-statement once, but it is easier just to multiply all the operations inside the loop by $n$, and since the big-Oh notation only guarantees us that we have an upper bound, it works out okay.^[In this particular case, we could just as easily count the `if`-body as constant time as well, but it doesn’t matter for the big-Oh analysis. The entire algorithm runs in $\Theta(n)$, so we are not even ending up with a strict upper bound but the actual asymptotic running time.]

To make the big-Oh reasoning about these algorithms even easier, can work out how to add big-Oh expressions together.

### Doing arithmetic in big-Oh

When analysing algorithms like we just did, we often end up with multiplying a function to a big-Oh class, as we did with $nO(1)$, and if we want to combine a number of computations where we know their big-Oh class, we need to add them together, that is we want to know what $O(f) + O(g)$ means. 

First, we need to know how to do arithmetic on functions, but that is probably familiar to you. If you have a function $f$ and multiply it with a constant, $c$, you have $cf$ which is the function $h(n)=c f(n)$. Similarly, $f+c$ is the function $h(n)=f(n) + c$. If you have two functions, $f$ and $g$, then $fg$ is the function $h(n)=f(n)g(n)$ and $f+g$ is the function $h(n)=f(n)+g(n)$.


For the rules for doing arithmetic with big-Oh notation let $f$, $f_1$, $f_2$, $g$, $g_1$, and $g_2$ denote functions.

The first important rule is this:

$$f_1\in O(g_1) \land f_2\in O(g_2) \implies f_1+f_2\in O(g_1+g_2)$$

We can also write this as:

$$O(f) + O(g) = O(f + g).$$

Since $f+f$ is the same as $2f$ we furthermore have that

$$f_1\in O(g) \land f_2\in O(g) \implies f_1+f_2\in O(g)$$

This means that if we can bound two separate pieces of the running time of an algorithm by the same function, then the entire algorithm’s running time is bounded by that. Because of the transitivity rules for big-Oh this also means that if $f\in O(g)$ then $f+g\in O(g)$. So when we add together different parts of a runtime analysis, whenever one part dominates the other, we can simply discard the dominated part. So, if we add a constant number of operations, $O(1)$, to a linear number of operations, $O(n)$, as we have to in our examples, we end up with something that is still linear $O(n)$.

We also have a rule for multiplying functions:

$$f_1\in O(g_1) \land f_2\in O(g_2) \implies f_1f_2\in O(g_1g_2)$$

or

$$O(f)\times O(g) = O(fg).$$

This rule is particularly useful when we reason about loops. The time it takes to execute a loop over $n$ elements is $O(n)$ times the time it takes to execute the loop body, which could be some function $f(n)$, so the total running time for such a loop is $n\times f(n)\in O(n f(n))$.

We also have rules for multiplying into big-Oh classes:

$$f \cdot O(g) = O(fg)$$

We can use this in our examples where we have expressions such as $n\times O(1)$. This simply becomes $O(n)$. From this rule it follows that when $c$ is a non-zero constant we have:

$$c \cdot O(f) = O(f)$$

You can see this simply by considering $c$ a constant function: $c(n) = c$ and then apply the previous rule. 

We can now apply these rules to get a final, big-Oh, running time for our example algorithms. Consider first the summation:

```python
accumulator = 0       # O(1)
for n in numbers:     # O(n)
	accumulator += n  # n * O(1)
```

We have to work out $O(1) + O(n) + n\times O(1)$. This is the same as $O(1) + O(n) + O(n)$ by the rule for how to multiply into a big-Oh class, and then, by the addition rule, we get $O(1 + n + n)=O(n)$.

For the search we have

```python
in_list = False           # O(1)
for n in numbers:         # O(n)
	if n == x:            # n * O(1)
		in_list = True    # n * O(1)
		break             # n * O(1)
```

which breaks down to $O(1) + O(n) + 3\times n\times O(1) = O(n)$.

Consider now a different algorithm:

```python
for i in range(1,len(numbers)):
	x = numbers[i]
	j = i - 1
	while x < numbers[j]:
		numbers[i], numbers[j] = numbers[j], x
		i -= 1 ; j -= 1
```

This is an algorithm called “bubble sort”, and we return to it in the next chapter where we prove that it sorts `numbers`. Here, we will just consider its running time. The algorithm consists of two nested loops. So we reason as follows: the outer loop iterates over `numbers`, which gives us an $O(n)$ contribution. It doesn’t include the first element in `numbers`, but that doesn’t change anything since $O(n-1)=O(n)$. Inside the loop body we have two constant time contributions and an inner loop. The constant contributions gives us $O(1)$ and the inner loop is bounded by $O(n)$—we prove this in the next chapter. So we have the running time 

$$O\left(n\right)\times\left(O\left(1\right)+O\left(n\right)\right)=O\left(n^2\right).$$

In other words, this sorting algorithm runs in quadratic time. Or rather, we should say that its *worst-case* running time is $O\left(n^2\right)$. In the analysis, we assumed that the inner loop could execute $n$ iterations; if the list is already sorted, then `x < numbers[j]` is always `False`, and the inner loop will actually take constant time. So the *best-case* running time is only $O(n)$.

Now consider this algorithm, called *binary search*. This is another algorithm we return to in the next chapter. It searches for a number, $x$, in a sorted list of numbers using the third strategy from the guessing game we have seen a few times by now. It has an interval in which to search, bounded by `low` and `high`, and if $x$ is in the list, we know that it must be between these two indices. We pick the midpoint between these two numbers and check if we have found $x$, in which case we terminate the search. If we haven’t, we check if the midpoint number is smaller or larger than $x$. If it is smaller, we move `low` up to the midpoint plus one—if $x$ is in the sorted list it must be later than the midpoint. If it is larger, we move `high` down to the midpoint. We do not move it to the midpoint minus one. If we did, it is possible to miss the index where $x$ is hiding; this can happen if the interval is of length one and `low` is the index where $x$ resides, as we will see next chapter.

```python
low, high = 0, len(numbers)
found = False
while low < high:
	mid = (low + high) // 2
	if numbers[mid] == x:
		found = True
		break
	elif numbers[mid] < x:
		low = mid + 1
	else:
		high = mid
```

We can analyse its complexity as follows: we have some constant time operations at the beginning, $O(1)$, and then a loop. It is less obvious how many iterations the loop will make in this algorithm than in the previous ones we have looked at, but let us call that number $m$. Then the loop takes time $O(m)$ times the time it takes to execute the loop body. All the operations inside the loop are constant time, $O(1)$, so we have

$$O\left(1\right)+O\left(m\right)\times O\left(1\right)=O\left(m\right).$$

Of course, this isn’t quite satisfying. We don’t know what $m$ is, and we want the running time to be a function of $n$.

The interval we are searching in decreases in each iteration, so if we start with an interval of size $n$ we could, correctly, argue that the loop is bounded by $n$ as well, and derive a running time of $O(n)$. This is correct; $O(n)$ is an upper bound for the (worst-case) complexity. In fact, however, the algorithm us much faster than that. It actually runs in $O(\log_2 n)$, where $\log_2$ is the base-two logarithm. To see this, consider how much we shrink the interval by in each iteration. When we update the interval, we always shrink the interval to half the size of the previous one. So we can ask ourselves, how many times can you halve $n$ before you get below one? That is $\lceil\log_2 n\rceil$, so $m$ is bounded by $O(\lceil\log_2 n\rceil)$. We typically do not include the rounding-up notation when we write this but consider it implicit, so we simply write $O(\log_2 n)$.

### Important complexity classes

Some complexity classes pop up again and again in algorithms, so you should get familiar with them. From fastest to slowest, these are:

* Constant time: $O(1)$. This is, obviously, the best you can achieve asymptotically, but since it does not depend on the input size, we very rarely see algorithms running in this time. 
* Logarithmic time: $O(\log n)$. We saw that binary search was one such algorithm. Generally, we see this complexity when we can reduce the size of the input we look at by a fixed fraction in each iteration of a loop. Typically, we can cut the data in half and we get a base-two logarithm, but since the difference between two different-based logarithms is a constant, $\log_a(x)=1/\log_b(a)\cdot\log_b(x)$, we rarely write the base in big-Oh notation.
* Linear time: $O(n)$. We saw several examples of linear time algorithms in this chapter. Whenever we do something where we have to, in the worst case, examine all our input, the complexity will be at least this.
* Log-linear time: $O(n\log n)$. We haven’t seen examples of this class yet, but it will show up several times in the Divide-and-conquer chapter.
* Quadratic time: $O(n^2)$. We saw that bubble sort had this complexity. This complexity often shows up when we have nested loops.
* Cubic time: $O(n^3)$. This is another class we haven’t seen yet, but when we have three levels of nested loops, we see it. If you multiply two $n\times n$ algorithms the straightforward way,^[It is possible to multiply matrices faster than this, but that is beyond this book.]  $C=AB$, you have to compute $n\times n$ values in $C$, and for each $c_{ij} = \sum_k a_{ik}b_{kj}$, you sum $n$ values together, giving you a total running time of $n^3$.
* Exponential time: $O(2^n)$. You should avoid this complexity like the plague. Even for tiny $n$, this running time is practically for ever. It does, unfortunately, pop up in many important optimisation problems where we do not know of any faster algorithms. If you have a problem that can only be solved in this complexity, you should try to modify the problem to something you can solve more efficiently, or you should try to approximate the solution instead of getting the optimal solution. Algorithms that run in this complexity are rarely worth considering.

In [@fig:function-growth] I have plotted the growth of the different classes. As you can see in the upper-left frame, logarithmic growth is very slow compared to linear growth. On the upper-right you can see that log-linear growth is slower than linear, but compared to quadratic time, it is much faster. Cubic time, lower-left, is much slower than quadratic time, and exponential time, lower-right, just grows to infinity before you even get started.

![Growth of different complexity classes.](figures/function-growth){#fig:function-growth}

### Asymptotic complexity exercises

#### Bubble sort

Consider the bubble sort algorithm. We argued that the worst-case running time was $O\left(n^2\right)$ but the best-case running time was $O\left(n\right)$.

**Exercise:** Describe what the input data, `numbers`, should look like to actually achieve the worst- and best-case running times.

### Binary search

We argued that the worst-case running time for binary search is $O\left(\log n\right)$. **Exercise:** What is the best-case running time, and what would the input data look like to achieve it?


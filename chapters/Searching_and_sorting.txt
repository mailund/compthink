# Searching and sorting

In this chapter we will explore two key problems that are the foundations of many other algorithms: searching for an element in a sequence and sorting a sequence. These are very fundamental problems and Python has builtin functionality for solving them, and in most cases where you need to solve this problems, you should just use the existing implementations. They are well engineered and optimised, so you are not likely to implement faster solutions. We consider them here to get a better feeling for algorithms and how to analyse them. That being said, sometimes your data is in a form where customised algorithms will be better than the builtin solutions, so we also discuss pros and cons of the algorithms.

## Searching

We first consider the simplest of the two problems: searching.  Given a list of numbers, `numbers`, and a value, `x`, we want to determine if `x` is in `numbers`. We have one solution to this problem that does not make any assumptions about `numbers`, where we need a *linear search*, and one where we assume that `numbers` are sorted, where we can use *binary search*. The former takes worst-case linear time, thus the name, while the latter only takes logarithmic time. Because of their running time, you should prefer the latter over the former, as a rule of thumb. However, the cost of sorting before you can use binary search should also be taken into account. If you search for $m$ elements in a sequence of $n$ numbers[^search_types], linear search will take time $O(mn)$ while binary search will take time $O(S(n)+m\log n)$ where $S(n)$ is the cost of sorting $n$ numbers. The sorting algorithms we see in this chapter take time $O(n^2)$ or $O(n)$, depending on whether we can bound the values of the numbers. For general numbers it can be shown that any sorting algorithm must take time $\Omega(n\log n)$, and we shall see algorithms that run in this time in when we return to sorting after the divide-and-conquer chapter.

[^search_types]: In this chapter we assume we are working with numbers, but we can search in any type of data. If the data does not have a total order to it, we have to use linear search; if we can define an ordering on the type we search in, we can use both algorithms.

### Linear search

Linear search is straightforward. We loop over all the elements in `numbers` and compare them in turn to `x`. If we see `x` we break the search and report that we found it. If we reach the end of `numbers` without having seen `x`, we report that it wasn’t found.

```python
found = False
for n in numbers:
	if x == n:
		found = True
		break
```

The builtin solution in Python for linear search uses the `in` keyword:

```python
x in numbers
```

That this algorithm has a worst-case running time of $O(n)$ is equally straightforward. The `for`-loop has to run over all the $n$ elements in `numbers` if it doesn’t find $x$ and breaks early. The best-case running time is obviously $O(1)$ because we could get lucky and find $x$ in the very first iteration.

### Binary search

We already saw the binary search algorithm in the previous chapter. It looks like this:

```python
low, high = 0, len(numbers)
found = None
while low < high:
	mid = (low + high) // 2
	if numbers[mid] == x:
		found = mid
		break
	elif numbers[mid] < x:
		low = mid + 1
	else:
		high = mid
```

The algorithm works by keeping track of an interval, `[low,high)`, in which $x$ must be, if it is in the list at all. In each iteration of the `while`-loop we look at the mid-point of the interval (rounded down if the midpoint is not an integer). If the midpoint is equal to $x$ we are done. If it is less than $x$, we know that $x$ must be found in the interval `[mid+1,high)`. The elements are sorted, so if the midpoint is less than $x$, then all elements to the left of the midpoint are less than $x$ as well. If the midpoint is greater than $x$, then we know that all elements to the right of the midpoint are greater than $x$ as well, so if $x$ is in the list, then it must be in the interval $[low,mid)$. Notice the asymmetry in the updated intervals. When we increase `low` we use `mid+1`, but when we decrease `high` we use `mid`. This is a consequence of how we represent the interval. We include the point at `low` in the interval but not the point at `high`. So, when we decrease `high` to `mid`, we have already eliminated the point at `mid` from the search, just as when we increase `low` to `mid+1`.

**Exercise:** Consider a modified solution where we set `high` to `mid-1` instead of `mid`. Give an example where the algorithm would give you the wrong answer.

The builtin solution in Python is implemented in the module `bisect` and looks like this:

```python
import bisect
print(bisect.bisect(numbers, x))
```

The `bisect` algorithm does something *slightly* different from our implementation. It returns an index where $x$ is found, or where it should be put if we want to insert it. We just consider the simpler problem of determining if $x$ is there.

The `numbers` list[^random_access_bsearch] has to be sorted for the algorithm to work. It is this property that gives us information about where to continue the search if the midpoint of the interval is smaller or larger than $x$. If the list were not sorted, knowing whether the midpoint is smaller than or larger than $x$ would give us no information about where $x$ would be found if it were in the list.

[^random_access_bsearch]: The algorithm also exploits that we can do random-access into `numbers`. We can always do that for Python lists, but there are other ways to represent sequential data where this is not true. For those, we need to use linear search.

To hone our skills at reasoning about algorithms, we will formally prove termination and correctness. Termination first. For our termination function we take `high-low`. Clearly, if this function is at or below zero, the `while`-loop condition is false. To see that it decreases in each iteration, we consider the two options for updating the interval. If we set `low` to `mid + 1`, we clearly reduce `high-low`. To see that we also decrease the size of the interval when we update `high`, we observe that `mid < high` when we use integer-division when updating `mid`.

**Exercise:** If we updated `low` by setting it to `mid` instead of `mid + 1`, our reasoning about the termination function fails. Give an example where this would result in the loop never terminating.

To prove correctness, we specify the loop-invariant: `x` is not in the interval `[0,low)` and not in the interval `[high,n)`. This invariant guarantees us that if we reach the point where the `[low,high)` interval is empty, then `x` was not in `numbers`. Since we do not set `found` to true unless we actually see $x$, we also know that we do not report that $x$ *is* in `numbers` unless it is.

We already discussed the algorithms running time in the previous chapter, but to recall: if we reduce the size of a problem from $n$ to $n/2$ in each iteration, and only use $O(1)$ per iteration, then the running time amounts to how many times we can divide $n$ by two until we reach a size of one. That is the base-two logarithm of $n$, so the worst-case running time of binary search is $O(n \log n)$.

## Sorting

The sorting problem is this: given a sequence of elements $x$ of length $n$, create a new sequence $y$ that contain the same element as $x$ but such that $y[i]\leq y[i+1]$ for $i=0,\ldots,n-1$. That sounds simple enough, but there’s a devil in the details.

First of all, what kind of order do we mean when we write $a\leq b$. Not all kinds types of data have a total order. For some data, which we will not consider in this chapter, it is possible to have a *partial order*, which means that for *some* elements $a$ and $b$ we can determine if $a<b$ or $a>b$, but not all. In such a case, we do not require $y[i]\leq y[i+1]$, but only that $y[i]\not\geq y[i+1]$. This is know as *topological sorting*.

In this chapter, we assume that our elements have a *total order*, which means that for any two elements, $a$ and $b$, (exactly) one of these three relations are true: $a<b$, $a=b$, or $a>b$. This is true for numbers or when we consider the alphabetical order, also known as *lexical* order, of strings.

Another issue is whether elements $a=b$ are identical. This might sound trivial, after all, if $a=b$ it means they are the same objects, but it doesn’t actually mean this. To sort elements we need some order on them, but this does not mean that two elements that are identical with respect to this order are indistinguishable. If we sort a list of people by their last name, then we consider “John Doe” and “Jane Doe” as equal with relation to the order—after all “Doe” equals “Doe”—but that does not mean that “John” equals “Jane”. We can resolve this by first sorting by last name and then by first name, but this raises a new problem. If we first sort by last name and then by first name, do we risk of messing up the first order. If we sort “John Doe”, “John Smith”, and “Jane Doe” by last name we get the order “John Doe”, “Jane Doe”, and “Joe Smith” (or maybe we get “Jane Doe”, “John Doe”,  and “Joe Smith” since “Jane Doe” and “John Doe” are equal with respect to last name). If we then sort by first name, we mess up the first sort, because now we might get “Jane Doe”, “Joe Smith”, and “John Doe”.

When we only sort elements based on part of their value, an algorithm can be *stable* or *unstable*. If it is stable it means that elements we consider equal end up in the same order as they appear in the input, if it is unstable, there is no such guarantee. We can sort by last name and then first name using a stable sort algorithm in the opposite order: first name first and last name last. If we sort by first name, we get “Jane Doe”, “Joe Smith” and “John Doe”. If we then sort by last name with a stable sort, “Jane Doe” must appear before “John Doe” because that is the order they are in when we apply the stable sort.

When we sort elements based on part of the data they constitute of we call that part their *key*. When we sort elements, it is based on their keys and we consider two elements equal in their order if they keys are equal. If keys are the only thing we are sorting, i.e., there is no additional data associated with the values beside their keys, then a stable and an unstable algorithm are indistinguishable.

Another issue is how we can compare elements. If all we can do is test which of the three relationships they have, $a<b$, $a=b$, or $a>b$ we are doing what is called *comparison sort*. It can be shown that this cannot be done faster, by any algorithm, than $\Omega(n \log n)$. If our elements are totally ordered and positive integer numbers with values less than some $m$, however, we can sort them in time $O(n+m)$, which if $m\in O(n)$ is linear time. We will see how later in this chapter.

Yet another issue is whether we can sort $x$ by modifying it, turning $x$ into $y$. This issue is relevant for the memory complexity of a sorting algorithm. We have only focused on running time complexity in the previous chapter, but how much memory is used by an algorithm can be equally important. An algorithm that modifies $x$ and only uses a constant amount of extra memory has a $O(1)$ memory usage—we do not count the size of our input as part of the memory cost, just as we do not count the size of the input list when we do binary search. Some algorithms require more memory than that and inherently produce a new list for $y$. You can, of course, always produce a new list for $y$ even if the algorithm you use is based on modifying its input because you can always copy $x$ first, but that takes up extra $O(n)$ memory. Algorithms that modify their input when sorting $x$ are called *in place* and can use less than $O(n)$ memory. Algorithms that produce a new list cannot use less than $O(n)$ memory.

Python lists have a builtin sorting method

```python
x.sort()
```

that implements an algorithm known as *Timsort* (named after Tim Peters who implemented it). This is a comparison based stable sorting algorithms that runs in worst-case time $O(n\log n)$ and best-case time $O(n)$ with $O(n)$ memory usage. It is not entirely in-place and uses worst-case $O(n)$ memory, but it does modify the input list.^[Timsort modifies the input list in-place when it can get away with it but uses additional memory to speed up the computations.]

Calling `x.sort()` modifies `x`, but Python also provides a way to get a sorted copy:

```python
y = sorted(x)
```

This uses the same Timsort algorithm as `x.sort()`; it just doesn’t modify `x` but creates a new copy.

In this chapter we will three comparison based, in-place sorting algorithms that runs in worst-case $O(n^2)$ and use $O(1)$ memory (in addition to the input). Two of them, *insertion sort* and *bubble sort* are stable and have a best-case running time of $O(n)$. The third, *selection sort*, is not stable and has a best-case running time of $O(n^2)$.

After considering comparison based sorting we consider two non-comparison based stable algorithms: *bucket sort* and its big brother *radix sort*. Bucket sort assume that we sort positive integer keys with a max value of $m$ and runs in worst- and best-case $O(n+m)$ while using $O(m)$ memory if we only sort keys but $O(n+m)$ if we have additional data associated with our elements. Radix sort relaxes the  requirement that the keys should be less than $m$ in magnitude but instead assume that we can split keys into $k$ sub-keys that are bounded by $m$. It then uses bucket sort on these keys $k$ times, achieving a best- and worst-case running time of $O(k(m+n)$. Neither of these algorithms are in-place.

### Comparison sorts

The discussion at the beginning of this section was rather long and this point might have been lost in all the discussion, so please recall that when we talk about comparison based sorting we assume that we can answer which of the following relations are true, $a<b$, $a=b$, or $a>b$. We do not assume anything else about the elements.

### Selection sort

A very straightforward comparison sort algorithm is *selection sort*. It works similar to how you might sort elements by hand. You keep a list of sorted elements and a set of yet-to-be sorted elements and one by one you pick the smallest of the unsorted elements and append it to the list of sorted elements. Selection sort follows this strategy but keeps the list of sorted and the set of unsorted elements in the input list, doing in-place sorting and using $O(1)$ additional space. It uses a variable, $i$, that iterates from zero to $n-1$, where $n$ is the length of the input list. The elements below $i$ are kept sorted and are less than or equal to the elements above $i$. Formalised as a loop invariant we can write this as

$$I_1: \forall j\in[0,i) : x[j-1] \leq x[j]$$

and 

$$I_2: \forall j\in[0,i), \forall k\in[i,n) : x[j] \leq x[k]$$

where $x$ is the sequence to be sorted.

If $i$ iterates from $0$ to $n-1$, then in the last iteration, when $i$ is incremented form $n-1$ to $n$, invariant $I_1$ guarantees us that all elements in the range $[0,n)$ are sorted, so the loop invariant guarantees correctness. (We still have to guarantee that we satisfy the invariant, of course). The algorithm will consist of two loops, one nested inside the other, but both will be `for`-loops, iterating through finite length sequences, so termination is also guaranteed.

In each iteration, we locate the index of the smallest element in the range $[i,n)$, call it $j$, and we swap $x[i]$ and $x[j]$, see [@fig:selection-sort-swap] From invariant $I_2$ we know that the $x[j]$ we locate is greater than or equal to all elements in the range $[0,i)$, so when we put it at index $i$, the range $[0,i+1)$ is sorted, satisfying $I_1$. Since $x[j]$ was the smallest element in the range $[i,n)$, we also know that for all $\ell \in [i,n): x[j] \leq x[\ell]$, so after we swap $x[i]$ and $x[j]$—we can call the resulting list $x^\prime$—we have $\ell \in [i+1,n) : x^\prime[i]=x[j]\leq x[\ell]$, satisfying $I_2$. Thus, swapping the smallest element in $[i,n)$ into position $x[i]$ and incrementing $i$ each iteration satisfy the loop invariants, so the algorithm is correct.

![Swapping $x[i]$ and $x[j]$ in selection sort.](figures/selection-sort-swap){#fig:selection-sort-swap}

Implemented in Python, the algorithm looks as follows:

```python
infinity = float("inf")
for i in range(len(x) - 1):
	# find index of smallest elm in x[i:]
	min_idx, min_val = 0, infinity
	for j in range(i, len(x)):
		if x[j] < min_val:
			min_idx, min_val = j, x[j]
			
	# swap x[i] and x[j] puts
	# x[j] at the right position
	x[i], x[min_idx] = min_val, x[i]
```

If we start with this input sequence

```python
x = [1, 3, 2, 4, 5, 2, 3, 4, 1, 2, 3]
```

the states in the algorithm are given by these lists

```python
[1] [3, 2, 4, 5, 2, 3, 4, 1, 2, 3]
[1, 1] [2, 4, 5, 2, 3, 4, 3, 2, 3]
[1, 1, 2] [4, 5, 2, 3, 4, 3, 2, 3]
[1, 1, 2, 2] [5, 4, 3, 4, 3, 2, 3]
[1, 1, 2, 2, 2] [4, 3, 4, 3, 5, 3]
[1, 1, 2, 2, 2, 3] [4, 4, 3, 5, 3]
[1, 1, 2, 2, 2, 3, 3] [4, 4, 5, 3]
[1, 1, 2, 2, 2, 3, 3, 3] [4, 5, 4]
[1, 1, 2, 2, 2, 3, 3, 3, 4] [5, 4]
[1, 1, 2, 2, 2, 3, 3, 3, 4, 4] [5]
[1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5] []
```

as they look at the end of each outer `for`-loop body, where the first list shows the elements in the range $[0,i+1)$ and the second list the elements in the range $[i+1,n)$—before the loop body, we have $[0,i)$ sorted but at the end of the body we have $[0,i+1)$ sorted, which is what I show here.

In each iteration, we locate the smallest element amongst those not yet sorted and append it to the sorted list. If we always picked the left-most minimal value when there were ties, it seems like the algorithm should be stable, i.e. that the input order is preserved when elements have the same key. This reasoning is faulty, however, since it does not take into account exactly *how* we append a minimal value to the sorted list. The swap we use to move the element at index $j$ into index $i$ also moves the element at index $i$ over to index $j$. If there is another index, $k<j$, where the key of $x[k]$ is the same as the key of $x[i]$, then the swap of $x[i]$ and $x[j]$ has changed the order of $x[i]$ and $x[k]$ as well, which a stable algorithm shouldn’t do.

**Exercise:** Give an example input where selection sort is not stable.

It *is* possible to make the algorithm stable, but not by swapping as we do here.^[Using a so-called *linked list* data structure we can implement a selection sort in-place that is stable and still use $O(1)$ additional memory.]

To derive the running time of the algorithm, we see that the outer loop execute $n-1$ operation and the inner look execute $n-i$ iterations in iteration $i$ of the outer loop, which gives us $n + (n-1) + (n-2) + \cdot +1 = \sum_{i=1}^n i = \frac{1}{2}n(n+1) = \frac{1}{2}n^2 + \frac{1}{2}n \in O(n^2)$. The running time does not depend on the actual input, only on the size, so this running time is also the best case running time.

#### Insertion sort

The *insertion sort* algorithm works similar to selection sort in that it does an in-place sorting where it keeps the first part of the input array sorted. 

In Python, it can be implemented as follows:  

```python
for i in range(1,len(x)):
	x = x[i]
	j = i
	while j > 0 and x[j-1] > x[j]:
		x[j-1], x[j] = x[j], x[j-1]
		j -= 1		
```

Just as for selection sort, in iteration $i$, we have the loop invariant:

$$I: \forall j\in[0,i) : x[j-1] \leq x[j]$$

As we also observed for selection sort, this invariant guarantees us that when we reach $i=n$, and the algorithm terminates, all the elements in $x$ are sorted. Unlike selection sort, however, we do not guarantee that the elements to the left of index $i$ are smaller than or equal to the elements to the right of index $i$, the $I_2$ invariant for selection sort. And we do not search for the smallest element in the unsorted part of $x$ in each iteration, but rather move $x[i]$ to the left until we find its proper location in the range $[0,i+1)$. 

When we consider $x[i]$ in iteration $i$, the elements to the left of $i$ are sorted. This means that we can split them into three contiguous regions (all three can potentially be empty), where the $x$ list first contain elements with keys less than $x[i]$’s, then a region containing elements with keys equal to $x[i]$’s and then a region containing elements with keys larger than $x[i]$’s, see [@fig:insertion-sort-swap]. The inner loop in the insertion sort algorithm moves $x[i]$ to the left until it reaches the first index with a key that is equal to or less than $x[i]$. This movement is done through swapping, so a side effect of moving $x[i]$ to the left is that all the elements that end up to the right of the new index for $x[i]$ are moved to the right.

![Moving element $x[i]$ to its correct location in $x^\prime$ by swapping it to the left in insertion sort.](figures/insertion-sort-swap){#fig:insertion-sort-swap}

That the invariant $I_1$ is satisfied after this movement, if it was satisfied before the inner loop, simply by considering the three blocks in [@fig:insertion-sort-swap]. The invariant guarantees us that the two grey segments of $x$ are sorted, so when we extend the region where all keys are the same as $x[i]$’s by one, we still have a sorted list.

To see that the algorithm terminates, we first observe that the outer loop is a `for`-loop through $n-1$ elements, so this is guaranteed to terminate if the inner loop does. For the inner loop, we condition that $j>0$ and we decrease $j$ by one each iteration, so we can use $j$ as a termination function. It  might not actually reach $j=0$ because we also condition on $x[j-1] > x[j]$, but using this termination function we are guaranteed that the inner loop terminates.

For the running time, we can do an analysis similar to selection sort. We iteration $n-1$ times in the outer loop and the inner loop is bounded by $i$ in iteration $i$, so we have the worst-case upper bound $1+2+3+\cdots+(n-2)+(n-1) = \frac{1}{2}(n-1)n\in O(n^2)$. In the best case, however, we only ever test the inner loop condition once, so the best-case time usage is only $O(n)$, unlike selection sort where the best-case is also $O(n^2)$.

**Exercise:** Describe what the input list should look like to achieve best-case and worst-case time performance, respectively.

Now, since selection sort and insertion sort both have the same worst-case running time but insertion sort has a better best-case running time—and would be expected to perform better every time we have runs of consecutive, non-decreasing elements in the input—we could argue that insertion sort is always the better choice. If the elements are all single computer words, so so swapping elements cost no more than decreasing a counter, this would be true. The elements in the input list *could*, however, be more complex, and swapping could therefore be a more expensive operation. If that is the case, we would also have to take into account the cost of swapping. Selection sort swaps $O(n)$ elements on all input while insertion sort swaps $O(n^2)$ elements, worst-case, so for some applications, selection sort could still outperform insertion sort. If we need the sort to be stable, however, selection sort, as we have seen it, cannot be used.

#### Bubble sort

The *bubble sort* algorithm is a stable sort that runs in the same complexity as insertion sort, worst-case $O(n^2)$ and best-case $O(n)$ with worst-case $O(n^2)$ swaps. 

```python
while True:
	swapped = False
	for i in range(1,len(x)):
		if x[i-1] > x[i]:
			x[i-1], x[i] = x[i], x[i-1]
			swapped = True

	if not swapped:
		break
```


### Non-comparison sorts

#### Bucket sort

#### Radix sort


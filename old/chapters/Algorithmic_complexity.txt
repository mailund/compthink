
# Algorithmic efficiency {#sec:algorithmic-efficiency}

As a general rule, we want our algorithms to work as efficiently as possible, which usually means that we want them to quickly solve the problem they are designed for using as few resources, such as memory or disk space, as possible. This is just a rule of thumb, of course, because we also need to factor in programmer time. If you need a problem solved a month from now, and you have the choice between an algorithm that can solve the problem in two weeks and takes one week to implement, that is preferable over an algorithm that can solve the problem in an hour but takes a month to implement.

It is notoriously hard to estimate how long it will take to implement a new algorithm, however, as this depends on the skills of the programmer, the chosen programming language, and which libraries are available; we will not attempt to do that here. With experience, you will get a feel for how hard it will be to implement any given algorithm, and while you are unlikely ever to be able to estimate precisely how much work goes into implementing one, you will be able to determine which of two choices is the more complex algorithm. Unless there are good reasons for wanting a more complicated but potentially more efficient algorithm, you should go for the most straightforward algorithm that can get the job done. Sometimes, the simplest algorithm *will* be too slow for your problem, and you have to replace it with a more complex one. Or sometimes you expect to run your algorithm many times on many data sets, and spending a little more programming time for faster execution down the line might pay off. But as a general rule, you do not want to spend more time programming than what is warranted for your problem. Programmer-time is much more valuable than computer-time, so simple algorithms that can be implemented quickly are generally preferable to more sophisticated algorithms that are hard to implement.

This chapter is not about the complexity of algorithms in the sense of how complicated they are to implement. Instead, it is about how efficiently algorithms solve a problem once implemented. That is, the chapter is about how fast you can expect an algorithm to complete its job on any given data. Whenever you have the choice between two algorithms that look roughly equally complicated to implement, you want to go with the one that is likely to be the most efficient of the two.

The best way to measure which of two algorithms is superior for solving a given problem is, of course, to program the best implementations of the two that you can manage and then measure how fast they are on your data. Whichever you stopwatch tells you is the best, unquestionably is. That, of course, is not an option if your focus is on actually solving a problem rather than comparing two algorithms. To answer your question that way, you have to do twice the work. You need to implement both algorithms and then run both of them on your actual data. What we need is a way of reasoning about how likely it is that one algorithm is better than another without using a stop-watch.

The way we reason about algorithmic efficiency is similar to how we reason about the physical world when we do science: we use a model of a computer that is much simpler than the real thing, but one we can reason about without too many complications. We then work out how many primitive operations an algorithm needs to make on the input we give it. This is not an entirely accurate measurement of how long the actual algorithm will run on a real computer, but it is a reasonable estimate.

To make it even simpler to reason about efficiency than this, we often don’t care about counting the number of primitive operations accurately either. We will only care about how the number of calculations depends on the input in an asymptotic sense that should be clear by the end of this chapter. This might sound a bit sloppy, but there is a mathematical underpinning that justifies this, and by using this approach we can very quickly determine which of two algorithms is likely to be faster than the other.

## The RAM model of a computer and its primitive operations

Any actual computer you will ever use is a very complex machine. At its core is a gazillion transistors, wired together to implement different logical operations in hardware. The hardware in your computer handles both simple computations, such as comparing numbers or doing arithmetic, and manages how data is moved around. Information is stored different places in such a computer; you have persistent storage on disk, that lives outside of running programs, and while programs run, you have data moving between different layers of memory hierarchies. At the core of the computer is one or more central processing units, or CPUs, that do actual computations.^[There is also graphical processing units, GPUs. These are often used for running many simple calculations in parallel. For the purpose of this chapter, you can just consider them a different kind of CPU.] Data needs to be moved to these CPUs for them to compute on it. They can hold some data themselves, in what is called *registers*, and they can do computations on such data very quickly. There is only a finite number of registers, though, so data needs to be moved to and from these registers. Below registers are several layers of memory called *caches*, and the closer caches are to the registers, the faster data can be moved between them. Below caches, you have *random-access memory*, or RAM, and below that, disks, which can be solid state or actual rotating disks, or some combination thereof.

The levels of the memory hierarchy are vital to the actual performance of an algorithm. Every time you move data from one level to the next, the time it takes to access it grows by orders of magnitude. Your CPUs can compute on data much faster than it can fetch it from memory and put it back. Moving data up and down the memory hierarchy, however, is handled automatically by the computer, so we don’t have to deal with it explicitly. We can construct algorithms that ignore the memory hierarchy entirely and still get well-working programs. Considering the memory hierarchy can help us improve the running time of a program immensely, but doing this also hugely complicates our algorithms, and in many cases, solutions will depend on the actual hardware our software runs on.

I am mentioning all this to make it clear that considering any actual computer dramatically complicates how we can reason about the actual performance of an algorithm. If we have to consider any real computer, reasoning about running time becomes practically impossible. When we reason about algorithmic complexity, we use much simpler models of computers, and a traditional model is the so-called *RAM model*.

The critical property of the RAM model is that we assume that any data we need and know where to find, we can access in constant time. We understand that there is such a thing as a *computer word*, which you should think of as a number, a character, or a primitive piece of data like that. We cannot store arbitrarily large chunks of data in single computer words, but we assume that if the size of the input data for your algorithm is $n$, then our computer words can hold at least the number $n$. In other words, if your input data is of size $n$, then you can represent $\lceil \log_2 n \rceil$ bits in a single computer word. Any computer word in our input data, or any computer word we create in our algorithm, can be accessed on constant time. We ignore that some computer words can be located much faster than others; they are all just accessible in constant time. We also assume that we can compare computer words in constant time and that any arithmetic operation we do on numbers that can be stored in a single computer word can be done in constant time.

The reason we have to be a little careful about what a computer word is that we want to make the assumption about how fast we can perform calculations on them. We want our abstract computer words to roughly match what the actual computer can work within its registers. In the hardware of a real computer, the different operations you can do on numbers and bit-patterns are wired together, so the CPUs consider operating on words in registers as primitive operations. The time it takes actual hardware to do a primitive operation depends on what that operation is. The hardware for multiplication is more complicated than the hardware for simple comparison, but a lot of the computation involves parallel calculations, and at a rough approximation, all primitive operations take the same time. To actually compare two numbers that can be up to $n$ large, we need to compare $\lceil \log_2 n \rceil$ bits, so it *isn’t* constant time, but if a computer word has at least that many bits, we can; then it is a single CPU operation.

As long as you do not work with numbers that are larger than the size of the input, the RAM model says that:

1. You can compare numbers in constant time.
2. You can access any computer word in constant time.
3. You can do any arithmetic or bit-wise operation on computer words in constant time.

This is a considerable simplification of actual hardware. If we make these simple assumptions about how fast our model of a computer can do calculations, and we assume that all such operations take the same amount of time, then we can just count how many of these primitive operations an algorithm will need on any given input to get an estimate of how fast it will run.

Let us try counting primitive operations in a small example. Let us assume that we have a list of numbers,

```python
numbers = [1, 2, 3, 4, 5]
```

In this case, we have $n=5$ numbers. We usually do not count the operations it takes to get our input. Sometimes, we need to read it in from disk, or in this example, we need to tell Python what the numbers are, and this will take around $n$ operations to create this list. If the data is part of our input, however, we do not count those instructions. The reason for this is that there are many algorithms where we use less than $n$ operations to answer questions about our data, and we do not want to have our analysis of the efficiency of these dominated by how long it takes to get the input. Such algorithms are used as building blocks for other algorithms that will be responsible for getting the input in the right form for what we need, and we consider that a cost of the other algorithm. If you need to create a list as part of your algorithm, the number of operations this takes have to be included in the accounting, but here we just assume that the input is present before we start counting.

Let us now consider the task of adding all these numbers up. We can do this using this algorithm:

```python
accumulator = 0
for n in numbers:
	accumulator += n
```

First, we create a variable to store intermediate values, `accumulator`. Then we have a `for`-loop that runs over our input and for each input-number add that number to `accumulator`. Initialising a variable with a number amounts to setting a computer word to a value, so it takes one operation in our RAM model. Adding two numbers is also a single operation, and on all modern computers you can add a number to an existing computer word as a single operation, so we consider `accumulator += n` as a single operation—if you want to think about it as `accumulator = accumulator + n` you can count it as two operations. If, however, we consider updating `accumulator` a single operation, we have $1 + n$ operations for initialising and updating `accumulator`. It is less straightforward to know how many operations we spend on the `for`-loop, however. That depends on how Python handles `for`-loop statements. The `for`-loop construction is not as primitive as the RAM model. We can implement the same loop using a `while`-loop, however, using only primitive operations:

```python
accumulator = 0
length = len(numbers)
i = 0
while i < length:
	n = numbers[i]
	accumulator += n
	i += 1
```

Here, we have a more transparent view of the primitive operations a `for`-loop likely implements. To iterate over a list, we need to know how long it is.^[In Python, iterating over sequences doesn’t actually require that we know how many elements we have, because the `for`-loop construction is vastly more powerful than what we implement with this simple `while`-loop, but using the length is the simplest we can do if we implement the `for`-construction ourselves.] The statement

```python
length = len(numbers)
```

isn’t actually primitive either. It uses Python’s `len` function to get the length of a sequence, and we don’t know how many primitive operations *that* takes. If we assume, however, that lists store their lengths in a single computer word, which they really do, and that `len` simply gets that computer word, which is not far from the truth, we can consider this statement a single operation as well.^[Again we have an operation that Python provides for us, like the `for`-loop construction, that is actually more powerful than we give it credit for when we count the number of primitive operations the operation takes. We have to have to limit the level of detail we consider, so this is as deep as I want to take us down this particular rabbit hole.]

```python
accumulator = 0.        # one operation
length = len(numbers)   # one operation
```

We then initialise the variable `i` that we use to keep track of how many elements we have processed. That is also a single primitive operation.

The `while`-loop construction does two things. It evaluates its condition and if it is `True` it executes the code in the loop body; if it is `False`, it sends the program to the point right after the loop to start executing the statements there, if any, if any are left. We call such an operation a *branching operation* because it sends the point of execution to one of two places, and such an operation is implemented as a single instruction in hardware. So we will count the decision of where to continue execution as a single operation. We need to evaluate the loop condition first, however, which is a single numeric comparison, so that takes another primitive operation. We evaluate the loop-condition once for each element in `numbers`, i.e., $n$ times, so now we have:

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
  ...
```

Each of the statements inside the loop-body will be evaluated $n$ times, so we just need to figure out how many operations are required for each of the statements there and multiply that with $n$. The last two statements are simple, they update a number by adding another number to them, and that is a single operation.

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
	n = numbers[i]      # ? * n operations
	accumulator += n    # 2n operations
	i += 1              # 2n operations
```

The remaining question is how many operations it takes to handle the statement `n = numbers[i]`. Here, we need to understand what something like `numbers[i]` means. Once again, we use a Python construction that we do not necessarily know how many operations take to execute. To understand that, we need to know how Python represents lists in computer memory, and that is beyond the scope of this chapter. You will have to take my word on this, but it is possible to get the $i$’th element in a list by adding two computer words and then reading one, so we count this as two operations and end up with:

```python
accumulator = 0         # one operation
length = len(numbers)   # one operation
i = 0                   # one operation
while i < length:       # 2n operations 
	n = numbers[i]      # 2n operations
	accumulator += n    # n operations
	i += 1              # n operations
```

If we add it all up, we get that to compute the sum of $n$ numbers this way, we need to use $3 + 6n$ primitive operations.

If we assume that all `for`-loops are implemented by first capturing the length of the sequence, then setting a counter to zero, and then iterating through the sequence, comparing the counter with the length as the loop-condition and incrementing the counter at the end of the loop body, and accessing elements in the sequence by indexing, i.e., that the `for`-loop

```python
for element in a_list:
	# do something
```

can be translated into this `while`-loop

```python
length = length(a_list)        # 1 operation
counter = 0                    # 1 operation
while counter < length:        # 2n operations
    element = a_list[counter]  # 2n operations
    # do something
    counter += 1               # n operations
```

we can say that a `for`-loop iterating over a sequence of length $n$ takes $5n + 2$ operations. If we go back to our original `for`-loop adding algorithm we then get

```python
accumulator = 0       # 1 operation
for n in numbers:     # 5n + 2 operations
	accumulator += n  # n operations
```

which gives us the same $6n + 3$ operations as before.

If you feel a bit overwhelmed by how difficult it is to count the number of primitive operations in something as simple as this, I understand you and feel for you. If it is this hard to count operations in the abstract RAM model, where we even make some assumptions about how many operations Python needs to implement its operations, you will understand why we do not even attempt to accurately estimate how much time it will take to execute code on a real computer.

The good news is that we very rarely do count the number of operations an algorithm needs this carefully. Remember that I said, at the beginning of the chapter, that we generally consider functions such as $2x^2+x$ and $100x^2+1000x$ the same. We will also consider $6n + 3$ the same as $n$ number of operations, in a way I will explain a little later. First, however, you should try to count primitive operations in a few examples yourself.

### Counting primitive operations exercises

**Exercise:** Consider this way of computing the mean of a sequence of numbers:

```python
accumulator = 0
for n in numbers:
	accumulator += n
mean = accumulator / len(numbers)
```

Count how many primitive operations it takes. To do it correctly you need to distinguish between updating a variable and assigning a value to a new one. Updating the accumulator `accumulator += n` usually maps to a single operation on a CPU because it involves changing the value of a number that is most likely in a register. Assigning to a new variable, as in

```python
mean = accumulator / len(numbers)
```

doesn’t update `accumulator` with a new value, rather it needs to compute a division, which is one operation (and it needs `len(numbers)` before it can do this, which is another operation), and then write the result in a new variable, which is an additional operation.

**Exercise:** Consider this alternative algorithm for computing the mean of a sequence of numbers:

```python
accumulator = 0
length = 0
for n in numbers:
	accumulator += n
	length += 1
mean = accumulator / length
```

How many operations are needed here? Is it more or less efficient than the previous algorithm?

## Best-case, worst-case, and average-case efficiency

When computing the sum of a sequence of numbers, the number of operations we need is independent of what the actual numbers are, so we could express the number of operations as a function of the input size, $n$. Things are not always that simple.

Consider this algorithm for determine if a number $x$ is in the sequence of numbers `numbers`:

```python
in_list = False
for n in numbers:
	if n == x:
		in_list = True
		break
```

Here, we assume that both `numbers` and `x` are initialised beforehand. The algorithm sets a variable, `in_list`, to `False`—one primitive operation—and then iterates through the numbers. This would usually cost us $5n + 2$ operations for the `for`-loop, but notice that we `break` if we find the value in the list. We won’t spend more than $5n + 2$ operations on the `for`-loop, but we might spend considerably less. Let us say that we iterate $m$ times, so the cost of the `for`-loop is $5m + 2$ instead. Then we can count operations like this:

```python
in_list = False           # 1 operation
for n in numbers:         # 5m + 2 operations
	if n == x:            # 2m operations
		in_list = True    # 1 operation 
		break             # 1 operation
```

The `if`-statement is a branching operation just like the `while`-loop; we need to compare two numbers, which is one operation, and then decide whether we continue executing inside the `if`-body or whether we continue after the `if`-block, which is another operation. Inside the `if`-block, we have two primitive operations. Setting the variable `in_list` is a primitive operation like before, and `break` is another branching operation, that does not need a comparison first, but that simply moves us to the point in the code after the loop. These two operations are only executed once since they are only executed when the `n == x` expression evaluates to `True`, and we leave the loop right after that.

When we add up all the operations, we find that the algorithm takes $7m + 5$ operations in total, where $m$ depends on the input. It is not unusual that the running time of an algorithm depends on the actual input, but taking that into account vastly complicates the analysis. The analysis is greatly simplified if we count the running time as a function of the size of the input, $n$, and do not have to consider the actual data in the input.

For this simple search algorithm, we apparently cannot reduce the running time to a function of $n$, because it depends on how soon we see the value $x$ in the sequence, if at all. To get out of the problem, we usually consider one of three cases: the *worst-case* complexity of an algorithm, the *best-case* complexity, or the *average-case* complexity.

To get the worst-case estimate of the number of operations the algorithm needs to take, we need to consider how the data can have a form where the algorithm needs to execute the maximum number of operations it can possibly do. In the search, if $x$ is not found in the sequence, we will have to search the entire sequence, which is as bad as it gets, so the worst-case time usage is $7n + 5$. If the very first element in `numbers` is $x$, then $m=1$, which is as good as it gets, so the best-case running time is $12$ operations.

The average-case is much harder to pin down. To know how well our algorithm will work on average we need to consider our input as randomly sampled from some distribution—it does not make sense to think of averages over something that doesn’t have an underlying distribution the data could come from, because then we would have nothing to average over. Because of this, we will generally not consider average-case complexities in this book. Average-case complexity is an important part of so-called randomised algorithms, where randomness is explicitly used in algorithms, and where we have some control over the data. To get a feeling for how we would reason about average-case complexity, imagine for a second that $x$ is in the `numbers` list but at a random location. I realise that would make it very silly to run the algorithm in the first place, since the result will always be `True`, but indulge me for a second. If $x$ is a random number from `numbers`, then the probability that it is at index $j$ for any $j$ in $1,\ldots,n$ is $1/n$. Since we stop when we find the item we are searching for, our variable $m$ is $j$ The mean of $m$ is therefore $\mathrm{E}[m]=\sum_{j=1}^n j/n$ which is $\frac{n+1}{2}$. Plugging that into $7m + 5$ we get the average-case time usage $7/2(n+1)+5$.

Of the three cases, we are usually most concerned with the worst-case performance of an algorithm. The average-case is of interest if we have to run the algorithm many times over data that is distributed in a way that matches the distributions we use for the analysis, but for any single input, it does not tell us much about how long the algorithm will actually run. The best-case is only hit by luck, so relying on that in our analysis is often overly optimistic. The worst-case performance, however, gives us some guarantees about how fast the algorithm will be on any data, however pathological, we give it.

### Exercise

Recall the guessing game from the previous chapter, where one player thinks a number between 1 and 20 and the other has to guess it. We had three strategies for the guesser:

1. Start with one. If it isn't the right number, it has to be too low---there are no smaller numbers the right one could be. So if it isn't one, you guess it is two. If it isn't, you have once again guessed too low, so now you try three. You continue by incrementing your guess by one until you get the right answer.
2. Alternatively, you start at 20. If the right number is 20, great, you got it in one guess, but if it is not, your guess must be too high---it cannot possibly be too small. So you try 19 instead, and this time you work your way down until you get the right answer.
3. The third strategy is this: you start by guessing ten. If this is correct, you are done, if it is too high, you know the real number must be in the range $[1,9]$, and if the guess is too low, you know the right answer must be in the range $[11,20]$---so for your next guess, you pick the middle of the range it must be. With each new guess, you update the interval where the real number can be hidden and pick the middle of the new range.

**Exercise:** Identify the best- and worst-case scenarios for each strategy and derive the best-case and worst-case time usage.

## Asymptotic running time and big-Oh notation

As we have seen, counting the actual number of primitive operations used by an algorithm quickly becomes very tedious. Considering that all operations are not actually equal, and given that memory-hierarchies make access to different data more or less expensive, it also seems to be a waste of time to be that precise in counting operations. If adding ten numbers can be much faster than multiplying three, why should we worry that the first involves nine operations and the second only two? In fact, we very rarely do count the actual operations, and when we do, it is for elementary but essential algorithms. What we really care about is how the running time (or memory or disk space usage, or whatever critical resource our algorithm uses) grows as a function of the input size in some rough sense, that we will now consider.

When we have a function of the input size, $f(n)$, we care about the *order* of $f$. We classify functions into classes and only care about which class $f$ belongs to, not what the actual function $f$ is. Given a function $f$ we define its “order class” $O(f)$ as:

$$O(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq g(n) \leq c f(n) \,\right\}$$
	
Put into words, this means that the class of functions of order $f$ are those functions $g(n)$ that, as $n$ goes to infinity, will eventually be dominated by $f(n)$ times a positive constant $c$.

If $g$ is in $O(f)$ we write $g\in O(f)$, as per usual mathematical notation, but you will often also see it written as $g=O(f)$. This is an abuse of notation, but you might run into it, so I should mention it, even if I will not use that notation myself.

Consider functions $f(n) = 10n + 100$ and $g(n) = 3n + 100,000$. We have $g\in O(g)$ because, eventually, a line with incline 10 will grow faster than one with incline 3, so eventually, regardless of where the lines intersect the $y$-axis, the former will be above the latter. We also have $f\in O(g)$ because we can simply multiply $g(n)$ by $10/3$ and it is already above $f(n)$ because it intersects the $y$ axis at a higher value. This “big-Oh” notation is not symmetric, however. Consider the function $h(n)=n^2$. We have both $f\in O(h)$ and $g\in O(h)$ because, regardless of where a line intersects the $y$-axis, $n^2$ will eventually be larger than the line. We do *not* have $h\in O(f)$, however, for exactly the same reason. There is no constant $n_0$ after which a line, regardless of what its intercept is, will always be larger than $n^2$.

When we write $f\in O(g)$ we provide an *upper bound* on $f$. We know that as $n$ goes to infinity, $f(n)$ will be bounded by $cg(n)$ for some constant $c$. The upper bound need not be exact, however, as we saw with $h(n)$. We have a separate notation for that, the “big-Theta" notation:

$$\Theta(f) = \left\{\,
g(n): \exists c_1,c_2\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq c_1 f(n) \leq g(n) \leq c_2 f(n)
\,\right\}$$

The class of functions $\Theta(f)$ are those that, depending on which constant you multiply them with, can be both upper- and lower-bounds of $f(n)$ as $n$ goes to infinity. This notation *is* symmetric: $g\in\Theta(f)$ if and only if $f\in\Theta(g)$. For our two lines, $f(n) = 10n + 100$ and $g(n) = 3n + 100,000$, we have $f\in\Theta(g)$ and $g\in\Theta(f)$, and we can in good conscience write $\Theta(f)=\Theta(g)$ as they are the same classes of functions. We can also write $O(f(n))=O(g(n))=O(n)$ since these are also the same classes, but $O(f)\neq O(h)$ because $O(h(n))=O(n^2)$ is a large class than $O(n)$. There are functions that are bounded, up to a constant, by $n^2$ that are not bounded by $n$, regardless of which constant we multiply to $n$.

The big-Oh notation can be thought of as saying something like “less than or equal to”. When we say that $g\in O(f)$ we are saying that $g$ doesn’t grow faster than $f$, up to some constant and after a certain $n_0$. Because the notation is concerned with what happens as $n$ goes to infinity, it captures *asymptotic behaviour*. This is just a math term for saying that we concern ourselves with what a function does as it moves towards a limit, in this case, infinity. When we analyse algorithms, we usually care more about how efficient they are on large input data compared to how they behave on small input data, so we care about their asymptotic efficiency more than we care about the details of how they behave for small $n$.

Consider algorithms that use $n^2$ versus $10n + 100$ operations on input of size $n$. For small $n$, the $n^2$ algorithm will use fewer operations than the $10n + 100$ algorithm, but unless we expect that the input size will always be small, the $10n + 100$ algorithm is superior. As $n$ grows, it will eventually be *much* faster than the $n^2$ algorithm.

When we have an algorithm that uses $f(n)$ operations for input of size $n$, and we know $f\in O(g)$, then we know that $g$ will eventually bound the running time for the algorithm. We will say that “the algorithm runs in time $O(g)$”—or usually qualify that we consider worst-case complexity, say, by saying that “the algorithm runs in worst-case time $O(g)$”.

Consider the example from earlier where we added $n$ numbers together. We counted that this involved $6n + 3$ primitive operations. If we pick any constant $c>6$, then $cn$ will eventually be larger than $6n + 3$, so we can say that the algorithm runs in time $O(n)$. The cost of each individual operation will vary according to the actual hardware you will execute the algorithm on, and it will not be the same cost for each operation; knowing that the running time is linear, i.e., that the algorithm runs in time $O(n)$, is much more important to know than what the exact number of operations is. Therefore, we usually do not care about the actual number of operations but only the big-Oh of them.

The big-Oh notation is the most used notation of the two. It is often easier to show that an algorithm does not do more than a certain number of operations, $f(n)$, than to show that it doesn’t do fewer. In many cases, we can show that an algorithm’s running time is in big-Theta, $\Theta(f)$, which gives us more information about the actual running time, but it can at times be harder to get such an exact function of its running time. When we use big-Oh, it is not a problem to over-count how many operations we actually use; when we use big-Theta, we have to get it right.

### Other classes

In the vast majority of times when we use this asymptotic notation, we use big-Oh. We often do that even when we know that an algorithm’s complexity accurately and could use big-Theta. This is just because we usually care much more about bounding the running time of an algorithm than we care about getting the running time precisely right. For completeness, however, I will quickly define a few more classes, in case you run into these outside of this book.

The big-Oh class of a function is all functions that work as upper bounds of it. If $g\in O(f)$, then after some $n_0$ and for some constant $c$, $g(n) \leq c f(n)$. If we can change the inequality sign to be less, rather than less-than, we get a different class of functions, called “little-oh”:

$$o(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq g(n) < c f(n) \,\right\}$$
	
If $g\in o(f)$ then we know that $g\in O(f)$ but that $f\not\in O(g)$. Asymptotically, $f$ grows strictly faster than $g$.

We also have notation for lower bounds, similar to big-Oh and little-oh: we have big-Omega and little-omega:

$$\Omega(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq f(n) \leq c g(n) \,\right\}$$
$$\omega(f) = \left\{\, g(n): \exists c\in\mathbb{R}^+, n_0\in\mathbb{N} : \forall n\in\mathbb{N} \geq n_0 :
	0 \leq f(n) < c g(n) \,\right\}$$

These captures lower bounds, strict or otherwise. When $g\in\Theta(f)$ we have that, asymptotically $f$ can dominate $g$ and $g$ can dominate $f$; so we have that $g\in\Theta(f)$ is equivalent to saying $g\in O(f)$ as well as $g\in\Omega(f)$.

When we reason about the efficiency of an algorithm, we care about upper bounds and use the oh-notation; if we instead worry about how intrinsically complex problems are, we usually care about the lower bounds for any algorithm that solves them will have to be, and use the omega-notation. If we have a problem that we know requires $\Omega(f)$ to solve, but we also have an algorithm that solves it in $O(f)$, we have a tight bound of $\Theta(f)$. We do not concern ourselves much with the complexity of problems in this book, but it will come up from time to time, so keep this in mind.

### Properties of complexity classes

These classes of functions have a few rules that are easy to derive that can help us reason about them in general. As long as we do not use the strict upper- and lower-bound classes, a function is going to be in its own class:

$$f \in \Theta(f)$$
$$f \in O(f)$$
$$f \in \Omega(f)$$

There is a symmetry between upper- and lower bounds:

$$f\in O(g) \quad\text{ if and only if }\quad g\in\Omega(f)$$
$$f\in o(g) \quad\text{ if and only if }\quad g\in\omega(f)$$

Furthermore, there is transitivity in these senses:

$$f\in\Theta(g) \land g\in\Theta(h) \implies f\in\Theta(h)$$
$$f\in O(g) \land g\in O(h) \implies f\in O(h)$$
$$f\in\Omega(g) \land g\in\Omega(h) \implies f\in\Omega(h)$$
$$f\in o(g) \land g\in o(h) \implies f\in o(h)$$
$$f\in\omega(g) \land g\in\omega(h) \implies f\in\omega(h)$$

I have personally found very few opportunities where these rules have been useful, but if you can see why the rules must be correct, you have taken a significant step towards understanding this asymptotic function notation.

One rule that can be very useful is this: if $f\in\Theta(g)$, i.e., there exists $c_1,c_2\in\mathbb{R}^+$ such that $c_1g(n)\leq f(n)\leq c_2g(n)$ as $n$ goes to infinity, then we also have

$$c_1 \leq \frac{f(n)}{g(n)} \leq c_2.$$

This means that if we believe that an algorithm runs in time $f\in\Theta(g)$, we can experimentally measure its running time and divided it by $g(n)$. If we are right about the running time should eventually stay confined to the interval between these two constants. In other words, $f(n)/g(n)$ should flatten out as $n$ goes to infinity; not entirely, because it can fluctuate between $c_1$ and $c_2$, but it will eventually be bounded by these two constants. In practice, the ratio often does converge to a single constant non-zero. If we are wrong about the complexity, this doesn’t happen. If $f\in o(g)$, then the ratio will converge to zero and not a positive constant, while if $f\in\omega(g)$, the ratio will grow unbounded.

### Reasoning about algorithmic efficiency using the big-Oh notation

Here is how we would reason about an algorithm when using big-Oh notation. Everything that we only do once is simply $O(1)$. Regardless of what the actual cost of an operation is, say $c$, it is still $O(1)$ because $O(1)=O(c)$ for all constants. It also doesn’t matter how many things we do, as long as we do them once, because $O(1 + 1)=O(1)$, after all, $O(2)=O(1)$. Whenever we loop over $n$ elements, that loop costs us $O(n)$ operations. Any operation that happens inside the loop body can be multiplied by $n$ to account for its cost. It doesn’t matter if we actually *do* execute that operation for each iteration; the big-Oh notation gives us an upper bound for the running time, so if we count an operation more often than we really should, that is okay.

In the summation algorithm, we therefore have:

```python
accumulator = 0       # O(1)
for n in numbers:     # O(n)
	accumulator += n  # n * O(1)
```

where $n \times O(1)$ should be read as $O(n)$—we are really saying that we do something that has a constant cost, say $c$, but we do it $n$ times, so we do something that costs $cn$ which is in $O(n)$.

With the search algorithm, we can reason similarly and get:

```python
in_list = False           # O(1)
for n in numbers:         # O(n)
	if n == x:            # n * O(1)
		in_list = True    # n * O(1)
		break             # n * O(1)
```

We know that we only ever execute the body of the `if`-statement once, but it is easier just to multiply all the operations inside the loop by $n$, and since the big-Oh notation only guarantees us that we have an upper bound, it works out okay.^[In this particular case, we could just as easily count the `if`-body as constant time as well, but it doesn’t matter for the big-Oh analysis. The entire algorithm runs in $\Theta(n)$.]

To make the big-Oh reasoning about these algorithms even more relaxed, we will work out how to add big-Oh expressions together.

### Doing arithmetic in big-Oh

When analysing algorithms like we just did, we often end up with multiplying a function to a big-Oh class, as we did with $n\cdot O(1)$, and if we want to combine a number of calculations where we know their big-Oh class, we need to add them together, that is we want to know what $O(f) + O(g)$ means. 

First, we need to know how to do arithmetic on functions, but that is probably familiar to you. If you have a function $f$ and multiply it with a constant, $c$, you have $cf$ which is the function $h(n)=c\cdot f(n)$. Similarly, $f+c$ is the function $h(n)=f(n) + c$. If you have two functions, $f$ and $g$, then $f\cdot g$ is the function $h(n)=f(n)g(n)$ and $f+g$ is the function $h(n)=f(n)+g(n)$.^[Do not confuse function multiplication, $f\cdot g$, with function composition, $f\cdot g$. The former means that $(f\cdot g)(n) = f(n)\cdot g(n)$, while the latter means that $(f\cdot g)(n) = f(g(n))$.]


For the rules for doing arithmetic with big-Oh notation let $f$, $f_1$, $f_2$, $g$, $g_1$, and $g_2$ denote functions.

The first important rule is this:

$$f_1\in O(g_1) \land f_2\in O(g_2) \implies f_1+f_2\in O(g_1+g_2)$$

We can also write this as:

$$O(f) + O(g) = O(f + g).$$

Since $g+g$ is the same as $2g$ we furthermore have that

$$f_1\in O(g) \land f_2\in O(g) \implies f_1+f_2\in O(g)$$

This means that if we can bound two separate pieces of the running time of an algorithm by the same function, then the entire algorithm’s running time is bounded by that. Because of the transitivity rules for big-Oh, this also means that if $f\in O(g)$ then $f+g\in O(g)$. So when we add together different parts of a runtime analysis, whenever one function dominates the other, we can simply discard the dominated part. So, if we add a constant number of operations, $O(1)$, to a linear number of operations, $O(n)$, as we have to in our examples, we end up with something that is still linear $O(n)$.

We also have a rule for multiplying functions:

$$f_1\in O(g_1) \land f_2\in O(g_2) \implies f_1\cdot f_2\in O(g_1\cdot g_2)$$

or

$$O(f)\cdot O(g) = O(f\cdot g).$$

This rule is particularly useful when we reason about loops. The time it takes to execute a loop over $n$ elements is $O(n)$ times the time it takes to execute the loop body, which could be some function $f(n)$, so the total running time for such a loop is $n\cdot f(n)\in O(n f(n))$.

We also have rules for multiplying into big-Oh classes:

$$f \cdot O(g) = O(f\cdot g)$$

We can use this in our examples where we have expressions such as $n\times O(1)$. This simply becomes $O(n)$. From this rule, it follows that when $c$ is a non-zero constant we have:

$$c \cdot O(f) = O(f)$$

You can see this by merely considering $c$ a constant function: $c(n) = c$ and then apply the previous rule. 

We can now apply these rules to get a final, big-Oh, running time for our example algorithms. Consider first the summation:

```python
accumulator = 0       # O(1)
for n in numbers:     # O(n)
	accumulator += n  # n * O(1)
```

We have to work out $O(1) + O(n) + n\cdot O(1)$. This is the same as $O(1) + O(n) + O(n)$ by the rule for how to multiply into a big-Oh class, and then, by the addition rule, we get $O(1 + n + n)=O(n)$.

For the search we have

```python
in_list = False           # O(1)
for n in numbers:         # O(n)
	if n == x:            # n * O(1)
		in_list = True    # n * O(1)
		break             # n * O(1)
```

which breaks down to $O(1) + O(n) + 3\times n\times O(1) = O(n)$.

Consider now a different algorithm:

```python
for i in range(1,len(numbers)):
	x = numbers[i]
	j = i
	while j > 0 and numbers[j-1] > numbers[j]:
		numbers[j-1], numbers[j] = numbers[j], numbers[j-1]
		j -= 1
```

This is an algorithm called “insertion sort”, and we return to it in the next chapter where we prove that it sorts `numbers`. Here, we will just consider its running time. The algorithm consists of two nested loops. So we reason as follows: the outer loop iterates over `numbers`, which gives us an $O(n)$ contribution. It doesn’t include the first element in `numbers`, but that doesn’t change anything since $O(n-1)=O(n)$. Inside the loop body, we have two constant time contributions and an inner loop. The constant contributions give us $O(1)$ and the inner loop is bounded by $O(n)$ ($j$ starts at a number less than $n$ and is decreased by at least one in each iteration and leave the look if it reaches zero). So we have the running time 

$$O\left(n\right)\times\left(O\left(1\right)+O\left(n\right)\right)=O\left(n^2\right).$$

In other words, this sorting algorithm runs in quadratic time. Or rather, we should say that its *worst-case* running time is $O\left(n^2\right)$. In the analysis, we assumed that the inner loop could execute $n$ iterations; if the list is already sorted, then `numbers[j-1] > numbers[j]` is always `False`, and the inner loop will actually take constant time. So the *best-case* running time is only $O(n)$.

Now consider this algorithm, called *binary search*. This is another algorithm we return to in the next chapter. It searches for a number, $x$, in a sorted list of numbers using the third strategy from the guessing game we have seen a few times by now. It has a range in which to search, bracketed by `low` and `high`. If $x$ is in the list, we know that it must be between these two indices. We pick the midpoint of the range and check if we have found $x$, in which case we terminate the search. If we haven’t, we check if the midpoint number is smaller or larger than $x$. If it is smaller, we move `low` up to the midpoint plus one. If it is larger, we move `high` down to the midpoint. We do not move it to the midpoint minus one. If we did, it is possible to miss the index where $x$ is hiding; this can happen if the interval is of length one and `low` is the index where $x$ resides, as we will see next chapter.

```python
low, high = 0, len(numbers)
found = False
while low < high:
	mid = (low + high) // 2
	if numbers[mid] == x:
		found = True
		break
	elif numbers[mid] < x:
		low = mid + 1
	else:
		high = mid
```

We can analyse its complexity as follows: we have some constant time operations at the beginning, $O(1)$, and then a loop. It is less obvious how many iterations the loop will make in this algorithm than in the previous ones we have looked at, but let us call that number $m$. Then the loop takes time $O(m)$ times the time it takes to execute the loop body. All the operations inside the loop are constant time, $O(1)$, so we have

$$O\left(1\right)+O\left(m\right)\times O\left(1\right)=O\left(m\right).$$

Of course, this isn’t entirely satisfying. We don’t know what $m$ is, and we want the running time to be a function of $n$.

The interval we are searching in decreases in each iteration, so if we start with an interval of size $n$ we could, correctly, argue that the loop is bounded by $n$ as well, and derive a running time of $O(n)$. This is correct; $O(n)$ is an upper bound for the (worst-case) complexity. In fact, however, the algorithm is much faster than that. It actually runs in $O(\log_2 n)$, where $\log_2$ is the base-two logarithm. To see this, consider how much we shrink the interval by in each iteration. When we update the interval, we always shrink the interval to half the size of the previous one. So we can ask ourselves, how many times can you halve $n$ before you get below one? That is $\lceil\log_2 n\rceil$, so $m$ is bounded by $O(\lceil\log_2 n\rceil)$. We typically do not include the rounding-up notation when we write this but consider it implicit, so we simply write $O(\log_2 n)$.

### Important complexity classes

Some complexity classes pop up again and again in algorithms, so you should get familiar with them. From fastest to slowest, these are:

* Constant time: $O(1)$. This is, obviously, the best you can achieve asymptotically, but since it does not depend on the input size, we very rarely see algorithms running in this time. 
* Logarithmic time: $O(\log n)$. We saw that binary search was one such algorithm. Generally, we see this complexity when we can reduce the size of the input we look at by a fixed fraction in each iteration of a loop. Typically, we can cut the data in half, and we get a base-two logarithm, but since the difference between two different-based logarithms is a constant, $\log_a(x)=1/\log_b(a)\cdot\log_b(x)$, we rarely write the base in big-Oh notation.
* Linear time: $O(n)$. We saw several examples of linear time algorithms in this chapter. Whenever we do something where we have to, in the worst case, examine all our input, the complexity will be at least this.
* Log-linear time: $O(n\log n)$. We haven’t seen examples of this class yet, but it will show up several times in [Chapter @sec:divide-and-conquer].
* Quadratic time: $O(n^2)$. We saw that bubble sort had this complexity. This complexity often shows up when we have nested loops.
* Cubic time: $O(n^3)$. This is another class we haven’t seen yet, but when we have three levels of nested loops, we see it. If you multiply two $n\times n$ algorithms the straightforward way,^[It is possible to multiply matrices faster than this, but that is beyond this book.]  $C=AB$, you have to compute $n\times n$ values in $C$, and for each $c_{ij} = \sum_k a_{ik}b_{kj}$, you sum $n$ values together, giving you a total running time of $n^3$.
* Exponential time: $O(2^n)$. You should avoid this complexity like the plague. Even for tiny $n$, this running time is practically forever. It does, unfortunately, pop up in many important optimisation problems where we do not know of any faster algorithms. If you have a problem that can only be solved in this complexity, you should try to modify the problem to something you can solve more efficiently, or you should try to approximate the solution instead of getting the optimal solution. Algorithms that run in this complexity are rarely worth considering.

In [@fig:function-growth] I have plotted the growth of the different classes. As you can see in the upper-left frame, logarithmic growth is very slow compared to linear growth. On the upper-right, you can see that log-linear growth is slower than linear, but compared to quadratic time, it is much faster. Cubic time, lower-left, is much slower than quadratic time, and exponential time, lower-right, just grows to infinity before you even get started.

![Growth of different complexity classes.](figures/function-growth){#fig:function-growth}

### Asymptotic complexity exercises

In the following exercises, you can test your understanding of the big-Oh notation and in how you derive upper bounds for algorithm’s running time.

#### Function growth

Consider the classes 

1. $O(\log n)$, $o(\log n)$, $\Omega(\log n)$ and $\omega(\log n)$
3. $O(n)$, $o(n)$, $\Omega(n)$, and $\omega(n)$
4. $O(n^2)$, $o(n^2)$, $\Omega(n^2)$, and $\omega(n^2)$
5. $O(2^n)$, $o(2^n)$, $\Omega(2^n)$, and $\omega(2^n)$

**Exercise:** For each of the functions below, determine which of the 16 classes it belongs in. Remember that the complexity classes overlap, so for example, if $f\in o(g)$ then $f\in O(g)$ as well, and if $f\in\Theta(g)$ then $f\in O(g)$ as well as $\Omega(g)$ (but $f\not\in o(g)$ and $f\not\in\omega(g)$).

1. $f(n) = 23n$
2. $f(n) = 42n^2 - 100n$
3. $f(n) = n/\log n$
4. $f(n) = \log(n)/n$
5. $f(n) = n^2/\log n$
6. $f(n) = \log n + \log(n)/n$
7. $f(n) = 5^n - n^3$
8. $f(n) = n!$
9. $f(n) = 2^n/n$
10. $f(n) = \log\left(\log n\right)$

#### Insertion sort

Consider the insertion sort algorithm. We argued that the worst-case running time was $O\left(n^2\right)$ but the best-case running time was $O\left(n\right)$.

**Exercise:** Describe what the input data, `numbers`, should look like to actually achieve the worst- and best-case running times.

### Binary search

We argued that the worst-case running time for the binary search is $O\left(\log n\right)$.

**Exercise:** What is the best-case running time, and what would the input data look like to achieve it?

### Sieve of Eratosthenes

Recall the Sieve of Eratosthenes from the previous chapter. 

**Exercise:** Derive an upper bound for its running time. 

**Exercise:** Is there a difference between its best-case and worst-case running time?

### Longest increasing substring

Recall the exercise from the previous chapter where you should design an algorithm that finds the longest sub-sequence `x[i:j]` such that consecutive numbers are increasing, i.e. `x[k] < x[k+1]` for all `k` in `range(i,j)`  (or one of the longest, if there are more than one with the same length).

**Exercise:** What is the time complexity of your solution?

**Exercise:** Can you construct a linear-time algorithm for solving this problem?

*Hint:* One way to approach this is to consider the longest sequence seen so far and the longest sequence up to a given index into `x`. From this, you can formalise invariants that should get you through.


### Merging

Recall the *merging* algorithm from the previous chapter.

**Exercise:** Show that you can merge two sorted lists of size $n$ and $m$, respectively, into one sorted list containing the elements from the two, in time $O(n+m)$.
